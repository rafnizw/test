<!DOCTYPE html>
<!DOCTYPE html>

<head>
  
  <!-- Google Tag Manager -->
  <script>(function (w, d, s, l, i) { w[l] = w[l] || []; w[l].push({ 'gtm.start': new Date().getTime(), event: 'gtm.js' }); var f = d.getElementsByTagName(s)[0], j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f); })(window, document, 'script', 'dataLayer', 'GTM-PPZPQ6');</script>
  <!-- End Google Tag Manager -->
  <title>BIG-IP Next Fixes and Known Issues</title>
  

  <meta charset="utf-8">
  <!-- Twitter Bootstrap viewport setting -->
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <!-- F5 metadata -->
  <meta name="title" content="BIG-IP Next Fixes and Known Issues" />
  <meta name="product" content="BIG-IP Next" />
  <meta name="version" content="" />
  <meta name="updated_date" content="None" />
  <meta name="archived" content="Archived documents excluded" />
  <meta name="doc_type" content="Manual" />
  <meta name="lifecycle" content="release" />
  <meta name="BIG-IP Next" content="" />

  
  

  
  <link href="https://cdn.f5.com/favicon.ico" rel="icon">

</head>
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>BIG-IP Next Fixes and Known Issues &#8212; BIG-IP Next 1.0</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/bootstrap.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/table_styling.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.5ea377869091fd0449014c60fc090103.min.css" />
    <link rel="stylesheet" href="../_static/css/bootstrap.min.css" type="text/css" />
    <link rel="stylesheet" href="../_static/css/f5.css" type="text/css" />
    <link rel="stylesheet" href="../_static/css/f5-theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/css/CoveoFullSearch.css" type="text/css" />
    <link rel="stylesheet" href="../_static/css/custom.css" type="text/css" />
    <link rel="stylesheet" href="https://use.fontawesome.com/21fb8a09c3.css" type="text/css" />
    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/sphinx_highlight.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/design-tabs.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Install BIG-IP Next" href="../install/index.html" />
    <link rel="prev" title="BIG-IP Next 20.3.0 Overview" href="big-ip-next-rn-new-features.html" /> 
  </head><body>
<div id="clouddocs-header"></div>

        <div id="sidebar" class="section-nav">
            <!-- javascript code to maintain the sidebar scroll positon when loaded -->
            <script type="text/javascript">
                $(function () {
                    if (localStorage.tempScrollTop) {
                        $('#sidebar').scrollTop(localStorage.tempScrollTop);
                    }
                });
                $('#sidebar').on("scroll", function () {
                    localStorage.setItem("tempScrollTop", $('#sidebar').scrollTop());
                });
            </script>
            
            <!--  version selector ------------------>
            <div id="version_selector_wrapper">
            </div>
            <nav class="nav-sidebartoc">

                <div class="dropdown">
                    <button class="dropbtn">BIG-IP Next v20.3.0 (latest) <span class="downarrow"> &#8250; </span></button>
              
                      <div class="dropdown-content">
                        <a href="https://clouddocs.f5.com/bigip-next/20-2-1/" >
                            BIG-IP Next v20.2.1 </a>
                        <a href="https://clouddocs.f5.com/bigip-next/20-2-0/" >
                            BIG-IP Next v20.2.0</a>
                        <a href="https://clouddocs.f5.com/bigip-next/20-1-0/" >
                          BIG-IP Next v20.1.0</a>
                        <a href="https://clouddocs.f5.com/bigip-next/20-0-2/" >
                          BIG-IP Next v20.0.2</a>
                        <a href="https://clouddocs.f5.com/bigip-next/20-0-1/" >
                          BIG-IP Next v20.0.1</a>
                      
                      </div>
              
                    </div><!DOCTYPE html>
<div id="searchbox" role="search">
  <form class="search" action="../search.html" method="get">
    <input type="text" class="search" name="q" placeholder=" Search..." />
    <button type="submit" class="btn btn-info navbar-btn"><i class="fa fa-search"></i></button>
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
                <hr>
                <span class="nav-sidebartoc">
                    <ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Release Notes</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="big-ip-next-rn-new-features.html">BIG-IP Next 20.3.0 Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="big-ip-next-rn-new-features.html#system-requirements-and-compatibility">System Requirements and Compatibility</a></li>
<li class="toctree-l2"><a class="reference internal" href="big-ip-next-rn-new-features.html#whats-new-in-big-ip-next-20-3-0">Whatâs New in BIG-IP Next 20.3.0</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">BIG-IP Next Fixes and Known Issues</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../install/index.html">Install BIG-IP Next</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../use_cm/index.html">BIG-IP Next Central Manager</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../gslb_dns/index.html">Global Resiliency with BIG-IP Next DNS (EA)</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../waf_management/index.html">WAF Management</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../access_management/index.html">Access Management (LA)</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../sslo_management/index.html">SSL Orchestrator Management (LA)</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../irules/index.html">iRules</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../support/index.html">Support and Troubleshooting</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../attribution/index.html">Attribution Reports</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api_docs.html">API Specification</a></li>
</ul>

                </span><!DOCTYPE html>
<!-- Use this file to add extra links to the bottom of the ToC sidebar -->
<hr>
<span class="nav-sidebartoc">
  <!-- insert content here -->
</span>
            </nav>
        </div>
         <div id="right-sidebar">
      <h7 style="font-weight:normal">On this page:</h7>
      <nav class="nav-sidebartoc">
        <span class="nav-sidebartoc">
          <ul>
<li><a class="reference internal" href="#">BIG-IP Next Fixes and Known Issues</a></li>
</ul>

        </span>
           </nav>
             </div>





        

        <div class="main active" id="content" >

            <article class="docs-container site-article-inner">
                <!DOCTYPE html>
<nav class="site-breadcrumb-nav site-nav">
  <ul class="site-breadcrumb-list">
    <li class="breadcrumb-item"><button type="button" id="sidebarCollapse"
        class="btn btn-info navbar-btn site-hidden"><i class="fa fa-align-justify"></i></button>&#x20 <a
        href="/">CloudDocs Home</a> &gt; <a href="../index.html">BIG-IP Next </a> &gt; BIG-IP Next Fixes and Known Issues</li>
  </ul>
</nav>
                


                <!--a title="Export PDF" id="export-pdf" class="btn btn btn-link pull-right">View PDF</a-->
                <button type="button" id="export-pdf" class="btn btn-link right">PDF</button>


                
                
                <div class="sidebar" id="version-warning" style="display: none;">
                    <p class="first sidebar-title">
                        <span class="icon fa fa-info-circle fa-lg"></span> Version notice:
                    </p>
                    <p class="last" id="currentVersion"></p>
                </div>


                <div role="main">
                    
  <div class="section" id="big-ip-next-fixes-and-known-issues">
<h1>BIG-IP Next Fixes and Known Issues<a class="headerlink" href="#big-ip-next-fixes-and-known-issues" title="Permalink to this heading">Â¶</a></h1>
<p>This list highlights known issues for this BIG-IP Next release.</p>
<html>
  <head>
    <title>BIG-IP Next Release Notes</title>
    <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
    <style type="text/css">
    pre {
        white-space: pre-wrap;
        white-space: -moz-pre-wrap;
        white-space: -pre-wrap;
        white-space: -o-pre-wrap;
        word-wrap: break-word;
    }
    </style>
  </head>
  <body>
    <font size="5">
      <u><strong>BIG-IP Next Release Information</strong></u><br>
    </font>
    <br>
Version: 20.3.0<br>
Build: 2.716.2+0.0.50<br>
<br>
<a href="#KnownIssues">Known Issues in BIG-IP Next v20.3.0</a><br>
<br><a name="vOrange" rel="nofollow"></a><br><font size="4"><b><u>Cumulative fixes from BIG-IP Next v20.3.0 that are included in this release</u></b></font><br><br>
<br><font size="4"><u><strong>Vulnerability Fixes</strong></u></font><br><br>
<div style="background-color:#b3ecff"><table cellpadding="2" cellspacing="2" border="0" width="100%">
                    <tr valign="top">
                        <td width="10%"><u>ID Number</u></td>    <td width="10%"><u>CVE</u></td>    <td width="10%"><u>Links to More Info</u></td>
                    <td width="50%"><u>Description</u></td></tr>
  <tr valign="top">
    <td><a href="#A1449709-6">1449709-6</a></td>
    <td>CVE-2024-28889</td>
    <td> <a href="https://my.f5.com/manage/s/article/K000138912" target="_blank">K000138912</a>, <a href=http://cdn.f5.com/product/bugtracker/ID1449709.html target="_blank">BT1449709</a></td>
    <td>Possible TMM core under certain Client-SSL profile configurations</td></tr>
  <tr valign="top">
    <td><a href="#A1663073">1663073</a></td>
    <td>CVE-2024-24790</td>
    <td> <a href="https://my.f5.com/manage/s/article/K000141251" target="_blank">K000141251</a></td>
    <td>CVE-2024-24790 golang: net/netip: Unexpected behavior from Is methods for IPv4-mapped IPv6 addresses</td></tr>
  <tr valign="top">
    <td><a href="#A1662993">1662993</a></td>
    <td>CVE-2024-24790</td>
    <td> <a href="https://my.f5.com/manage/s/article/K000141251" target="_blank">K000141251</a></td>
    <td>CVE-2024-24790: golang: net/netip: Unexpected behavior from Is methods for IPv4-mapped IPv6 addresses</td></tr>
  <tr valign="top">
    <td><a href="#A1506949">1506949</a></td>
    <td>CVE-2024-0727</td>
    <td> <a href="https://my.f5.com/manage/s/article/K000138695" target="_blank">K000138695</a></td>
    <td>CVE-2024-0727 openssl: denial of service via null dereference</td></tr>
</table></div>
<br><font size="4"><u><strong>BIG-IP Next Fixes</strong></u></font><br><br>
<div style="background-color:#b3ecff"><table cellpadding="2" cellspacing="2" border="0" width="100%">
                    <tr valign="top">
                        <td width="10%"><u>ID Number</u></td>    <td width="10%"><u>Severity</u></td>    <td width="10%"><u>Links to More Info</u></td>
                    <td width="50%"><u>Description</u></td></tr>
  <tr valign="top">
    <td><a href="#A1630885">1630885</a></td>
    <td>0-Unspecified</td>
    <td></td>
    <td>CVE-2023-45142 - OpenTelemetry - potential memory exhaustion in otelhttp</td></tr>
  <tr valign="top">
    <td><a href="#A1671813">1671813</a></td>
    <td>1-Blocking</td>
    <td></td>
    <td>Upgrade shows complete at less then 100% progress bar</td></tr>
  <tr valign="top">
    <td><a href="#A1671721">1671721</a></td>
    <td>1-Blocking</td>
    <td></td>
    <td>Central Manager may hang permanently during the initial install process<span title="This issue may cause the configuration to fail to load or may significantly impact system performance after upgrade">&starf;</span></td></tr>
  <tr valign="top">
    <td><a href="#A1669641">1669641</a></td>
    <td>1-Blocking</td>
    <td></td>
    <td>Central Manager might not mount all filesystems properly during a reboot if external storage is configured<span title="This issue may cause the configuration to fail to load or may significantly impact system performance after upgrade">&starf;</span></td></tr>
  <tr valign="top">
    <td><a href="#A1634149">1634149</a></td>
    <td>1-Blocking</td>
    <td></td>
    <td>Node Validation improvements</td></tr>
  <tr valign="top">
    <td><a href="#A1630093">1630093</a></td>
    <td>1-Blocking</td>
    <td></td>
    <td>CM silently requires an internet connection when creating instances using the F5OS-based provider</td></tr>
  <tr valign="top">
    <td><a href="#A1612377">1612377</a></td>
    <td>1-Blocking</td>
    <td> <a href="https://my.f5.com/manage/s/article/K000140722" target="_blank">K000140722</a></td>
    <td>Central Manager cannot manage Provider if Provider certificate changes</td></tr>
  <tr valign="top">
    <td><a href="#A1611077">1611077</a></td>
    <td>1-Blocking</td>
    <td></td>
    <td>UI does not load after upgrading<span title="This issue may cause the configuration to fail to load or may significantly impact system performance after upgrade">&starf;</span></td></tr>
  <tr valign="top">
    <td><a href="#A1602141">1602141</a></td>
    <td>1-Blocking</td>
    <td></td>
    <td>Invalid certificates can disrupt configuration and status updates</td></tr>
  <tr valign="top">
    <td><a href="#A1601221">1601221</a></td>
    <td>1-Blocking</td>
    <td></td>
    <td>CM erroneously reports failover has failed during BIG-IP Next upgrade<span title="This issue may cause the configuration to fail to load or may significantly impact system performance after upgrade">&starf;</span></td></tr>
  <tr valign="top">
    <td><a href="#A1600445">1600445</a></td>
    <td>1-Blocking</td>
    <td></td>
    <td>Historic telemetry collected by BIG-IP Next Central Manager may be lost</td></tr>
  <tr valign="top">
    <td><a href="#A1599305">1599305</a></td>
    <td>1-Blocking</td>
    <td></td>
    <td>After upgrading, unable to edit the Central Manager part of policies attached to the applications<span title="This issue may cause the configuration to fail to load or may significantly impact system performance after upgrade">&starf;</span></td></tr>
  <tr valign="top">
    <td><a href="#A1597037">1597037</a></td>
    <td>1-Blocking</td>
    <td></td>
    <td>Adding a new TLS instance to an existing application (a default TLS instance) fails to flow traffic as expected</td></tr>
  <tr valign="top">
    <td><a href="#A1593605">1593605</a></td>
    <td>1-Blocking</td>
    <td></td>
    <td>HTTPS Traffic not working on BIG-IP Next HA formed from Central Manager with SSL Orchestrator topology</td></tr>
  <tr valign="top">
    <td><a href="#A1590065">1590065</a></td>
    <td>1-Blocking</td>
    <td></td>
    <td>The same gateway address is not considered as valid on multiple static routes</td></tr>
  <tr valign="top">
    <td><a href="#A1589069">1589069</a></td>
    <td>1-Blocking</td>
    <td></td>
    <td>AS3 application health status and alerts in the UI stay healthy and green, regardless of the application health</td></tr>
  <tr valign="top">
    <td><a href="#A1586501">1586501</a></td>
    <td>1-Blocking</td>
    <td> <a href="https://my.f5.com/manage/s/article/K000140380" target="_blank">K000140380</a></td>
    <td>Configuring external logger in Instance Log Management halts telemetry reception in Central Manager and other configured external loggers</td></tr>
  <tr valign="top">
    <td><a href="#A1585793">1585793</a></td>
    <td>1-Blocking</td>
    <td></td>
    <td>The f5-fsm-tmm crashes upon configuring BADOS under traffic</td></tr>
  <tr valign="top">
    <td><a href="#A1584753">1584753</a></td>
    <td>1-Blocking</td>
    <td> <a href="https://my.f5.com/manage/s/article/K000139851" target="_blank">K000139851</a></td>
    <td>TMM in BIG-IP Next expires the license after 50 days</td></tr>
  <tr valign="top">
    <td><a href="#A1576545-1">1576545-1</a></td>
    <td>1-Blocking</td>
    <td></td>
    <td>After upgrade, BIG-IP Next tenant os unable to export toda-otel (event logs) data to Cemtral Manager<span title="This issue may cause the configuration to fail to load or may significantly impact system performance after upgrade">&starf;</span></td></tr>
  <tr valign="top">
    <td><a href="#A1561053">1561053</a></td>
    <td>1-Blocking</td>
    <td></td>
    <td>Application status migration status incorrectly labeled as green when certain properties are removed</td></tr>
  <tr valign="top">
    <td><a href="#A1269733-6">1269733-6</a></td>
    <td>1-Blocking</td>
    <td><a href=http://cdn.f5.com/product/bugtracker/ID1269733.html target="_blank">BT1269733</a></td>
    <td>HTTP GET request with headers has incorrect flags causing timeout</td></tr>
  <tr valign="top">
    <td><a href="#A1670441">1670441</a></td>
    <td>2-Critical</td>
    <td></td>
    <td>Central Manager application migration tool is unable to receive very large UCS files</td></tr>
  <tr valign="top">
    <td><a href="#A1642165">1642165</a></td>
    <td>2-Critical</td>
    <td></td>
    <td>Central Manager could fail to onboard a BIG-IP Next instance even after setup appears complete</td></tr>
  <tr valign="top">
    <td><a href="#A1631197">1631197</a></td>
    <td>2-Critical</td>
    <td></td>
    <td>CVE-2024-41110 moby: Authz zero length regression</td></tr>
  <tr valign="top">
    <td><a href="#A1619945">1619945</a></td>
    <td>2-Critical</td>
    <td></td>
    <td>Boot time on KVM is excessively long when no management IP is assigned</td></tr>
  <tr valign="top">
    <td><a href="#A1612225">1612225</a></td>
    <td>2-Critical</td>
    <td></td>
    <td>Unable to Initiate BIG-IP Next Instance Upgrade</td></tr>
  <tr valign="top">
    <td><a href="#A1602697-3">1602697-3</a></td>
    <td>2-Critical</td>
    <td></td>
    <td>Full-proxy HTTP/2 may allow unconstrained buffering</td></tr>
  <tr valign="top">
    <td><a href="#A1602561">1602561</a></td>
    <td>2-Critical</td>
    <td></td>
    <td>Inspection services cannot be deployed when one of the instances managed by BIG-IP Next Central Manager is in unhealthy state</td></tr>
  <tr valign="top">
    <td><a href="#A1601949">1601949</a></td>
    <td>2-Critical</td>
    <td></td>
    <td>Moving a self IP from one VLAN to another VLAN across L1 networks may cause self IP unreachable</td></tr>
  <tr valign="top">
    <td><a href="#A1591209">1591209</a></td>
    <td>2-Critical</td>
    <td></td>
    <td>Unable to force re-authentication on IDP when BIG-IP Next is acting as SAML SP</td></tr>
  <tr valign="top">
    <td><a href="#A1587445">1587445</a></td>
    <td>2-Critical</td>
    <td></td>
    <td>WAF enforcer crash during handling of a specific HTTP POST request</td></tr>
  <tr valign="top">
    <td><a href="#A1587337">1587337</a></td>
    <td>2-Critical</td>
    <td></td>
    <td>HA cluster on CM UI could be unhealthy during standby upgrade<span title="This issue may cause the configuration to fail to load or may significantly impact system performance after upgrade">&starf;</span></td></tr>
  <tr valign="top">
    <td><a href="#A1584741">1584741</a></td>
    <td>2-Critical</td>
    <td></td>
    <td>In the Table commands in iRule, the subtable count command fails in BIG-IP Next 20.x</td></tr>
  <tr valign="top">
    <td><a href="#A1584681">1584681</a></td>
    <td>2-Critical</td>
    <td></td>
    <td>Application service creation fails if name contains &quot;fallback&quot;</td></tr>
  <tr valign="top">
    <td><a href="#A1584073-1">1584073-1</a></td>
    <td>2-Critical</td>
    <td></td>
    <td>WAF enforcer might crash when application is removed during handling traffic</td></tr>
  <tr valign="top">
    <td><a href="#A1580181">1580181</a></td>
    <td>2-Critical</td>
    <td></td>
    <td>When BIG-IP Next HA is created using CM, the spinner does not refresh</td></tr>
  <tr valign="top">
    <td><a href="#A1579365">1579365</a></td>
    <td>2-Critical</td>
    <td></td>
    <td>Unsupported nested properties are not underlined during application migration process</td></tr>
  <tr valign="top">
    <td><a href="#A1571993">1571993</a></td>
    <td>2-Critical</td>
    <td></td>
    <td>Access Session data is not cleared after TMM restart</td></tr>
  <tr valign="top">
    <td><a href="#A1564157">1564157</a></td>
    <td>2-Critical</td>
    <td></td>
    <td>BIG-IP Next Central Manager requires VELOS/rSeries systems to use an SSL certificate containing the host IP address in the CN or SANs list.<span title="This issue may cause the configuration to fail to load or may significantly impact system performance after upgrade">&starf;</span></td></tr>
  <tr valign="top">
    <td><a href="#A1560493">1560493</a></td>
    <td>2-Critical</td>
    <td></td>
    <td>Inaccurate Reflection of Selfip Prefix Length in TMM Statistics and &quot;ip addr&quot; Output</td></tr>
  <tr valign="top">
    <td><a href="#A1455677-3">1455677-3</a></td>
    <td>2-Critical</td>
    <td></td>
    <td>ACCESS Policy hardening</td></tr>
  <tr valign="top">
    <td><a href="#A1399137">1399137</a></td>
    <td>2-Critical</td>
    <td></td>
    <td>&quot;40001: bind: address already in use&quot; failure logs on BIG-IP Next HA setup</td></tr>
  <tr valign="top">
    <td><a href="#A1329853">1329853</a></td>
    <td>2-Critical</td>
    <td></td>
    <td>Application traffic is intermittent when more than one virtual server is configured</td></tr>
  <tr valign="top">
    <td><a href="#A1678537-1">1678537-1</a></td>
    <td>3-Major</td>
    <td></td>
    <td>CVE-2024-6232: reDOS vulnerability in Python tarball module can cause crash</td></tr>
  <tr valign="top">
    <td><a href="#A1671069">1671069</a></td>
    <td>3-Major</td>
    <td></td>
    <td>CVE-2024-6119: OpenSSL vulnerability</td></tr>
  <tr valign="top">
    <td><a href="#A1634109">1634109</a></td>
    <td>3-Major</td>
    <td></td>
    <td>Instance Creation Failed for rSeries 2k and 4k<span title="This issue may cause the configuration to fail to load or may significantly impact system performance after upgrade">&starf;</span></td></tr>
  <tr valign="top">
    <td><a href="#A1633977">1633977</a></td>
    <td>3-Major</td>
    <td></td>
    <td>On rSeries system, operations which involve reboot, might result in Tenant failure state<span title="This issue may cause the configuration to fail to load or may significantly impact system performance after upgrade">&starf;</span></td></tr>
  <tr valign="top">
    <td><a href="#A1630889">1630889</a></td>
    <td>3-Major</td>
    <td></td>
    <td>CVE-2023-45288 - golang: x/net/http2: unlimited number of CONTINUATION frames causes DoS</td></tr>
  <tr valign="top">
    <td><a href="#A1630877">1630877</a></td>
    <td>3-Major</td>
    <td></td>
    <td>CVE-2023-44487: Multiple HTTP/2 enabled web servers are vulnerable to a DDoS attack (Rapid Reset Attack)</td></tr>
  <tr valign="top">
    <td><a href="#A1629793">1629793</a></td>
    <td>3-Major</td>
    <td></td>
    <td>WebSocket messages do not arrive server when using waf policy</td></tr>
  <tr valign="top">
    <td><a href="#A1627337">1627337</a></td>
    <td>3-Major</td>
    <td></td>
    <td>Removed deprecated APIs</td></tr>
  <tr valign="top">
    <td><a href="#A1610997">1610997</a></td>
    <td>3-Major</td>
    <td></td>
    <td>CM Scale: waf-policy-builder in CrashLoopBackoff during WAF policies deployment</td></tr>
  <tr valign="top">
    <td><a href="#A1607837">1607837</a></td>
    <td>3-Major</td>
    <td></td>
    <td>BIG-IP Next Central Manager does not support NTP configuration via cloud-init</td></tr>
  <tr valign="top">
    <td><a href="#A1593381-1">1593381-1</a></td>
    <td>3-Major</td>
    <td></td>
    <td>When upgrade fails, release version displayed in GUI is different from CLI release version.<span title="This issue may cause the configuration to fail to load or may significantly impact system performance after upgrade">&starf;</span></td></tr>
  <tr valign="top">
    <td><a href="#A1592929">1592929</a></td>
    <td>3-Major</td>
    <td></td>
    <td>Attaching or detaching of an iRule version is not supported for AS3 application</td></tr>
  <tr valign="top">
    <td><a href="#A1592589">1592589</a></td>
    <td>3-Major</td>
    <td></td>
    <td>Suggestion details page for WAF policy with &quot;on-demand&quot; learning mode includes incorrect operations options</td></tr>
  <tr valign="top">
    <td><a href="#A1589577">1589577</a></td>
    <td>3-Major</td>
    <td></td>
    <td>When no token exists, LLM log writes &quot;LICENSING-1116:DecryptionFailed&quot;</td></tr>
  <tr valign="top">
    <td><a href="#A1587497-1">1587497-1</a></td>
    <td>3-Major</td>
    <td></td>
    <td>WAF security report shows alerted requests even though no alerts were generated</td></tr>
  <tr valign="top">
    <td><a href="#A1585773-1">1585773-1</a></td>
    <td>3-Major</td>
    <td></td>
    <td>Unable to migrate large number of applications at once</td></tr>
  <tr valign="top">
    <td><a href="#A1585285">1585285</a></td>
    <td>3-Major</td>
    <td></td>
    <td>Unable to stage applications for migration when session contains large number of application services</td></tr>
  <tr valign="top">
    <td><a href="#A1580545">1580545</a></td>
    <td>3-Major</td>
    <td></td>
    <td>iRule allows function local variable</td></tr>
  <tr valign="top">
    <td><a href="#A1569589">1569589</a></td>
    <td>3-Major</td>
    <td></td>
    <td>Default values of Access policy are not migrated</td></tr>
  <tr valign="top">
    <td><a href="#A1560473">1560473</a></td>
    <td>3-Major</td>
    <td></td>
    <td>Traffic won&#39;t work with http monitor for L3, http-transparent service</td></tr>
  <tr valign="top">
    <td><a href="#A1472669-1">1472669-1</a></td>
    <td>3-Major</td>
    <td></td>
    <td>idle timer in BIG-IP Next Central Manager can log out user during file uploads<span title="This issue may cause the configuration to fail to load or may significantly impact system performance after upgrade">&starf;</span></td></tr>
  <tr valign="top">
    <td><a href="#A1348837">1348837</a></td>
    <td>3-Major</td>
    <td></td>
    <td>Admin can delete their own account</td></tr>
  <tr valign="top">
    <td><a href="#A1348833">1348833</a></td>
    <td>3-Major</td>
    <td></td>
    <td>A cryptographically insecure pseudo-random number generator was used to create passwords during the reset process.</td></tr>
  <tr valign="top">
    <td><a href="#A1309265">1309265</a></td>
    <td>3-Major</td>
    <td></td>
    <td>CVE-2022-41723 golang.org/x/net vulnerable to Uncontrolled Resource Consumption</td></tr>
  <tr valign="top">
    <td><a href="#A1309257">1309257</a></td>
    <td>3-Major</td>
    <td></td>
    <td>CVE-2022-41715 potential golang regex DoS</td></tr>
  <tr valign="top">
    <td><a href="#A1308845">1308845</a></td>
    <td>3-Major</td>
    <td></td>
    <td>CVE-2022-46146 exporter-toolkit: authentication bypass via cache poisoning</td></tr>
  <tr valign="top">
    <td><a href="#A1251181">1251181</a></td>
    <td>3-Major</td>
    <td></td>
    <td>VLAN names longer than 15 characters can cause issues with troubleshooting</td></tr>
  <tr valign="top">
    <td><a href="#A1232521-6">1232521-6</a></td>
    <td>3-Major</td>
    <td></td>
    <td>SCTP connection sticking on BIG-IP even after connection terminated</td></tr>
  <tr valign="top">
    <td><a href="#A1572437">1572437</a></td>
    <td>4-Minor</td>
    <td></td>
    <td>CVE-2024-0450: : python: The zipfile module is vulnerable to zip-bombs leading to denial of service</td></tr>
  <tr valign="top">
    <td><a href="#A1531845">1531845</a></td>
    <td>4-Minor</td>
    <td></td>
    <td>CVE-2023-27043: python: Parsing errors in email/_parseaddr.py lead to incorrect value in email address part of tuple</td></tr>
  <tr valign="top">
    <td><a href="#A1516785">1516785</a></td>
    <td>4-Minor</td>
    <td></td>
    <td>CVE-2023-49081: aiohttp: HTTP request modification</td></tr>
  <tr valign="top">
    <td><a href="#A1509361">1509361</a></td>
    <td>4-Minor</td>
    <td></td>
    <td>CVE-2023-50782 python-cryptography: Bleichenbacher timing oracle attack against RSA decryption</td></tr>
  <tr valign="top">
    <td><a href="#A1507021">1507021</a></td>
    <td>4-Minor</td>
    <td></td>
    <td>CVE-2023-45803: urllib3: Request body not stripped after redirect</td></tr>
  <tr valign="top">
    <td><a href="#A1498489">1498489</a></td>
    <td>4-Minor</td>
    <td></td>
    <td>LDAP Bind Password not Re-populated in BIG-IP Next Central Manager GUI</td></tr>
  <tr valign="top">
    <td><a href="#A1490381">1490381</a></td>
    <td>4-Minor</td>
    <td></td>
    <td>Pagination for iRules page not supported with a large number of iRules</td></tr>
  <tr valign="top">
    <td><a href="#A1472337">1472337</a></td>
    <td>4-Minor</td>
    <td></td>
    <td>Missing object referenced in authenticationTrustCA</td></tr>
  <tr valign="top">
    <td><a href="#A1394625">1394625</a></td>
    <td>4-Minor</td>
    <td></td>
    <td>Application service failes to deploy even if marked as green (ready to deploy)</td></tr>
</table></div>
<br><h2><u>Cumulative fix details for BIG-IP Next v20.3.0 that are included in this release</u></h2>
<div style="background-color:#b3ecff"><h4><a name="A1678537-1" rel="nofollow"></a>1678537-1 : CVE-2024-6232: reDOS vulnerability in Python tarball module can cause crash</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>Regular expressions that allowed excessive backtracking during tarfile.TarFile header parsing are vulnerable to ReDoS via specifically-crafted UCS file.</p>
<p><strong>Conditions:</strong><br>Uses uploads UCS for migration.</p>
<p><strong>Impact:</strong><br>UCS load might crash or timeout.</p>
<p><strong>Workaround:</strong><br>Never upload untrusted UCS files.</p>
<p><strong>Fix:</strong><br>Python has been updated to a non-vulnerable version.</p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1671813" rel="nofollow"></a>1671813 : Upgrade shows complete at less then 100% progress bar</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>Upgrade in progress bar may incorrectly show that the upgrade is completed and allow users to close the progress bar.</p>
<p><strong>Conditions:</strong><br>Upgrade is from either 20.2.0 or 20.2.1 to 20.3.0 and CM is in HA mode (3 nodes).</p>
<p><strong>Impact:</strong><br>During a CM HA upgrade from 20.2.0 or 20.2.1 to 20.3.0, the &quot;BIG-IP NEXT CM upgrade in Progress&quot; dialog may incorrectly indicate that the upgrade is complete. This can allow users to close the progress bar and navigate the CM while the upgrade is still ongoing. However, users will soon be redirected back to the maintenance page as the upgrade continues.</p>
<p><strong>Workaround:</strong><br>Once the CM HA upgrade begins, wait until the progress bar reaches 100% before attempting to navigate the system.</p>
<p><strong>Fix:</strong><br>Upgrade progress bar correctly displays the status of upgrade</p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1671721" rel="nofollow"></a>1671721 : Central Manager may hang permanently during the initial install process<span title="This issue may cause the configuration to fail to load or may significantly impact system performance after upgrade">&starf;</span></h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>The CM install process can be initiated by clicking the button in the GUI after logging in for the first time. <br>
<br>
CM install can also be done via CLI:<br>
<br>
/opt/cm-bundle/cm install<br>
<br>
After that the system may hang during the install.<br>
<br>
To verify this specific defect, check the end of the /var/log/central-manager/central-manager-cli.log file and it will end here:<br>
<br>
Release &quot;kafka&quot; does not exist. Installing it now.<br>
NAME: kafka<br>
LAST DEPLOYED: Sat Aug  3 11:02:57 2024<br>
NAMESPACE: default<br>
STATUS: deployed<br>
REVISION: 1<br>
TEST SUITE: None<br>
2024-08-03T11:03:21+00:00 info: Installing central manager application resources...<br>
secret/cm-bootstrap-status patched<br>
Release &quot;mbiq&quot; does not exist. Installing it now.</p>
<p><strong>Conditions:</strong><br> - New Central Manager deployment<br>
- &quot;setup&quot; has been run<br>
- The CM install process has been started via web UI or CLI</p>
<p><strong>Impact:</strong><br>Central Manager install never completes.</p>
<p><strong>Workaround:</strong><br>There are two options:<br>
<br>
1. Delete the Central Manager instance and deploy a new one.<br>
<br>
2. At the CLI, run uninstall and install again:<br>
<br>
# /opt/cm-bundle/cm uninstall<br>
<br>
Wait for this to complete, then:<br>
<br>
/opt/cm-bundle/cm install</p>
<p><strong>Fix:</strong><br>Central Manager will not hang during install.</p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1671069" rel="nofollow"></a>1671069 : CVE-2024-6119: OpenSSL vulnerability</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>Applications performing certificate name checks (e.g., TLS clients checking server certificates) may attempt to read an invalid memory address resulting in abnormal termination of the application process.</p>
<p><strong>Conditions:</strong><br>Applications performing certificate name checks between an expected name and an `otherName` subject alternative name of an X.509 certificate.<br>
<br>
The FIPS modules in 3.3, 3.2, 3.1 and 3.0 are not affected by this issue.</p>
<p><strong>Impact:</strong><br>Denial of service can occur only when the application also specifies an expected DNS name, Email address or IP address.</p>
<p><strong>Workaround:</strong><br>NA</p>
<p><strong>Fix:</strong><br>Applied the patch for openssl CVE-2024-6119</p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1670441" rel="nofollow"></a>1670441 : Central Manager application migration tool is unable to receive very large UCS files</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>A UCS file will fail to upload to the Central Manager application migration manager when the file is very large.</p>
<p><strong>Conditions:</strong><br> - Upload UCS file to Central Manager<br>
- File is roughly over 1.4GB</p>
<p><strong>Impact:</strong><br>File will fail to upload.</p>
<p><strong>Workaround:</strong><br>UCS files don&#39;t need to be so large for the config migration. They can become very large due to archived epsec files.<br>
<br>
An administrator can untar the file on a linux system, remove the epsec files and tar back up again.<br>
<br>
Alternatively and more easily, follow the instructions here and gather a new UCS file:<br>
<br>
K21175584: Removing unnecessary OPSWAT EPSEC packages from the BIG-IP APM system</p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1669641" rel="nofollow"></a>1669641 : Central Manager might not mount all filesystems properly during a reboot if external storage is configured<span title="This issue may cause the configuration to fail to load or may significantly impact system performance after upgrade">&starf;</span></h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>The Central Manager VM might not properly mount all filesystems after a reboot when external storage is configured. The following symptoms have been observed in such cases:<br>
- Subsequent upgrade to the BIG-IP Next 20.3.0 release failed due to an error writing to the /tmp directory as the admin user.<br>
- Multiple pods remain stuck with the CreateContainerConfigError due to a missing volume.<br>
- One or both of the /opt/cm-backup and /opt/cm-qkview directories are no longer using external storage.</p>
<p><strong>Conditions:</strong><br>When External storage is configured and the BIG-IP Next Central Manager is rebooted.</p>
<p><strong>Impact:</strong><br>If the Central Manager VM failed to mount all filesystems properly after a reboot, future operations on the Central Manager would fail.</p>
<p><strong>Workaround:</strong><br>Before rebooting the system, including prior to upgrading, modify the /etc/fstab and /etc/fstab.external-storage files to add the _netdev mount option immediately after the nosuid mount option for the mounts of the /opt/cm-backup and /opt/cm-qkview directories, as shown below. This must be done on every node in a multi-node cluster. If the system has already been rebooted and is experiencing any of the documented symptoms, this workaround can be applied, followed by rebooting the VM to recover.<br>
<br>
$ cat /etc/fstab<br>
...<br>
10.1.1.1:/export/data /mnt/external-storage nfs auto,nofail,noatime,noexec,nosuid,nodev,nolock,tcp,actimeo=1800,retry=2,_netdev 0 0/mnt/external-storage/a05d57ec-2703-47b8-a742-ca1c2148ad2b/cm-backup /opt/cm-backup none bind,rw,auto,nouser,nodev,noatime,exec,nosuid,_netdev 0 0<br>
/mnt/external-storage/a05d57ec-2703-47b8-a742-ca1c2148ad2b/cm-qkview /opt/cm-qkview none bind,rw,auto,nouser,nodev,noatime,exec,nosuid,_netdev 0 0<br>
<br>
$ cat /etc/fstab.external-storage<br>
10.1.1.1:/export/data /mnt/external-storage nfs auto,nofail,noatime,noexec,nosuid,nodev,nolock,tcp,actimeo=1800,retry=2,_netdev 0 0<br>
/mnt/external-storage/7cdb4e2f-4c2f-42bf-b759-f106da06100d/cm-backup /opt/cm-backup none bind,rw,auto,nouser,nodev,noatime,exec,nosuid,_netdev 0 0<br>
/mnt/external-storage/7cdb4e2f-4c2f-42bf-b759-f106da06100d/cm-qkview /opt/cm-qkview none bind,rw,auto,nouser,nodev,noatime,exec,nosuid,_netdev 0 0</p>
<p><strong>Fix:</strong><br>The _netdev mount option will be automatically added during the upgrade to the BIG-IP Next 20.3.0 release. This new mount option will also be included in fresh installations starting with the BIG-IP Next 20.3.0 release.</p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1663073" rel="nofollow"></a>1663073 : CVE-2024-24790 golang: net/netip: Unexpected behavior from Is methods for IPv4-mapped IPv6 addresses</h4>
<p><strong>Links to More Info: </strong> <a href="https://my.f5.com/manage/s/article/K000141251" target="_blank">K000141251</a></p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1662993" rel="nofollow"></a>1662993 : CVE-2024-24790: golang: net/netip: Unexpected behavior from Is methods for IPv4-mapped IPv6 addresses</h4>
<p><strong>Links to More Info: </strong> <a href="https://my.f5.com/manage/s/article/K000141251" target="_blank">K000141251</a></p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1642165" rel="nofollow"></a>1642165 : Central Manager could fail to onboard a BIG-IP Next instance even after setup appears complete</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>When attempting to onboard a BIG-IP Next Instance that has been set up via console setup script, Central Manager may display an error &quot;DEVICE-0206 BIG-IP Next Instance Discovery error&quot; or &quot;DEVICE-0207 Failed to Configure Analytics Service&quot;</p>
<p><strong>Conditions:</strong><br>Installing a BIG-IP Next instance via the console setup script in VMWare or KVM.</p>
<p><strong>Impact:</strong><br>The BIG-IP Next instance is not able to be onboarded to Central Manager.</p>
<p><strong>Workaround:</strong><br>Use the Check Health API (/api/v1/health/ready) to check the Instance&#39;s Health Status until healthy status is received. This indicates the Instance is ready to be added to Central Manager.</p>
<p><strong>Fix:</strong><br>The setup script on the BIG-IP Next instance now insures that all elements of the system are ready for use before indicating that setup has completed successfully.</p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1634149" rel="nofollow"></a>1634149 : Node Validation improvements</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>Instability or crashes in the Central Manager Node&#39;s Kubernetes Service.</p>
<p><strong>Conditions:</strong><br>N/A</p>
<p><strong>Impact:</strong><br>The Kubernetes on the Central Manager node could crash and disrupt availability.</p>
<p><strong>Workaround:</strong><br>N/A</p>
<p><strong>Fix:</strong><br>Node validation improvements have been completed.</p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1634109" rel="nofollow"></a>1634109 : Instance Creation Failed for rSeries 2k and 4k<span title="This issue may cause the configuration to fail to load or may significantly impact system performance after upgrade">&starf;</span></h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>Trying to create a BIG-IP Next tenant on rSeries 2k and 4k fails.</p>
<p><strong>Conditions:</strong><br>Trying to create a BIG-IP Next tenant on rSeries 2k and 4k. <br>
<br>
The logs indicates that &quot;there are containers with unready status&quot;.</p>
<p><strong>Impact:</strong><br>Unable to create a BIG-IP Next instance. The tenant will not be deleted by Central Manager.</p>
<p><strong>Workaround:</strong><br>When creating instance in rSeries 2k and 4k, In the Instance Creation wizard, under the &quot;Troubleshooting&quot; tab, select &quot;1200&quot; in the &quot;Timeout (seconds)&quot; dropdown.</p>
<p><strong>Fix:</strong><br>Selecting the Timeout to be 1200 when creating instance from CM for rSeries 2k and 4k</p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1633977" rel="nofollow"></a>1633977 : On rSeries system, operations which involve reboot, might result in Tenant failure state<span title="This issue may cause the configuration to fail to load or may significantly impact system performance after upgrade">&starf;</span></h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>After reboot of the F5OS-A rSeries system in any operations (for example, live upgrade, reboot) with multiple tenants deployed, some or all of the tenants might not become operational. This is due to the vfio device problem. With this the tenant pods get into restarting loop and never comes up.<br>
<br>
The tenant pod state can be checked with the below command on the host system. <br>
<br>
[root@appliance-1:Active] vfio # kubectl get pods <br>
NAME                             READY   STATUS            RESTARTS   AGE<br>
f5-resource-manager-bpnrr        1/1     Running           0          3h<br>
virt-launcher-bigip-14-1-kz56l   1/1     Running           0          3h4m<br>
virt-launcher-bigip-19-1-5m72j   1/1     Running           0          3h4m<br>
virt-launcher-bigip-3-1-pn6c2    1/1     Running           0          3h4m<br>
virt-launcher-bigip-4-1-8x4cc    1/1     Running           0          3h4m<br>
virt-launcher-bigip-20-1-q99b7   1/1     Running           0          3h4m<br>
virt-launcher-bigip-5-1-vr4cf    1/1     Running           0          3h4m<br>
virt-launcher-bigip-18-1-zfrns   1/1     Running           0          162m<br>
virt-launcher-bigip-1-1-qhjd5    1/1     Terminating       0          4m8s<br>
virt-launcher-bigip-13-1-vjwwd   1/1     Terminating       0          3m19s<br>
virt-launcher-bigip-12-1-7swfq   0/1     Completed         0          87s<br>
virt-launcher-bigip-16-1-pqjx6   1/1     Running           0          43s<br>
virt-launcher-bigip-15-1-56x2g   0/1     PodInitializing   0          5s<br>
[root@appliance-1:Active] vfio #</p>
<p><strong>Conditions:</strong><br>WThe issue might occur in a live software upgrade or any situation that involves a reboot of the rSeries F5OS-A system with multiple tenants deployed.<br>
<br>
The below logs will be observed in issue occurring pod logs repeatedly for every retry of the vfio device access by qemu-kvm.<br>
<br>
[root@appliance-1:Active] # kubectl get pods,  this command shows the pod name. You can use the following command to see the log in the problem pod. Hash in the pod name changes for every restart of the pod.<br>
<br>
[root@appliance-1:Active] # kubectl logs &lt;&lt;Problem Pod name displayed in above command&gt;&gt;  | grep busy<br>
<br>
qemu-kvm: -device vfio-pci,host=0000:54:02.1,id=hostdev0,bus=pci.10,addr=0x0: vfio 0000:54:02.1: failed to open /dev/vfio/130: Device or resource busy</p>
<p><strong>Impact:</strong><br>Some or all of the vfio devices are the problem, which results in some or all tenants deployed on the rSeries host do not work as expected. They do not change to a RUNNING state.</p>
<p><strong>Workaround:</strong><br>As the vfio devices are in problem state, a reboot of appliance will resolve the issue.</p>
<p><strong>Fix:</strong><br>NA</p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1631197" rel="nofollow"></a>1631197 : CVE-2024-41110 moby: Authz zero length regression</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>A vulnerability was found in Authorization plugins in Docker Engine (AuthZ). Using a specially-crafted API request, an Engine API client could make the daemon forward a request or response to an authorization plugin without the body. In certain circumstances, the authorization plugin may allow a request that it would have otherwise denied if the body had been forwarded to it.</p>
<p><strong>Impact:</strong><br>While vulnerable code is present, it is not exposed in default, recommended, or standard configurations.</p>
<p><strong>Fix:</strong><br>The logcli package is no longer included in the system.</p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1630889" rel="nofollow"></a>1630889 : CVE-2023-45288 - golang: x/net/http2: unlimited number of CONTINUATION frames causes DoS</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>A vulnerability was discovered with the implementation of the HTTP/2 protocol in the Go programming language. There were insufficient limitations on the amount of CONTINUATION frames sent within a single stream. An attacker could potentially exploit this to cause a Denial of Service (DoS) attack.</p>
<p><strong>Impact:</strong><br>While vulnerable code is present, it is not exposed in default, recommended, or standard configurations.</p>
<p><strong>Fix:</strong><br>The logcli package is no longer included in the system.</p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1630885" rel="nofollow"></a>1630885 : CVE-2023-45142 - OpenTelemetry - potential memory exhaustion in otelhttp</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>A memory leak was found in the otelhttp handler of open-telemetry. This flaw allows a remote, unauthenticated attacker to exhaust the server&#39;s memory by sending many malicious requests, affecting the availability.</p>
<p><strong>Impact:</strong><br>While vulnerable code is present, it is not exposed in default, recommended, or standard configurations.</p>
<p><strong>Fix:</strong><br>The logcli package is no longer included in the system.</p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1630877" rel="nofollow"></a>1630877 : CVE-2023-44487: Multiple HTTP/2 enabled web servers are vulnerable to a DDoS attack (Rapid Reset Attack)</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>The HTTP/2 protocol allows a denial of service (server resource consumption) because request cancellation can reset many streams quickly.</p>
<p><strong>Impact:</strong><br>While vulnerable code is present, it is not exposed in default, recommended, or standard configurations.</p>
<p><strong>Fix:</strong><br>The logcli package is no longer included in the system.</p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1630093" rel="nofollow"></a>1630093 : CM silently requires an internet connection when creating instances using the F5OS-based provider</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>In CM internet-disconnected environment, creating F5OS based instance from CM will fail due to CM tries to connect to the phonehome URL which requires internet connectivity.</p>
<p><strong>Conditions:</strong><br> -- Creating a new BIG-IP Next instance on VELOS<br>
-- Central Manager does not have access to the Internet</p>
<p><strong>Impact:</strong><br>You cannot create an instance on F5OS from CM when the CM does not have any internet connectivity.</p>
<p><strong>Workaround:</strong><br>Create the F5OS based instance directly in the F5OS provider, and discover it to CM.</p>
<p><strong>Fix:</strong><br>CM that has no internet-connectivity can now create F5OS-based instance</p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1629793" rel="nofollow"></a>1629793 : WebSocket messages do not arrive server when using waf policy</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>Websocket messages are stalled in bigip-next and do not arrive to the server, when WAF policy is attached to the application.</p>
<p><strong>Conditions:</strong><br> -- WAF policy is attached to the application.<br>
-- Websocket traffic is sent.</p>
<p><strong>Impact:</strong><br>Handshake seems successful, but websocket messages do not arrive to server.</p>
<p><strong>Workaround:</strong><br>None.</p>
<p><strong>Fix:</strong><br>Bypass websocket messages by WAF when there is no websocket configuration.</p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1627337" rel="nofollow"></a>1627337 : Removed deprecated APIs</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>Several WAF APIs are deprecated and are now removed:<br>
<br>
GET /api/v1/security/filetype-violations<br>
<br>
GET /api/v1/spaces/default/security/waf-policies/{id}/filetype-violations<br>
<br>
GET /api/v1/spaces/default/security/waf-policies/{id}/filetype-violations/{viol_id}<br>
<br>
PUT /api/v1/spaces/default/security/waf-policies/{id}/filetype-violations/{viol_id}<br>
<br>
POST /api/v1/spaces/default/security/waf-policies/{id}/filetype-violations/update</p>
<p><strong>Conditions:</strong><br>Using the iControl REST API</p>
<p><strong>Impact:</strong><br>You will need to use new API endpoints that replaced the deprecated API endpoints</p>
<p><strong>Workaround:</strong><br>The following API endpoints should be used instead of the old ones:<br>
<br>
Old API: /api/v1/spaces/default/security/waf-policies/{id}/filetype-violations<br>
New API: /api/v1/spaces/default/security/waf-policies/{id}/violations<br>
<br>
<br>
Old API: /api/v1/spaces/default/security/waf-policies/{id}/filetype-violations/update<br>
New API: /api/v1/spaces/default/security/waf-policies/{id}/violations/update<br>
<br>
Old API: /api/v1/spaces/default/security/waf-policies/{id}/filetype-violations/{viol_id}<br>
New API: /api/v1/spaces/default/security/waf-policies/{id}/violations/{viol_id}<br>
<br>
Old API: /api/v1/security/filetype-violations<br>
New API: /api/v1/security/violations</p>
<p><strong>Fix:</strong><br>Deprecated APIs removed.</p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1619945" rel="nofollow"></a>1619945 : Boot time on KVM is excessively long when no management IP is assigned</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>The BIG-IP Next VM takes 15 to 20 minutes to boot.</p>
<p><strong>Conditions:</strong><br>Attempting to create a KVM Next instance without a management IP and no cloud-init datasource is provided.</p>
<p><strong>Impact:</strong><br>The BIG-IP Next instance will boot in about 15 to 20 minutes.</p>
<p><strong>Workaround:</strong><br>If the BIG-IP Next instance is created without using DHCP or cloud-init to assign the management IP on KVM, download and use the kvm-boot.iso to deploy your KVM instance.<br>
This will speed-up the boot time.<br>
<br>
You can find a link to kvm-boot.iso at https://clouddocs.f5.com/bigip-next/latest/install/next_install_kvm_setup.html</p>
<p><strong>Fix:</strong><br>No cloud and without using DHCP on KVM, must download and use the kvm-boot.iso.</p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1612377" rel="nofollow"></a>1612377 : Central Manager cannot manage Provider if Provider certificate changes</h4>
<p><strong>Links to More Info: </strong> <a href="https://my.f5.com/manage/s/article/K000140722" target="_blank">K000140722</a></p>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>Central Manager will no longer be able to manage Next instances with a Provider if the Provider certificate changes.</p>
<p><strong>Conditions:</strong><br> -- Central Manager has a Provider configured.<br>
-- The Provider certificate changes.<br>
-- Central Manager can no longer manage Next instance states via the Provider.</p>
<p><strong>Impact:</strong><br>Central Manager cannot deploy or delete any instance under that Provider.</p>
<p><strong>Workaround:</strong><br>Mitigation: In pre-orange release, do not change the provider certificate once the provider has been managed by CM.<br>
<br>
Workaround: If provider certificate has been changed in pre-orange release while the provider is managed under CM, update the certificate to be exactly the original certificate.</p>
<p><strong>Fix:</strong><br>Re-trusting the provider certificate in CM has been fixed.</p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1612225" rel="nofollow"></a>1612225 : Unable to Initiate BIG-IP Next Instance Upgrade</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>The BIG-IP Next Central Manager UI sends an API request to the BIG-IP Next instance to retrieve the file ID of the uploaded upgrade bundle. If the API request encounters any file entries without a set &#39;fileName&#39; attribute, the UI will display the following error message: &quot;Failed to initialize upgrade process: Failed to initialize upgrade form: Failed to fetch instance files: Cannot read properties of undefined (reading &#39;endsWith&#39;).&quot;</p>
<p><strong>Conditions:</strong><br>BIG-IP Next instance upgrade attempted via the BIG-IP Next Central Manager UI when files with no &#39;fileName&#39; attribute exist on the BIG-IP Next instance.</p>
<p><strong>Impact:</strong><br>The BIG-IP Next Central Manager UI for initiating the BIG-IP Next instance upgrade will fail to load properly.</p>
<p><strong>Workaround:</strong><br>The following steps must be followed using the BIG-IP Next Central Manager API:<br>
1. Use the &#39;POST /api/login&#39; endpoint to obtain a token.<br>
2. Use the &#39;GET api/v1/spaces/default/instances&#39; endpoint to  identify the instance ID exhibiting this issue.<br>
3. Use the &#39;GET api/device/v1/proxy/&lt;instance ID&gt;?path=/files&#39; endpoint to identify the list of files for the instance that do not contain a &#39;fileName&#39; attribute.<br>
4. Use the &#39;DELETE api/device/v1/proxy/&lt;instance ID&gt;?path=/files/&lt;file ID&gt;&#39; endpoint to delete each identified file.</p>
<p><strong>Fix:</strong><br>Delete all file entries from each BIG-IP Next instance that do not have a &#39;fileName&#39; attribute.</p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1611077" rel="nofollow"></a>1611077 : UI does not load after upgrading<span title="This issue may cause the configuration to fail to load or may significantly impact system performance after upgrade">&starf;</span></h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>After upgrading Central Manager, you are logged out of the UI and when you attempt to reconnect you do not get the CM login page, but rather get an NGINX 404 error page.</p>
<p><strong>Conditions:</strong><br>Upgrading Central Manager from 20.1.0 to 20.2.0</p>
<p><strong>Impact:</strong><br>Central Manager upgrade fails and Central Manager won&#39;t start.</p>
<p><strong>Workaround:</strong><br>Perform a restore operation on a new machine(s).</p>
<p><strong>Fix:</strong><br>After upgrading BIG-IP Next Central Manager, you are no longer logged out of the UI.</p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1610997" rel="nofollow"></a>1610997 : CM Scale: waf-policy-builder in CrashLoopBackoff during WAF policies deployment</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>There are two issues:<br>
1. The CPU utilization of waf-policy-builder was high (exceeded K8s limit).<br>
2. After 1k WAF policies deployment, it was observed that waf-policy-builder pod was in CrashLoopBackoff after multiple restarts. - This might be a duplication of https://bugzilla.olympus.f5net.com/show_bug.cgi?id=1677333</p>
<p><strong>Conditions:</strong><br>1. The CPU utilization of waf-policy-builder occurs as soon as the system starts.<br>
2. The CrashLoopBackoff might be due to waf-policy-builder exceeding K8s memory limit following multiple policies deployment.</p>
<p><strong>Impact:</strong><br>1. The high CPU should not cause a crash but a decreased performance.<br>
2. CrashLoopBackoff is a fatal error for waf-policy-builder pod.<br>
<br>
The affected module is waf-policy-builder in CM and the implications of that.</p>
<p><strong>Workaround:</strong><br>The workaround is partial, it relates only to the crash. <br>
<br>
You might be able to mitigate the crash by reducing the number of policies deployed (if applicable in the configuration). Waf-policy-builder pod should then be manually deleted (using kubectl). This is in order to clear the CrashLoopBackoff of waf-policy-builder and to start with the new configuration.</p>
<p><strong>Fix:</strong><br>The first of the two issues was fixed: The read of the `kafka_poll_interval_ms` value from the configuration was fixed. The wrong value read caused to a high CPU of waf-policy-builder.</p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1607837" rel="nofollow"></a>1607837 : BIG-IP Next Central Manager does not support NTP configuration via cloud-init</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>If you supply cloud-init user-data that specifies NTP pools and or sources, chrony will not be configured to use that data.</p>
<p><strong>Conditions:</strong><br>Cloud-init user data with NTP configuration supplied when first booting BIG-IP Next Central Manager.</p>
<p><strong>Impact:</strong><br>Custom NTP sources must be configured via the setup utility instead of via cloud-init.</p>
<p><strong>Workaround:</strong><br>Either run the setup utility to configure the custom NTP server IP addresses or modify the /etc/chrony/sources.d/central-manager.sources file to contain the sources being advertised by DHCP.</p>
<p><strong>Fix:</strong><br>If cloud-init user-data with pools or servers defined for the cloud-init NTP module is supplied when the BIG-IP Next Central Manager is first booted, it will now correctly configure chrony to use those sources.</p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1602697-3" rel="nofollow"></a>1602697-3 : Full-proxy HTTP/2 may allow unconstrained buffering</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>tmm crashes and restarts due to memory pressure</p>
<p><strong>Conditions:</strong><br>When using HTTP2 Full proxy configuration, under certain conditions, tmm restarts.</p>
<p><strong>Impact:</strong><br>Traffic disrupted while tmm restarts.</p>
<p><strong>Workaround:</strong><br>NA</p>
<p><strong>Fix:</strong><br>No unconstrained buffering is seen after the fix</p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1602561" rel="nofollow"></a>1602561 : Inspection services cannot be deployed when one of the instances managed by BIG-IP Next Central Manager is in unhealthy state</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>Inspections services cannot be deployed to the instances using UI.</p>
<p><strong>Conditions:</strong><br>One of the three instances managed by BIG-IP Next Central Manager is in unknown state.</p>
<p><strong>Impact:</strong><br>You won&#39;t be able to deploy inspection services.</p>
<p><strong>Workaround:</strong><br>1. Use Central Manager API to deploy on healthy instances.<br>
or<br>
2. Fix the state of the instance that is in the unknown state.</p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1602141" rel="nofollow"></a>1602141 : Invalid certificates can disrupt configuration and status updates</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>A virtual address with RHI configuration marked as Never may be advertised over BGP.</p>
<p><strong>Conditions:</strong><br>There are multiple virtual servers with the same virtual address and RHI configuration is marked as Never, and RHI configuration is created before the application or stack is created.</p>
<p><strong>Impact:</strong><br>A virtual address that should not be advertised is advertised through BGP.</p>
<p><strong>Workaround:</strong><br>Create the RHI configuration for Never after the application or stack is configured.</p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1601949" rel="nofollow"></a>1601949 : Moving a self IP from one VLAN to another VLAN across L1 networks may cause self IP unreachable</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br> -- Ping to the self IP fails after assigning it to a different VLAN<br>
-- The Self IP address might not exist in the kernel</p>
<p><strong>Conditions:</strong><br> -- Two different L1 networks configured<br>
-- Two VLANs configured each on different L1 networks<br>
-- A Self IP is moved from one VLAN to another VLAN</p>
<p><strong>Impact:</strong><br>Traffic drop to the virtual servers/pools using the underlying self IP</p>
<p><strong>Workaround:</strong><br>Remove VLAN/Self IP and re-add it</p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1601221" rel="nofollow"></a>1601221 : CM erroneously reports failover has failed during BIG-IP Next upgrade<span title="This issue may cause the configuration to fail to load or may significantly impact system performance after upgrade">&starf;</span></h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>After the first node of an HA pair has been upgraded,  failover is triggered, either automatically by CM or manually by the user, and CM reports failover has failed due to &quot;401 Failed to authenticate.&quot; even though failover has occurred.</p>
<p><strong>Conditions:</strong><br> -- BIG-IP Next HA pair is upgraded using CM.<br>
-- VMware environment</p>
<p><strong>Impact:</strong><br>CM shows BIG-IP Next HA status as Unhealthy though the actual BIG-IP Next status is healthy.</p>
<p><strong>Workaround:</strong><br>1. Open the properties drawer for the instance and go to the HA section.<br>
2. Confirm that the nodes have swapped roles in the cluster. The new active should be at the upgraded version and the standby should be at the older version.  <br>
&nbsp;&nbsp;2.1 The cluster health API could also be used here via postman to confirm that failover has finished.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;GET https://{{CM-address}}/api/v1/spaces/default/instances/{{Big-IP-Next-ID}}/health<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The response should show the nodes have swapped roles and one is ACTIVE and the other is STANDBY.<br>
3. Disable the toggle for &quot;Enable automatic failover&quot; and click upgrade for the standby node and follow normal upgrade workflow steps.<br>
4. When upgrade has finished the HA instance in the Instance list grid will show the upgraded version and cluster will be healthy.</p>
<p><strong>Fix:</strong><br>None</p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1600445" rel="nofollow"></a>1600445 : Historic telemetry collected by BIG-IP Next Central Manager may be lost</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>If one of the BIG-IP Next Central Manager high availability (HA) nodes become unavailable, the BIG-IP Next instance telemetry may no longer be available through the BIG-IP Next Central Manager.</p>
<p><strong>Conditions:</strong><br>Any of the BIG-IP Next Central Manager high availability (HA) nodes become unavailable.</p>
<p><strong>Impact:</strong><br>Historic BIG-IP Next instance telemetry may no longer be available through BIG-IP Next Central Manager. Once the node is restored, or replaced by a new node, BIG-IP Next Central Manager will start collecting and presenting telemetry again.</p>
<p><strong>Workaround:</strong><br>None</p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1599305" rel="nofollow"></a>1599305 : After upgrading, unable to edit the Central Manager part of policies attached to the applications<span title="This issue may cause the configuration to fail to load or may significantly impact system performance after upgrade">&starf;</span></h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>Before upgrade, if there are applications attached with WAF policies, then after the upgrade, parts of the policies are not editable until the application is re-deployed.</p>
<p><strong>Conditions:</strong><br>Applications attached with WAF policies exist before the upgrade.</p>
<p><strong>Impact:</strong><br>Unable to edit part of the WAF policies that are attached to applications before upgrade.</p>
<p><strong>Workaround:</strong><br>Re-deploy the application to edit the policies.</p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1597037" rel="nofollow"></a>1597037 : Adding a new TLS instance to an existing application (a default TLS instance) fails to flow traffic as expected</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>Traffic flow does not work as expected when a new TLS instance is added to an existing application.</p>
<p><strong>Conditions:</strong><br>1. Create default SSL certificate and custom certificate from Central Manager UI.<br>
<br>
2. Deploy an https application and validate LTM traffic with default certificate.<br>
<br>
Edit the application to add new certificate for TLS instance under protocols and profiles.  <br>
<br>
4. Add the imported certificate (custom cert) using &quot;enable https client side&quot;<br>
<br>
5. Save the application with new TLS settings and certificate added.<br>
<br>
6. Click on Review and deploy. <br>
<br>
7. Validate the changes done on application.<br>
<br>
8. If validation is successful. Click on deploy application</p>
<p><strong>Impact:</strong><br>Traffic flow does not work as expected</p>
<p><strong>Workaround:</strong><br>Suggested Workarounds:<br>
<br>
1. Delete the existing cert in the UI and recreate the same certificate (either before or after adding new certificate) and save the application.<br>
2. Use API with multiCerts to true for each certificate block.</p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1593605" rel="nofollow"></a>1593605 : HTTPS Traffic not working on BIG-IP Next HA formed from Central Manager with SSL Orchestrator topology</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>HTTPS traffic is not working.</p>
<p><strong>Conditions:</strong><br>BIG-IP Next HA Setup with SSL Orchestrator Provisioned</p>
<p><strong>Impact:</strong><br>User can experience traffic downtimes, if instance gets down during upgrade or due to network interruption issues.</p>
<p><strong>Workaround:</strong><br>None</p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1593381-1" rel="nofollow"></a>1593381-1 : When upgrade fails, release version displayed in GUI is different from CLI release version.<span title="This issue may cause the configuration to fail to load or may significantly impact system performance after upgrade">&starf;</span></h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>During a CM upgrade, if the upgrade fails (for example, in an air-gap environment where the CM is disconnected from the internet), the version displayed in the GUI is different from the version shown in the CLI. Typically, the GUI retains the current version, while the CLI shows the target version that failed to upgrade. This discrepancy in version causes confusion about whether the upgrade was successful or not.</p>
<p><strong>Conditions:</strong><br>Upgrade CM in an air-gapped environment.</p>
<p><strong>Impact:</strong><br>CM becomes dysfunctional, the failed upgrade leads to discrepancies in version representation, where the CM GUI  displays the current version while the CLI indicates the target version.</p>
<p><strong>Workaround:</strong><br>If upgrade has failed due to a ephemeral condition(ie, pod startup timeout), user should restore CM to a previous version backup and retry upgrade to version of 20.2.1. Refer How to: Back up and restore BIG-IP Next Central Manager (https://clouddocs.f5.com/bigip-next/20-2-0/use_cm/cm_backup_restore_using_ui_api.html).<br>
<br>
if upgrading from 20.0.2-&gt;20.2.0 caused the failure and there is no backup, perform the following steps on your CLI to create a backup and restore back to 20.0.2:<br>
helm rollback mbiq-vault 1 (will fail)<br>
helm rollback mbiq-vault 1 (succeeds)<br>
kubectl get statefulset mbiq-vault -o yaml &gt; vault.yaml<br>
vi vault.yaml #delete the line that reads &quot;value: https://$(HOSTNAME).mbiq-vault-internal.default.svc.cluster.local:8200&quot;<br>
kubectl delete statefulset mbiq-vault<br>
kubectl apply -f vault.yaml<br>
#wait for vault to come back up<br>
/opt/cm-bundle/cm backup<br>
&nbsp;<br>
#create new 20.0.2 CM<br>
#scp backup file to CM<br>
#restore using backup</p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1592929" rel="nofollow"></a>1592929 : Attaching or detaching of an iRule version is not supported for AS3 application</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>In Central Manager, from iRule space, attaching or detaching  of a different iRule version of deployed AS3 application is not supported, it is supported only for FAST applications.</p>
<p><strong>Conditions:</strong><br> - Migrating the AS3 application which is having an iRule<br>
- Attach or detach a different iRule version from iRule space</p>
<p><strong>Impact:</strong><br>Unable to deploy the application with different iRule version.</p>
<p><strong>Workaround:</strong><br>Redeploy the application with new iRule version by directly editing the AS3 declaration from application space. <br>
Following is an example:<br>
<br>
&nbsp;&nbsp;&quot;iRules&quot;: [<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;cm&quot;: &quot;migrated_myfakeiRule2::v2&quot;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;],</p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1592589" rel="nofollow"></a>1592589 : Suggestion details page for WAF policy with &quot;on-demand&quot; learning mode includes incorrect operations options</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>For event logs associated with WAF policy which has &quot;on-demand&quot; learning mode - its suggestion details can include operations like Ignore and Delete, which should not be available in that case. Only Accept operation should be visible and available.</p>
<p><strong>Conditions:</strong><br>1. Configure WAF policy with &quot;On-demand&quot; learning mode and application attached to BIG-IP Next instance.<br>
2. Run traffic which produces suggestion(s).<br>
3. For the event of this traffic, open its details and press &quot;Accept Request&quot; button.<br>
4. A table with at least one suggestion should appear. Click on on of the suggestions.<br>
5. The suggestion details page appear with Ignore and Delete options.</p>
<p><strong>Impact:</strong><br>You cannot &quot;undo&quot; the operations in this scenario, the Accept Request case aims to let you perform the &quot;Accept&quot; operation only.</p>
<p><strong>Workaround:</strong><br>Don&#39;t use the Ignore or Delete operations in suggestion details shown via Event logs details.</p>
<p><strong>Fix:</strong><br>Delete and Ignore operations for Accept Request case are not visible.</p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1591209" rel="nofollow"></a>1591209 : Unable to force re-authentication on IDP when BIG-IP Next is acting as SAML SP</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>When BIG-IP Next is configured as a SAML SP with force authentication enabled in the SAML Auth item, IDP still does not re-authenticate the user when trying to access the SP.</p>
<p><strong>Conditions:</strong><br>Issue is observed for all usecases where force authentication is enabled in SAML Auth item.</p>
<p><strong>Impact:</strong><br>User in not re-authenticated while trying to access the SP, even though the admin configured the SP to force re-authentication.</p>
<p><strong>Workaround:</strong><br>None</p>
<p><strong>Fix:</strong><br>Not available yet</p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1590065" rel="nofollow"></a>1590065 : The same gateway address is not considered as valid on multiple static routes</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>When configured with multiple static routes with the same gateway IP address as mentioned below, the BIG-IP Next instance considers the first static route and does not configure the remaining static routes.<br>
<br>
- destination prefix 192.17.17.17/24 with gateway IP 198.2.1.1<br>
- destination prefix 192.18.18.18/24 with gateway IP 198.2.1.1</p>
<p><strong>Conditions:</strong><br>Multiple static routes with same gateway IP address.</p>
<p><strong>Impact:</strong><br>Unable to configure multiple static routes with same gateway IP address.</p>
<p><strong>Workaround:</strong><br>Change the environment variable &#39;DPVD_NETWORK_VALIDATOR_ENABLE &#39; to False from True, following is an example command:<br>
<br>
sudo kubectl edit deploy f5-fsm-tmm</p>
<p><strong>Fix:</strong><br>The same gateway IP address can be used on multiple static routes.</p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1589577" rel="nofollow"></a>1589577 : When no token exists, LLM log writes &quot;LICENSING-1116:DecryptionFailed&quot;</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>The LLM log will write error level messages as so:<br>
<br>
[ERRO] token/crypter.go:57 JWT decryption failed. Error: LICENSING-1116:DecryptionFailed:&#39;zero token&#39; text is too short to decrypt<br>
[ERRO] Error while decrypting token - zero token error - LICENSING-1116:DecryptionFailed:&#39;zero token&#39; text is too short to decrypt<br>
<br>
This message is benign and can be ignored.</p>
<p><strong>Conditions:</strong><br>Visiting the licensing page on Central Manager, when no token is setup.</p>
<p><strong>Impact:</strong><br>There is no functional impact.</p>
<p><strong>Workaround:</strong><br>Ignore this log message under the described condition.</p>
<p><strong>Fix:</strong><br>Empty token does not cause LLM to write error logs.</p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1589069" rel="nofollow"></a>1589069 : AS3 application health status and alerts in the UI stay healthy and green, regardless of the application health</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>The health status and alerts for AS3 applications are not showing as expected in the UI. Regardless of the status, the AS3 application health always shows as healthy.</p>
<p><strong>Conditions:</strong><br>An AS3 application monitored by BIG-IP Next Central Manager encounters a health issue.</p>
<p><strong>Impact:</strong><br>The application health and alert status remains green and healthy in the UI, not reflecting the correct state.</p>
<p><strong>Workaround:</strong><br>None</p>
<p><strong>Fix:</strong><br>The BIG-IP Next Central Manager UI now correctly reflects the health and alert status of AS3 applications.</p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1587497-1" rel="nofollow"></a>1587497-1 : WAF security report shows alerted requests even though no alerts were generated</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>When creating a security report, the generated report might show alerts, even though none were reported in the WAF dashboards and event log.</p>
<p><strong>Conditions:</strong><br>Generate a security report for a policy that is blocking traffic.</p>
<p><strong>Impact:</strong><br>The generated report might incorrectly show blocked requests as alerts even though no alerts were reported.</p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1587445" rel="nofollow"></a>1587445 : WAF enforcer crash during handling of a specific HTTP POST request</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>WAF enforcer crashes while handling a very large POST request.</p>
<p><strong>Conditions:</strong><br>A very large POST request received by enforcer.</p>
<p><strong>Impact:</strong><br>Enforcer crashes. Traffic disrupted while waf-enforcer restarts.</p>
<p><strong>Workaround:</strong><br>None</p>
<p><strong>Fix:</strong><br>Fixed handling of the request.</p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1587337" rel="nofollow"></a>1587337 : HA cluster on CM UI could be unhealthy during standby upgrade<span title="This issue may cause the configuration to fail to load or may significantly impact system performance after upgrade">&starf;</span></h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>This is an intermittent issue due to a race condition during HA cluster creation and standby writing self-signed certificates in standby vault.<br>
<br>
Following is an expected HA workflow steps:<br>
	1. Two BIG-IP Next instances (instance-1 and Instance-2) boot up as standalone on BIG-IP Next 20.2.0 image.<br>
	2. Both instances create and store self-signed certificates in vault DB.<br>
	3. HA cluster creation job is initiated.<br>
	4. Active instance creates self-signed certificates for new cluster IP and updates vault DB.<br>
	5. Standby instance creates self-signed certificates for new cluster IP and updates vault database.<br>
	6. During HA cluster join and database sync, active DB replaces standby DB.<br>
<br>
<br>
From the above steps, if Step.5 occurs before Step.6, then HA cluster goes into unknown state.<br>
<br>
If Step.5 occurs after Step.6, then HA cluster is healthy and upgrade works fine as expected.</p>
<p><strong>Conditions:</strong><br>After creating BIG-IP Next cluster, upgrade the version on standby.</p>
<p><strong>Impact:</strong><br>BIG-IP Next HA cluster is unreachable from CM.</p>
<p><strong>Workaround:</strong><br>During HA upgrades, if standby node is not reachable, follow below steps:<br>
<br>
&nbsp;&nbsp;&nbsp;&nbsp;1. Disable &quot;enable automatic failover&quot; flag and Force failover.<br>
&nbsp;&nbsp;&nbsp;&nbsp;2. On CM UI, click on the HA cluster name-&gt; certificates-&gt; Establish Trust. HA status on CM UI changes from Unknown to Unhealthy.<br>
&nbsp;&nbsp;&nbsp;&nbsp;3. Upgrade new standby instance to BIG-IP Next 20.2.1.<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Both active and standby should be on BIG-IP Next 20.2.1 and HA should be healthy in CM UI.</p>
<p><strong>Fix:</strong><br>After upgrades, HA cluster should always come-up in Healthy state.</p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1586501" rel="nofollow"></a>1586501 : Configuring external logger in Instance Log Management halts telemetry reception in Central Manager and other configured external loggers</h4>
<p><strong>Links to More Info: </strong> <a href="https://my.f5.com/manage/s/article/K000140380" target="_blank">K000140380</a></p>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>When you go to Instance &gt; Log Management in BIG-IP Next Central Manager and set up an external logger, the Central Manager will no longer receive any telemetry while the configured external logger will get all of the available logs from the BIG-IP Next instance. When new external loggers are configured, only the last will receive any telemetry, while all previously configured external loggers will stop receiving telemetry.</p>
<p><strong>Conditions:</strong><br>Configure external logger for the BIG-IP Next instance.</p>
<p><strong>Impact:</strong><br>Configured Central Manager stops receiving telemetry. Other external loggers stop receiving telemetry too.</p>
<p><strong>Workaround:</strong><br>F5 offers two bash scripts that you can run via the Central Manager CLI, one to identify impacted instances (find_broken_telemetry_instances.sh) and a second script to fix telemetry streaming to Central Manager (update_cm_logger.sh). There is no workaround for allowing streaming telemetry to Central Manager and external loggers without the fix.</p>
<p><strong>Fix:</strong><br>The creation or deletion of external loggers no longer interferes with other loggers, including telemetry. Although the certificates for the external loggers are visible in the &ldquo;Certificates &amp; Keys&rdquo; screen, they cannot be deleted or updated from there.</p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1585793" rel="nofollow"></a>1585793 : The f5-fsm-tmm crashes upon configuring BADOS under traffic</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>The f5-fsm-tmm crashes.</p>
<p><strong>Conditions:</strong><br>Deploy BIG-IP Next WAF and perform external IP vulnerability scan.<br>
<br>
Configure BADOS while traffic is running to the WAF application service.</p>
<p><strong>Impact:</strong><br>Traffic disrupted while tmm restarts.</p>
<p><strong>Workaround:</strong><br>None</p>
<p><strong>Fix:</strong><br>The f5-fsm-tmm works as expected after configuring BADOS under traffic.</p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1585773-1" rel="nofollow"></a>1585773-1 : Unable to migrate large number of applications at once</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>When you click the Deploy button to migrate a large number of applications (over 500 applications) at once, you might get the following error: <br>
<br>
Cannot read properties of undefined (reading &#39;status_code&#39;)</p>
<p><strong>Conditions:</strong><br>Select more than 500 applications to migrate to BIG-IP Next Central Manager.</p>
<p><strong>Impact:</strong><br>More than 500 applications cannot be migrated at once.</p>
<p><strong>Workaround:</strong><br>None</p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1585285" rel="nofollow"></a>1585285 : Unable to stage applications for migration when session contains large number of application services</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>When you click Add application on Application Migration page,  the following error is returned:<br>
<br>
Unexpected Error: applications?limit-1000000</p>
<p><strong>Conditions:</strong><br>Migrate a UCS file that contains a large number of virtual servers (more than 2000).</p>
<p><strong>Impact:</strong><br>Applications cannot be migrated using UCS files that have a large number (more than 2000) of virtual servers.</p>
<p><strong>Workaround:</strong><br>Increase amount of memory for mbiq-journeys-feature deployment.<br>
1. Log in to BIG-IP Next Central Manager using SSH<br>
2. Execute following command: kubectl patch deployment mbiq-journeys-feature -p &#39;{&quot;spec&quot;:{&quot;template&quot;:{&quot;spec&quot;:{&quot;containers&quot;:[{&quot;name&quot;:&quot;mbiq-journeys-feature&quot;,&quot;resources&quot;:{&quot;limits&quot;:{&quot;cpu&quot;:&quot;1&quot;,&quot;memory&quot;:&quot;1.5Gi&quot;}}}]}}}}&#39;</p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1584753" rel="nofollow"></a>1584753 : TMM in BIG-IP Next expires the license after 50 days</h4>
<p><strong>Links to More Info: </strong> <a href="https://my.f5.com/manage/s/article/K000139851" target="_blank">K000139851</a></p>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br> -- BIG-IP Next suddenly stops passing application traffic.<br>
-- The TMM logs show that the license has expired<br>
-- The TMM state changes to unlicensed.</p>
<p><strong>Conditions:</strong><br> -- BIG-IP Next instances<br>
-- A valid license is applied, with more than 50 days until expiration<br>
-- 50 (49.7) days elapse after the license activation</p>
<p><strong>Impact:</strong><br>TMM becomes unlicensed and stops passing application traffic</p>
<p><strong>Workaround:</strong><br>Restart the BIG-IP Next instance before 49.7 days has elapsed.</p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1584741" rel="nofollow"></a>1584741 : In the Table commands in iRule, the subtable count command fails in BIG-IP Next 20.x</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>The Table iRule command allows storage of user data during runtime inside &quot;subtables&quot;, administrators use these to store states. The Table command allows to count the number of records in a subtable, following is an example:<br>
<br>
table keys -subtable TABLE -count</p>
<p><strong>Conditions:</strong><br>Using Table &quot;count&quot; command:<br>
<br>
table keys -subtable MYSUBTABLE -count</p>
<p><strong>Impact:</strong><br>Count is incorrectly reported as 0.</p>
<p><strong>Workaround:</strong><br>None</p>
<p><strong>Fix:</strong><br>The Table count command returns the correct number of records.</p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1584681" rel="nofollow"></a>1584681 : Application service creation fails if name contains &quot;fallback&quot;</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>Application service will not be created if name contains &quot;fallback&quot; key in it.</p>
<p><strong>Conditions:</strong><br>Application service name contains &quot;fallback&quot; key.</p>
<p><strong>Impact:</strong><br>Application service is not created.</p>
<p><strong>Workaround:</strong><br>Application service name should not contain &quot;fallback&quot; key.</p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1584073-1" rel="nofollow"></a>1584073-1 : WAF enforcer might crash when application is removed during handling traffic</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>WAF enforcer might crash when configured application is removed while network traffic is passing through it.</p>
<p><strong>Conditions:</strong><br> -- Application was configured with a WAF profile<br>
-- Traffic is ongoing<br>
-- The application is removed</p>
<p><strong>Impact:</strong><br>Enforcer might sometimes crash. Traffic disrupted while waf-enforcer restarts.</p>
<p><strong>Workaround:</strong><br>None</p>
<p><strong>Fix:</strong><br>Fixed handling of configuration change.</p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1580545" rel="nofollow"></a>1580545 : iRule allows function local variable</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>This iRule code makes DPVD core dump:<br>
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;when RULE_INIT {<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;set ::tmm [TMM::cmp_unit]<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}<br>
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;when HTTP_REQUEST {<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;log local0. &quot;Request $::tmm&quot;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</p>
<p><strong>Conditions:</strong><br>When using a function local variable like &quot;set ::my_bool true&quot;</p>
<p><strong>Impact:</strong><br>DPVD crashes.</p>
<p><strong>Workaround:</strong><br>None</p>
<p><strong>Fix:</strong><br>This code now works without coredump:<br>
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;when RULE_INIT {<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;set ::tmm [TMM::cmp_unit]<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}<br>
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;when HTTP_REQUEST {<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;log local0. &quot;Request $::tmm&quot;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1580181" rel="nofollow"></a>1580181 : When BIG-IP Next HA is created using CM, the spinner does not refresh</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>The spinner will not refresh upon the completion of the HA task and keep spinning.</p>
<p><strong>Conditions:</strong><br>BIG-IP Next HA is established via Central Manager.</p>
<p><strong>Impact:</strong><br>The spinner gives an impression that HA creation is still in progress even though the process has completed.</p>
<p><strong>Workaround:</strong><br>If you have waited 20+ minutes for high availability to be established and the spinner has not gone away, you can close the drawer to go back to the Instances screen and refresh the page. The instance status will be green and healthy if high availability was successfully established.</p>
<p><strong>Fix:</strong><br>The spinner now reflects the correct state of the task. It will spin when the task is in progress and stop once the task is complete.</p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1579365" rel="nofollow"></a>1579365 : Unsupported nested properties are not underlined during application migration process</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>During application migration, if there are unsupported nested properties (sub-properties of properties) of tmsh objects, they will not be underlined during the Configuration Analyzer.</p>
<p><strong>Conditions:</strong><br>Configuration of migrated application contains an object with nested properties that are not supported on BIG-IP Next, e.g.:<br>
<br>
ltm pool /AS3_Tenant/AS3_Application/testItem2 {<br>
&nbsp;&nbsp;&nbsp;&nbsp;members {<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;/AS3_Tenant/192.168.2.2:400 {<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;address 192.168.2.2<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;connection-limit 1000<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dynamic-ratio 50<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;monitor min 1 of { /Common/http } // unsupported nested property that will not be underlined, but should be<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;priority-group 4<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;rate-limit 100<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ratio 50<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}<br>
&nbsp;&nbsp;&nbsp;&nbsp;}<br>
&nbsp;&nbsp;&nbsp;&nbsp;min-active-members 1 <br>
}</p>
<p><strong>Impact:</strong><br>Functionality of the application service might not work as expected.</p>
<p><strong>Workaround:</strong><br>You can check if all nested properties are present in the AS3 preview of the AS3 declaration. Those that are not present will not be migrated.<br>
<br>
Refer to the AS3 Next schema reference: https://clouddocs.f5.com/bigip-next/latest/schemasupport/schema-reference.html</p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1576545-1" rel="nofollow"></a>1576545-1 : After upgrade, BIG-IP Next tenant os unable to export toda-otel (event logs) data to Cemtral Manager<span title="This issue may cause the configuration to fail to load or may significantly impact system performance after upgrade">&starf;</span></h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>After upgrade, the BIG-IP Next tenant is unable to export toda-otel (event logs) data to CM in VELOS</p>
<p><strong>Conditions:</strong><br>Upgrading BIG-IP Next tenant from 20.1 to 20.2 on a VELOS system.</p>
<p><strong>Impact:</strong><br>After upgrade, the BIG-IP Next tenant is unable to export toda-otel (event logs) data to CM</p>
<p><strong>Workaround:</strong><br>For VELOS Standalone<br>
====================<br>
After upgrade, if the f5-toda-otel-collector cannot connect to host change the tenant status from &quot;DEPLOYED&quot; TO &quot;CONFIGURED&quot; TO &quot;DEPLOYED&quot; to fix the issue. Please note that it will take 5 to 10 min for tenant status to change and it might impact the traffic. <br>
<br>
For VELOS HA follow the following steps<br>
=======================================<br>
1. Setup CM on Mango build<br>
2. Add 2 BIG-IP Next instances(Mango build) on the CM<br>
3. Bring up HA on CM with the Enable Auto Failover option unchecked<br>
4. Add a license to the HA instance.<br>
5. Deploy a basic HTTP app in FAST mode with WAF policy attached (Enforcement mode - Blocking, Log Events - all)<br>
6. Send the traffic and verify the  WAF Dashboard under the Security section, should be able to see  the Total Requests and Blocked response fields with non-zero values<br>
7. Upgrade standby instance to latest nectarine build with the &quot;auto-failover&quot; button switched off.<br>
8. We will observe the instances goes into an unhealthy state on CM.<br>
9. Change the status of the standby instance from Deployed to Configure Mode and save it through partition GUI/CLI.<br>
10. After confirming the status of the pods, change the state of the standby instance back to the Deployed state from the configured state. There should be no impact on the traffic flow during this step.<br>
11. Now do the force failover and check the health status of instances, it will still show unhealthy as instances are in between upgrades.(one instance with Mango build (standby node) and other with Nectarine build(Active node))<br>
12. Now Upgrade the standby instance to the latest nectarine build with the &quot;auto-failover&quot; button switched off.<br>
13. HA should look healthy in this state and traffic should continue to flow. <br>
14. Change the state of the standby instance from Deployed to Configure Mode and save it using partition GUI/CLI<br>
15. After confirming the status of the pods for the instance on partition CLI, change the state of the standby instance back to the Deployed state from the configured state.<br>
16. We will observe the Event logs on the WAF Dashboard under the security section on CM.<br>
17. We can also observe the logs on the &quot;f5-toda-otel-collector&quot; pod showing no Export failures.<br>
18. Upgrade the CM. Systems should be Healthy.</p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1572437" rel="nofollow"></a>1572437 : CVE-2024-0450: : python: The zipfile module is vulnerable to zip-bombs leading to denial of service</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>A flaw was found in the Python/CPython &#39;zipfile&#39; that can allow a zip-bomb type of attack. An attacker may craft a zip file format, leading to a Denial of Service when processed.</p>
<p><strong>Impact:</strong><br>While vulnerable code is present, it is not exposed in default, recommended, or standard configurations.</p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1571993" rel="nofollow"></a>1571993 : Access Session data is not cleared after TMM restart</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>The session entry stored in Redis server will not be cleared when TMM restarts and the user session does not come back to the BIG-IP (after TMM restarts).</p>
<p><strong>Conditions:</strong><br>User session is created in Redis server<br>
TMM restarts<br>
Traffic for the corresponding user session does not come back to BIG-IP.</p>
<p><strong>Impact:</strong><br>The session record in Redis server is not cleared in specific scenario of TMM restart and the traffic for user session does not come back to BIG-IP.</p>
<p><strong>Workaround:</strong><br>None</p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1569589" rel="nofollow"></a>1569589 : Default values of Access policy are not migrated</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>Default values of an Access policy are not migrated to BIG-IP Next Central Manager.</p>
<p><strong>Conditions:</strong><br>Migrate a virtual server with an Access policy that contains default values.</p>
<p><strong>Impact:</strong><br> - Access Policy imported to BIG-IP Next Central Manager does not have default values populated.<br>
- If the affected policy is deployed to a BIG-IP Next instance, it will use default values applied by BIG-IP Next.</p>
<p><strong>Workaround:</strong><br>From the BIG-IP Next Central Manager UI, you can edit Access policy values for property or leave it un-selected.</p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1564157" rel="nofollow"></a>1564157 : BIG-IP Next Central Manager requires VELOS/rSeries systems to use an SSL certificate containing the host IP address in the CN or SANs list.<span title="This issue may cause the configuration to fail to load or may significantly impact system performance after upgrade">&starf;</span></h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>BIG-IP Next Central Manager requires that virtualization providers use a valid SSL certificate. A self-signed certificate can also be explicitly accepted by BIG-IP Next Central Manager users, if the certificate otherwise passes SSL validation successfully.<br>
When F5OS generates self-signed SSL certificates for its HTTPS services, it does not include the actual hostname or IP address in the Common Name or Subject Alternative Names (SANs) fields. As a result, this self-signed certificate will not pass SSL validation for strict TLS clients, because the HTTPS server name does not match any Subject names in the certificate.</p>
<p><strong>Conditions:</strong><br>A BIG-IP Next Central Manager user attempts to add a VELOS or rSeries system as a virtualization provider, when the VELOS or rSeries system is using the default self-signed certificate generated by the system.</p>
<p><strong>Impact:</strong><br>BIG-IP Next Central Manager cannot successfully add VELOS or rSeries systems as virtualization providers, and therefore cannot dynamically create new BIG-IP Next instances on VELOS or rSeries systems.</p>
<p><strong>Workaround:</strong><br>1. Create a self-signed SSL certificate that includes the F5OS system&#39;s actual IP address in the Subject Alternative Names (SANs) field. For example, the following steps can be used:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A. Save the following data into a file named &ldquo;ip-san.cnf&quot;:<br>
<br>
[req]<br>
default_bits  = 2048<br>
distinguished_name = req_distinguished_name<br>
req_extensions = req_ext<br>
x509_extensions = v3_req<br>
prompt = no<br>
<br>
[req_distinguished_name]<br>
countryName = XX<br>
stateOrProvinceName = N/A<br>
localityName = N/A<br>
organizationName = Self-signed certificate<br>
commonName = F5OS Self-signed certificate<br>
<br>
[req_ext]<br>
subjectAltName = @alt_names<br>
<br>
[v3_req]<br>
subjectAltName = @alt_names<br>
<br>
[alt_names]<br>
IP.1 = 127.0.0.1<br>
DNS.1 = f5platform.host<br>
<br>
&nbsp;&nbsp;&nbsp;&nbsp;B. Edit the file -- change IP.1 at the end to be the rSeries or VELOS partition management IP address. Optionally, other certificate fields may also be updated if the new cert should have specific values for them (e.g., commonName, organizationName, localityName, etc.).<br>
&nbsp;&nbsp;&nbsp;&nbsp;C. Run the following command, to create the two certificate files &quot;ip-san-cert.pem&quot; and &quot;ip-san-key.pem&rdquo;:<br>
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;openssl req -x509 -nodes -days 730 -newkey rsa:2048 -keyout ip-san-key.pem -out ip-san-cert.pem -config ip-san.cnf<br>
<br>
2. In the VELOS Partition or rSerie s Hardware UI:<br>
&nbsp;&nbsp;&nbsp;A. Navigate to the AUTHENTICATION &amp; ACCESS -&gt; TLS Configuration page.<br>
&nbsp;&nbsp;&nbsp;B. Locate and update the &quot;TLS Certificate&quot; and &quot;TLS Key&quot; text boxes to the new Cert file &amp; Key file, respectively. <br>
&nbsp;&nbsp;&nbsp;C. The F5OS system will then use this new certificate with its HTTPS services.</p>
<p><strong>Fix:</strong><br>F5OS (VELOS/rSeries) has included a default valid self-signed certificate in F5OS version 1.8.0. With that, CM now can discover the fresh-installed VELOS/rSeries</p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1561053" rel="nofollow"></a>1561053 : Application status migration status incorrectly labeled as green when certain properties are removed</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>When migrating applications to BIG-IP Next, certain unsupported properties might be removed during the migration process, but the virtual server status is incorrectly labelled as &quot;Ready for migration&quot; (green status), rather than notify with a &quot;Warning&quot; (yellow status).</p>
<p><strong>Conditions:</strong><br>Migration of a UCS to BIG-IP Central Manager that contains application services with certain unsupported properties. Some examples are:<br>
<br>
min-active-members<br>
slow-ramp-time<br>
<br>
Following migration, the following can be reviewed:<br>
<br>
- Virtual server status is green (&quot;Ready for migration&quot;)<br>
- Virtual server contains configuration with tmsh objects that can be translated into AS3 classes supported in BIG-IP Next.<br>
- tmsh objects that contain unsupported properties cannot be translated into configurable options for AS3 class.<br>
<br>
AS3 Schema Reference: https://clouddocs.f5.com/bigip-next/latest/schemasupport/schema-reference.html</p>
<p><strong>Impact:</strong><br>Unsupported properties are silently dropped without logs and the status of the migration is incorrect (status is green, but should be yellow). The application service after migration might not be functional because of the missing properties.</p>
<p><strong>Workaround:</strong><br>None</p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1560493" rel="nofollow"></a>1560493 : Inaccurate Reflection of Selfip Prefix Length in TMM Statistics and &quot;ip addr&quot; Output</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>Changes to the prefix length of selfips are not reflected in TMM statistics or the &quot;ip addr&quot; output.</p>
<p><strong>Conditions:</strong><br>Configure an L1-network with VLAN and self-ip with a certain prefix, then altering the prefix length or subnet of the self-ip.</p>
<p><strong>Impact:</strong><br>The modifications made to the self-ip prefix length are not reflected in TMM statistics.</p>
<p><strong>Workaround:</strong><br>To address changes in self-ip subnets, it is necessary to delete the L1-network and subsequently re-add it.</p>
<p><strong>Fix:</strong><br>The selfip lookup in create/update configuration handler will now validate the prefix-length along with the address and route-domain.</p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1560473" rel="nofollow"></a>1560473 : Traffic won&#39;t work with http monitor for L3, http-transparent service</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>User will configure the http monitor in L3, http-transparent or http-explicit service and no package seen.</p>
<p><strong>Conditions:</strong><br>Http monitor should be added in these services</p>
<p><strong>Impact:</strong><br>Due to ID 1584485, traffic will not work for l3 and http-transparent service. This ID is for ltm.<br>
http-explicit traffic pass should be ok.</p>
<p><strong>Workaround:</strong><br>Do not add HTTP monitor</p>
<p><strong>Fix:</strong><br>Http-explicit should be ok after this fix.</p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1531845" rel="nofollow"></a>1531845 : CVE-2023-27043: python: Parsing errors in email/_parseaddr.py lead to incorrect value in email address part of tuple</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>The email module of Python through 3.11.3 incorrectly parses e-mail addresses that contain a special character. The wrong portion of an RFC2822 header is identified as the value of the addr-spec. In some applications, an attacker can bypass a protection mechanism in which application access is granted only after verifying receipt of e-mail to a specific domain (e.g., only @company.example.com addresses may be used for signup). This occurs in email/_parseaddr.py in recent versions of Python.</p>
<p><strong>Impact:</strong><br>While vulnerable code is present, it is not exposed in default, recommended, or standard configurations.</p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1516785" rel="nofollow"></a>1516785 : CVE-2023-49081: aiohttp: HTTP request modification</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>A flaw was found in the python-aiohttp package. This issue could allow a remote attacker to modify an existing HTTP request or create a new request that could have minor confidentiality or integrity impacts.</p>
<p><strong>Impact:</strong><br>While vulnerable code is present, it is not exposed in default, recommended, or standard configurations.</p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1509361" rel="nofollow"></a>1509361 : CVE-2023-50782 python-cryptography: Bleichenbacher timing oracle attack against RSA decryption</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>A flaw was found in the python-cryptography package. This issue may allow a remote attacker to decrypt captured messages in TLS servers that use RSA key exchanges, which may lead to exposure of confidential or sensitive data</p>
<p><strong>Impact:</strong><br>While vulnerable code is present, it is not exposed in default, recommended, or standard configurations.</p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1507021" rel="nofollow"></a>1507021 : CVE-2023-45803: urllib3: Request body not stripped after redirect</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>urllib3 previously wouldn&#39;t remove the HTTP request body when an HTTP redirect response using status 301, 302, or 303 after the request had its method changed from one that could accept a request body (like `POST`) to `GET` as is required by HTTP RFCs.</p>
<p><strong>Conditions:</strong><br>NA</p>
<p><strong>Impact:</strong><br>The vulnerability requires a previously trusted service to become compromised to affect confidentiality.</p>
<p><strong>Workaround:</strong><br>NA</p>
<p><strong>Fix:</strong><br>urllib3 module has been updated to a non-vulnerable version</p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1506949" rel="nofollow"></a>1506949 : CVE-2024-0727 openssl: denial of service via null dereference</h4>
<p><strong>Links to More Info: </strong> <a href="https://my.f5.com/manage/s/article/K000138695" target="_blank">K000138695</a></p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1498489" rel="nofollow"></a>1498489 : LDAP Bind Password not Re-populated in BIG-IP Next Central Manager GUI</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>After successfully saving an LDAP configuration in the BIG-IP Next Central Manager GUI Auth Providers, the &quot;Bind User Password&quot; field does not re-populate when returning to the same LDAP configuration tab.<br>
<br>
Any time an administrator needs to make further changes to the LDAP configuration or use the &#39;test&#39; feature, the Bind User Password must be re-entered.</p>
<p><strong>Conditions:</strong><br>The LDAP Bind Password is not re-populated in the BIG-IP Next Central Manager GUI after initial configuration. Once the password is set and saved, navigating back to the LDAP configuration page does not show the bind password field populated. This issue may prevent users from confirming whether the password has been saved or requires re-entry during subsequent configuration updates.</p>
<p><strong>Impact:</strong><br>Users are required to re-enter the saved Bind user password, leading to a poor UI experience.</p>
<p><strong>Workaround:</strong><br>Re-enter the Bind User Password when making any changes to the LDAP authentication provider configuration settings.</p>
<p><strong>Fix:</strong><br>The GUI no longer requires the bind password be re-entered when making other configuration changes to the LDAP authentication provider or using the Test Auth Provider Settings feature.</p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1490381" rel="nofollow"></a>1490381 : Pagination for iRules page not supported with a large number of iRules</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>Pagination is not supported in the iRules data grid when 100s of iRules are configured to BIG-IP Next Central Manager.</p>
<p><strong>Conditions:</strong><br>This issue occurs when there are 100s of iRules on BIG-IP Next Central Manager, which do not fit in a single iRule view.</p>
<p><strong>Impact:</strong><br>If iRules data exceeds about 500, then all iRules data will be shown at once. So it will be difficult to find specific iRules.</p>
<p><strong>Workaround:</strong><br>Search for iRule name from the search bar to find a specific iRule.</p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1472669-1" rel="nofollow"></a>1472669-1 : idle timer in BIG-IP Next Central Manager can log out user during file uploads<span title="This issue may cause the configuration to fail to load or may significantly impact system performance after upgrade">&starf;</span></h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>During a file upload,  the UI idle timer will log out after ~ 20 minutes, possibly terminating the file upload, or making it appear as though the upload hasn&#39;t completed when it has.</p>
<p><strong>Conditions:</strong><br>Upload a file to BIG-IP Next Central Manager.</p>
<p><strong>Impact:</strong><br>File upload is incomplete.</p>
<p><strong>Workaround:</strong><br>Interact periodically with the UI by moving the mouse or pressing keys in the browser window during a file upload that takes longer than ~20 minutes.  This will reset the idle timer and prevent the UI from terminating the user session.</p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1472337" rel="nofollow"></a>1472337 : Missing object referenced in authenticationTrustCA</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>If a authenticationTrustCA is used in the declaration, then the object referenced in it (Certificate and its key) does not point to the available object in the declaration.</p>
<p><strong>Conditions:</strong><br>Application with authenticationTrustCA is migrated.</p>
<p><strong>Impact:</strong><br>Application will not be deployable to an Instance. Application will be migratable as draft only.</p>
<p><strong>Workaround:</strong><br>User can manually import Certificate and Key to CM and then create an object in the application declaration that would match the reference used in authenticationTrustCA.</p>
<p><strong>Fix:</strong><br>Object referenced in authenticationTrustCA is now generated in the application declaration.</p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1455677-3" rel="nofollow"></a>1455677-3 : ACCESS Policy hardening</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>Under certain traffic patterns, ACCESS policy may crash</p>
<p><strong>Conditions:</strong><br>ACCESS policy evaluation is enabled</p>
<p><strong>Impact:</strong><br>A TMM core.</p>
<p><strong>Workaround:</strong><br>None</p>
<p><strong>Fix:</strong><br>The core is resolved.</p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1449709-6" rel="nofollow"></a>1449709-6 : Possible TMM core under certain Client-SSL profile configurations</h4>
<p><strong>Links to More Info: </strong> <a href="https://my.f5.com/manage/s/article/K000138912" target="_blank">K000138912</a>, <a href=http://cdn.f5.com/product/bugtracker/ID1449709.html target="_blank">BT1449709</a></p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1399137" rel="nofollow"></a>1399137 : &quot;40001: bind: address already in use&quot; failure logs on BIG-IP Next HA setup</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>Following error logs/events are displayed as part of HA cluster configuration of BIG-IP Next tenants: <br>
<br>
&quot;40001: bind: address already in use&quot;</p>
<p><strong>Conditions:</strong><br>Errors are observed when HA is configured between two BIG-IP Next tenants.</p>
<p><strong>Impact:</strong><br>These are just error messages. No functional impact.</p>
<p><strong>Workaround:</strong><br>N/A</p>
<p><strong>Fix:</strong><br>N/A</p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1394625" rel="nofollow"></a>1394625 : Application service failes to deploy even if marked as green (ready to deploy)</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>Deployment fails for a migrated application service that is marked green (ready for deployment.</p>
<p><strong>Conditions:</strong><br>During application migration, upload a UCS with a virtual server that has clientssl profile attached that points to a cert/key pair with RSA 512 OR 1024 key (unsupported).<br>
<br>
Complete the migration and pre-deployment process, and deploy the application service.</p>
<p><strong>Impact:</strong><br>The application service will not have a deployment location option and can only be saved as a draft.</p>
<p><strong>Fix:</strong><br>Application services with unsupported certificates and key pairs, or other missing/unsupported objects are marked with a blue status to indicate that the application service requires changes once saved as a draft.</p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1348837" rel="nofollow"></a>1348837 : Admin can delete their own account</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>The default admin user can delete their own account via the API.</p>
<p><strong>Conditions:</strong><br> -- Admin user<br>
-- Deleting the admin account via the API</p>
<p><strong>Impact:</strong><br>This inconsistency between the API and GUI can lead to a lockout of the BIG-IP Next Central Manager if an admin user deletes their account via the API and no other backup admin or non-admin users are available to log in.</p>
<p><strong>Workaround:</strong><br>Perform operations through the GUI.</p>
<p><strong>Fix:</strong><br>The issue has been resolved. Self-deletion of user accounts is now disallowed via the API.</p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1348833" rel="nofollow"></a>1348833 : A cryptographically insecure pseudo-random number generator was used to create passwords during the reset process.</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>The Math.random() function in JavaScript does not produce cryptographically secure random numbers.</p>
<p><strong>Conditions:</strong><br>When resetting a user password, using the &ldquo;Randomly Generate&rdquo; option from the reset drawer may result in a weak password being generated.</p>
<p><strong>Impact:</strong><br>This could result in weak and easily guessable passwords, increasing the risk of brute force attacks.</p>
<p><strong>Workaround:</strong><br>Manually input the password using the &lsquo;Manually Enter&rsquo; option during the password reset process.</p>
<p><strong>Fix:</strong><br>The use of Math.random for password generation has been removed.</p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1329853" rel="nofollow"></a>1329853 : Application traffic is intermittent when more than one virtual server is configured</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>After deploying an application containing multiple virtual servers, only one of the virtual servers responds to clients.<br>
<br>
In the Central Manager GUI, one virtual server is marked as red and the other is marked as green, even though you can ping all of the pool members for each of the virtual servers.</p>
<p><strong>Conditions:</strong><br> -- The application contains multiple virtual servers<br>
-- The virtual addresses for each of the virtual servers is identical and the port is identical<br>
<br>
Alternatively, you could encounter this by deploying two different applications where the virtual address and port are identical.</p>
<p><strong>Impact:</strong><br>The application will deploy without error even if an IP address/port conflict occurs, and traffic will be disrupted to one or both of the virtual addresses.</p>
<p><strong>Workaround:</strong><br>Assign different virtual addresses and/or virtual ports for different application services. If any two existing applications has same listeners defined, you can change the data by adding unique listeners and re-deploy.</p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1309265" rel="nofollow"></a>1309265 : CVE-2022-41723 golang.org/x/net vulnerable to Uncontrolled Resource Consumption</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>A maliciously crafted HTTP/2 stream could cause excessive CPU consumption in the HPACK decoder, sufficient to cause a denial of service from a small number of small requests.</p>
<p><strong>Impact:</strong><br>While vulnerable code is present, it is not exposed in default, recommended, or standard configurations.</p>
<p><strong>Fix:</strong><br>The logcli package is no longer included in the system.</p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1309257" rel="nofollow"></a>1309257 : CVE-2022-41715 potential golang regex DoS</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>Programs which compile regular expressions from untrusted sources may be vulnerable to memory exhaustion or denial of service. The parsed regexp representation is linear in the size of the input, but in some cases the constant factor can be as high as 40,000, making relatively small regexps consume much larger amounts of memory. After fix, each regexp being parsed is limited to a 256 MB memory footprint. Regular expressions whose representation would use more space than that are rejected. Normal use of regular expressions is unaffected.</p>
<p><strong>Impact:</strong><br>While vulnerable code is present, it is not exposed in default, recommended, or standard configurations.</p>
<p><strong>Fix:</strong><br>The logcli package is no longer included in the system.</p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1308845" rel="nofollow"></a>1308845 : CVE-2022-46146 exporter-toolkit: authentication bypass via cache poisoning</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>Prometheus Exporter Toolkit is a utility package to build exporters. Prior to versions 0.7.2 and 0.8.2, if someone has access to a Prometheus web.yml file and users&#39; bcrypted passwords, they can bypass security by poisoning the built-in authentication cache. Versions 0.7.2 and 0.8.2 contain a fix for the issue. There is no workaround, but attacker must have access to the hashed password to use this functionality.</p>
<p><strong>Impact:</strong><br>While vulnerable code is present, it is not exposed in default, recommended, or standard configurations.</p>
<p><strong>Fix:</strong><br>The logcli package is no longer included in the system.</p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1269733-6" rel="nofollow"></a>1269733-6 : HTTP GET request with headers has incorrect flags causing timeout</h4>
<p><strong>Links to More Info: </strong><a href=http://cdn.f5.com/product/bugtracker/ID1269733.html target="_blank">BT1269733</a></p>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>The 504 Gateway Timeout pool member responses are generated  from a Microsoft webserver handling HTTP/2 requests.<br>
<br>
The tcpdump shows that the HTTP/2 stream sends the request without an appropriate End Stream flag on the Headers packet.</p>
<p><strong>Conditions:</strong><br>The server has to provide settings with max-frame-size small enough to force BIG-IP to split the headers across multiple HTTP/2 frames, otherwise this issue does not occur.</p>
<p><strong>Impact:</strong><br>The HTTP GET request causing timeout.</p>
<p><strong>Workaround:</strong><br>None</p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1251181" rel="nofollow"></a>1251181 : VLAN names longer than 15 characters can cause issues with troubleshooting</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>If the VLAN name is longer than 15 characters, traffic originating from the debug-sidecar will not work correctly and can cause issues with troubleshooting.</p>
<p><strong>Conditions:</strong><br>The user creates an L1 network with a VLAN that has a name longer than 15 characters.</p>
<p><strong>Impact:</strong><br>Traffic that originates from the debug sidecar will not work correctly.<br>
For example, if an internal VLAN is configured with a long name, the name in the output from &#39;ip addr&#39; and &#39;ip route&#39; on the debug sidecar will show a truncated name. Additionally, if a ping is attempted to a destination that is connected using this VLAN, the ping packets will be dropped and ping will fail.</p>
<p><strong>Workaround:</strong><br>Use VLAN names less than 16 characters long.</p>
</div><hr>
<div style="background-color:#b3ecff"><h4><a name="A1232521-6" rel="nofollow"></a>1232521-6 : SCTP connection sticking on BIG-IP even after connection terminated</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>After an SCTP client has terminated, the BIG-IP still shows the connection when issuing &quot;show sys conn protocol sctp&quot;</p>
<p><strong>Conditions:</strong><br>Under certain conditions, an SCTP client connection may still exist even if the client has sent a SHUTDOWN request.</p>
<p><strong>Impact:</strong><br>Memory resources will be consumed as these type of lingering connections accumulate</p>
<p><strong>Fix:</strong><br>SCTP connections are properly internally closed when required.</p>
</div><hr>
<a name="KnownIssues" rel="nofollow"></a>
<br><font size="4"><b><u>Known Issues in BIG-IP Next v20.3.0</u></b></font><br><br>
<br><font size="4"><u><strong>BIG-IP Next Issues</strong></u></font><br><br>
<table cellpadding="2" cellspacing="2" border="0" width="100%">
                    <tr valign="top">
                        <td width="10%"><u>ID Number</u></td>    <td width="10%"><u>Severity</u></td>    <td width="10%"><u>Links to More Info</u></td>
                    <td width="50%"><u>Description</u></td></tr>
  <tr valign="top">
    <td><a href="#A1694325-1">1694325-1</a></td>
    <td>1-Blocking</td>
    <td></td>
    <td>Unable to save &quot;new&quot; HTTPS client-side certificates on HTTP2 app</td></tr>
  <tr valign="top">
    <td><a href="#A1682089-1">1682089-1</a></td>
    <td>1-Blocking</td>
    <td></td>
    <td>Configuration migration of an access policy from BIG-IP 17.1.0 or above to BIG-IP Next 20.3.0 Access is causing an invalid login page</td></tr>
  <tr valign="top">
    <td><a href="#A1678561-1">1678561-1</a></td>
    <td>1-Blocking</td>
    <td></td>
    <td>Application health remains stuck in Unknown state.</td></tr>
  <tr valign="top">
    <td><a href="#A1674409-1">1674409-1</a></td>
    <td>1-Blocking</td>
    <td></td>
    <td>High API load can render BIG-IP Next unresponsive.</td></tr>
  <tr valign="top">
    <td><a href="#A1644653-1">1644653-1</a></td>
    <td>1-Blocking</td>
    <td></td>
    <td>BIG-IP Next Central Manager displays a Failed status for High Availability (HA) when adding a third node</td></tr>
  <tr valign="top">
    <td><a href="#A1596021-1">1596021-1</a></td>
    <td>1-Blocking</td>
    <td></td>
    <td>serverTLS/clientTLS name in Service_TCP do not match the clientSSL/serverSSL profile name</td></tr>
  <tr valign="top">
    <td><a href="#A1579441-1">1579441-1</a></td>
    <td>1-Blocking</td>
    <td></td>
    <td>Connection requests on rSeries may not appear to be DAG distributed as expected</td></tr>
  <tr valign="top">
    <td><a href="#A1574585-3">1574585-3</a></td>
    <td>1-Blocking</td>
    <td></td>
    <td>Auto-Failback cluster cannot upgrade active node<span title="This issue may cause the configuration to fail to load or may significantly impact system performance after upgrade">&starf;</span></td></tr>
  <tr valign="top">
    <td><a href="#A1353589">1353589</a></td>
    <td>1-Blocking</td>
    <td></td>
    <td>Provisioning of BIG-IP Next Access modules is not supported on VELOS, but containers continue to run</td></tr>
  <tr valign="top">
    <td><a href="#A1352969">1352969</a></td>
    <td>1-Blocking</td>
    <td></td>
    <td>Upgrades with TLS configuration can cause TMM crash loop</td></tr>
  <tr valign="top">
    <td><a href="#A1350285-1">1350285-1</a></td>
    <td>1-Blocking</td>
    <td></td>
    <td>Traffic is not passing after the tenant is licensed and network is configured</td></tr>
  <tr valign="top">
    <td><a href="#A1696253-1">1696253-1</a></td>
    <td>2-Critical</td>
    <td></td>
    <td>Failed to upload Instance QKView file to iHealth from BIG-IP Next Central Manager</td></tr>
  <tr valign="top">
    <td><a href="#A1696129-1">1696129-1</a></td>
    <td>2-Critical</td>
    <td></td>
    <td>Network Interface Instance Data Metrics show all available interfaces rather than only those that are used</td></tr>
  <tr valign="top">
    <td><a href="#A1696113-1">1696113-1</a></td>
    <td>2-Critical</td>
    <td></td>
    <td>Save button on application configure network may not be enabled.</td></tr>
  <tr valign="top">
    <td><a href="#A1695977-1">1695977-1</a></td>
    <td>2-Critical</td>
    <td></td>
    <td>BGP is unsupported in High Availability (HA) Mode on VELOS</td></tr>
  <tr valign="top">
    <td><a href="#A1695053-1">1695053-1</a></td>
    <td>2-Critical</td>
    <td></td>
    <td>QKView Generation may fail if upgrade file remains on BIG-IP Next Instance</td></tr>
  <tr valign="top">
    <td><a href="#A1694241-1">1694241-1</a></td>
    <td>2-Critical</td>
    <td></td>
    <td>CM Disconnected mode: Module provision is failing</td></tr>
  <tr valign="top">
    <td><a href="#A1692209-1">1692209-1</a></td>
    <td>2-Critical</td>
    <td></td>
    <td>Central Manager backup fails after upgrade</td></tr>
  <tr valign="top">
    <td><a href="#A1691633-1">1691633-1</a></td>
    <td>2-Critical</td>
    <td></td>
    <td>VELOS upgrade error alert states status 400 when image is not present or ready on tenant<span title="This issue may cause the configuration to fail to load or may significantly impact system performance after upgrade">&starf;</span></td></tr>
  <tr valign="top">
    <td><a href="#A1689457-1">1689457-1</a></td>
    <td>2-Critical</td>
    <td></td>
    <td>Potential Issues with Debug Utility Activation for Users with Underscores (_) in Usernames.</td></tr>
  <tr valign="top">
    <td><a href="#A1689421-1">1689421-1</a></td>
    <td>2-Critical</td>
    <td></td>
    <td>Upgrade of BIG-IP Next instance may require reestablishing trust<span title="This issue may cause the configuration to fail to load or may significantly impact system performance after upgrade">&starf;</span></td></tr>
  <tr valign="top">
    <td><a href="#A1682029-1">1682029-1</a></td>
    <td>2-Critical</td>
    <td></td>
    <td>Application and instances graphs may show traffic spikes after HA failover or active node shutdown</td></tr>
  <tr valign="top">
    <td><a href="#A1682017-1">1682017-1</a></td>
    <td>2-Critical</td>
    <td></td>
    <td>A possible gap in app/instance graphs might be shown after HA failover/blade shutdown</td></tr>
  <tr valign="top">
    <td><a href="#A1679977-1">1679977-1</a></td>
    <td>2-Critical</td>
    <td></td>
    <td>Creating multiple L1-Networks with same names will only create one L1-Network</td></tr>
  <tr valign="top">
    <td><a href="#A1678817-1">1678817-1</a></td>
    <td>2-Critical</td>
    <td></td>
    <td>Intermittent failure in Data Group deployment when added from SSL Orchestrator policy</td></tr>
  <tr valign="top">
    <td><a href="#A1678677-1">1678677-1</a></td>
    <td>2-Critical</td>
    <td></td>
    <td>Re-discover Active node</td></tr>
  <tr valign="top">
    <td><a href="#A1678453-1">1678453-1</a></td>
    <td>2-Critical</td>
    <td></td>
    <td>A high number of application creations and deletions can cause frequent Out of Memory (OOM) errors for WebSSO</td></tr>
  <tr valign="top">
    <td><a href="#A1678009-1">1678009-1</a></td>
    <td>2-Critical</td>
    <td></td>
    <td>CM may not display the complete FQDN if the FQDN is long</td></tr>
  <tr valign="top">
    <td><a href="#A1677913-1">1677913-1</a></td>
    <td>2-Critical</td>
    <td></td>
    <td>Redeployment fails when CRLDP Responder mode is changed</td></tr>
  <tr valign="top">
    <td><a href="#A1677537-1">1677537-1</a></td>
    <td>2-Critical</td>
    <td></td>
    <td>Inspection services endpoints related configuration are not propagated to BIG-IP Next instances during upgrades<span title="This issue may cause the configuration to fail to load or may significantly impact system performance after upgrade">&starf;</span></td></tr>
  <tr valign="top">
    <td><a href="#A1677141">1677141</a></td>
    <td>2-Critical</td>
    <td></td>
    <td>Updating a L1-Network is not allowed if either HA Control-Plane VLAN or Data-Plane VLAN are part of the same L1-Network</td></tr>
  <tr valign="top">
    <td><a href="#A1672109-1">1672109-1</a></td>
    <td>2-Critical</td>
    <td></td>
    <td>Unable to reach backend application when configured with &quot;host&quot; in Network Access Optimized Application</td></tr>
  <tr valign="top">
    <td><a href="#A1671645-1">1671645-1</a></td>
    <td>2-Critical</td>
    <td></td>
    <td>Interfaces not properly mapped after switching port profiles from 8x10 to 4x25</td></tr>
  <tr valign="top">
    <td><a href="#A1670689-1">1670689-1</a></td>
    <td>2-Critical</td>
    <td></td>
    <td>BIG-IP Next Central Manager High Availability Installation Failures in High Disk Latency Environments<span title="This issue may cause the configuration to fail to load or may significantly impact system performance after upgrade">&starf;</span></td></tr>
  <tr valign="top">
    <td><a href="#A1668017-1">1668017-1</a></td>
    <td>2-Critical</td>
    <td></td>
    <td>Cannot add new VLANs to existing HA L1 Network</td></tr>
  <tr valign="top">
    <td><a href="#A1644545-1">1644545-1</a></td>
    <td>2-Critical</td>
    <td></td>
    <td>Central Manager (CM) restore fails when using a full backup file with an external storage configuration</td></tr>
  <tr valign="top">
    <td><a href="#A1644157">1644157</a></td>
    <td>2-Critical</td>
    <td></td>
    <td>&quot;Error sending OCSP request&quot; seen in apmd logs for OCSP authentication access policy</td></tr>
  <tr valign="top">
    <td><a href="#A1641909">1641909</a></td>
    <td>2-Critical</td>
    <td></td>
    <td>Applications created from the API sometimes can&#39;t be edited in the GUI.</td></tr>
  <tr valign="top">
    <td><a href="#A1641901-1">1641901-1</a></td>
    <td>2-Critical</td>
    <td></td>
    <td>App configs are not reflected in f5-fsm, causing traffic failure</td></tr>
  <tr valign="top">
    <td><a href="#A1636229-1">1636229-1</a></td>
    <td>2-Critical</td>
    <td></td>
    <td>A vCPU count change can stop traffic for up to 3 hours</td></tr>
  <tr valign="top">
    <td><a href="#A1635421">1635421</a></td>
    <td>2-Critical</td>
    <td></td>
    <td>License server unavailable when a node goes down<span title="This issue may cause the configuration to fail to load or may significantly impact system performance after upgrade">&starf;</span></td></tr>
  <tr valign="top">
    <td><a href="#A1634065-1">1634065-1</a></td>
    <td>2-Critical</td>
    <td></td>
    <td>BIG-IP Next application telemetry data missing for a brief period from Central Manager when a CM node goes down</td></tr>
  <tr valign="top">
    <td><a href="#A1632833-1">1632833-1</a></td>
    <td>2-Critical</td>
    <td></td>
    <td>Upgrade to Release version 20.3.0 might create a core file<span title="This issue may cause the configuration to fail to load or may significantly impact system performance after upgrade">&starf;</span></td></tr>
  <tr valign="top">
    <td><a href="#A1629161-1">1629161-1</a></td>
    <td>2-Critical</td>
    <td></td>
    <td>L1-Network cannot be deleted</td></tr>
  <tr valign="top">
    <td><a href="#A1604997-1">1604997-1</a></td>
    <td>2-Critical</td>
    <td></td>
    <td>Central Manager (CM) Prometheus pod in CrashLoopBackOff</td></tr>
  <tr valign="top">
    <td><a href="#A1600809-1">1600809-1</a></td>
    <td>2-Critical</td>
    <td></td>
    <td>Upgrading BIG-IP Next Central Manager does not show unsupported properties in migrations created before upgrade.<span title="This issue may cause the configuration to fail to load or may significantly impact system performance after upgrade">&starf;</span></td></tr>
  <tr valign="top">
    <td><a href="#A1600377-1">1600377-1</a></td>
    <td>2-Critical</td>
    <td></td>
    <td>The BIG-IP Central Manager GUI does not support backup file uploads when external storage is configured.</td></tr>
  <tr valign="top">
    <td><a href="#A1596929-1">1596929-1</a></td>
    <td>2-Critical</td>
    <td></td>
    <td>Policy-compiler supports policy versions only up to 17.0.0.</td></tr>
  <tr valign="top">
    <td><a href="#A1596801-1">1596801-1</a></td>
    <td>2-Critical</td>
    <td></td>
    <td>Route Health Injection default for BIG-IP Next is &quot;ANY&quot;<span title="This issue may cause the configuration to fail to load or may significantly impact system performance after upgrade">&starf;</span></td></tr>
  <tr valign="top">
    <td><a href="#A1593613">1593613</a></td>
    <td>2-Critical</td>
    <td></td>
    <td>When an upgrade fails, CM cannot be restored and becomes dysfunctional due to multiple containers entering the &#39;CrashLoopBackOff&#39; state<span title="This issue may cause the configuration to fail to load or may significantly impact system performance after upgrade">&starf;</span></td></tr>
  <tr valign="top">
    <td><a href="#A1590037-1">1590037-1</a></td>
    <td>2-Critical</td>
    <td></td>
    <td>Provisioning SSL Orchestrator on BIG-IP NEXT HA cluster fails when using Central Manager UI</td></tr>
  <tr valign="top">
    <td><a href="#A1585309">1585309</a></td>
    <td>2-Critical</td>
    <td></td>
    <td>Server-Side traffic flows using a default VRF even though pool is configured in a non-default VRF</td></tr>
  <tr valign="top">
    <td><a href="#A1579977-1">1579977-1</a></td>
    <td>2-Critical</td>
    <td></td>
    <td>BIG-IP Next instance telemetry data is missing from the BIG-IP Next Central Manager when a BIG-IP Next Central Manager High Availability node goes down.</td></tr>
  <tr valign="top">
    <td><a href="#A1576277">1576277</a></td>
    <td>2-Critical</td>
    <td></td>
    <td>&#39;Backup file creation failed&#39; for instance after upgrade to v20.2.0</td></tr>
  <tr valign="top">
    <td><a href="#A1550345-2">1550345-2</a></td>
    <td>2-Critical</td>
    <td></td>
    <td>BIG-IP Next API gateway takes long time to respond large access policy playload</td></tr>
  <tr valign="top">
    <td><a href="#A1492705">1492705</a></td>
    <td>2-Critical</td>
    <td></td>
    <td>During upgrading to BIG-IP Next 20.1.0, the BIG-IP Next 20.1.0 Central Manager failed to connect with BIG-IP Next 20.0.2 instance</td></tr>
  <tr valign="top">
    <td><a href="#A1474669-2">1474669-2</a></td>
    <td>2-Critical</td>
    <td></td>
    <td>Fluentbit core may be generated when restarting the pod</td></tr>
  <tr valign="top">
    <td><a href="#A1466305">1466305</a></td>
    <td>2-Critical</td>
    <td></td>
    <td>Anomaly in factory reset behavior for DNS enabled BIG-IP Next deployment</td></tr>
  <tr valign="top">
    <td><a href="#A1410241-1">1410241-1</a></td>
    <td>2-Critical</td>
    <td></td>
    <td>Traffic for TAP is not seen on service interface when connection mirroring is turned on</td></tr>
  <tr valign="top">
    <td><a href="#A1365005">1365005</a></td>
    <td>2-Critical</td>
    <td></td>
    <td>Analytics data is not restored after upgrading to BIG-IP Next version 20.0.1<span title="This issue may cause the configuration to fail to load or may significantly impact system performance after upgrade">&starf;</span></td></tr>
  <tr valign="top">
    <td><a href="#A1354265">1354265</a></td>
    <td>2-Critical</td>
    <td></td>
    <td>The icb pod may restart during install phase</td></tr>
  <tr valign="top">
    <td><a href="#A1343005-1">1343005-1</a></td>
    <td>2-Critical</td>
    <td></td>
    <td>Modifying L4 serverside after the stack is created can result in the update not being applied</td></tr>
  <tr valign="top">
    <td><a href="#A1087937">1087937</a></td>
    <td>2-Critical</td>
    <td></td>
    <td>API endpoints do not support page query</td></tr>
  <tr valign="top">
    <td><a href="#A1696161-1">1696161-1</a></td>
    <td>3-Major</td>
    <td></td>
    <td>Unable to update the OAuth client configuration</td></tr>
  <tr valign="top">
    <td><a href="#A1695873-1">1695873-1</a></td>
    <td>3-Major</td>
    <td></td>
    <td>BIG-IP Next Central Manager does not load as expected after initial password change<span title="This issue may cause the configuration to fail to load or may significantly impact system performance after upgrade">&starf;</span></td></tr>
  <tr valign="top">
    <td><a href="#A1692233-1">1692233-1</a></td>
    <td>3-Major</td>
    <td></td>
    <td>AS3 Declaration fails to update serverTLS/clientTLS when multiple SSL profiles are configured</td></tr>
  <tr valign="top">
    <td><a href="#A1682085-1">1682085-1</a></td>
    <td>3-Major</td>
    <td></td>
    <td>OAuth Resource Server agent fails to deploy when using a private key to decrypt the access token</td></tr>
  <tr valign="top">
    <td><a href="#A1682021-1">1682021-1</a></td>
    <td>3-Major</td>
    <td></td>
    <td>Unable to save Service Provider configuration changes in the SAML Federation rule</td></tr>
  <tr valign="top">
    <td><a href="#A1680361-1">1680361-1</a></td>
    <td>3-Major</td>
    <td></td>
    <td>Expression of the first branch missed after a Rule node was moved and saved</td></tr>
  <tr valign="top">
    <td><a href="#A1680189-1">1680189-1</a></td>
    <td>3-Major</td>
    <td></td>
    <td>Creating instances does not enable the &quot;Default VRF&quot; field on VLANs by default</td></tr>
  <tr valign="top">
    <td><a href="#A1679593-1">1679593-1</a></td>
    <td>3-Major</td>
    <td></td>
    <td>User should provide an IPv4 address space when an IPv6 address space is provided</td></tr>
  <tr valign="top">
    <td><a href="#A1671465">1671465</a></td>
    <td>3-Major</td>
    <td></td>
    <td>FAST-0002: Internal Server Error: Unable to render template Examples/http: rpc error: code = Unknown desc = missed comma between flow collection entries</td></tr>
  <tr valign="top">
    <td><a href="#A1670977-1">1670977-1</a></td>
    <td>3-Major</td>
    <td></td>
    <td>The BIG-IP Next Central Manager backup fails when a node becomes unreachable</td></tr>
  <tr valign="top">
    <td><a href="#A1635369-1">1635369-1</a></td>
    <td>3-Major</td>
    <td></td>
    <td>CM pool has a manditory monitor constraint.</td></tr>
  <tr valign="top">
    <td><a href="#A1629897-1">1629897-1</a></td>
    <td>3-Major</td>
    <td></td>
    <td>Shared object installation status might be incorrect on a migration resume.</td></tr>
  <tr valign="top">
    <td><a href="#A1629105-1">1629105-1</a></td>
    <td>3-Major</td>
    <td></td>
    <td>Incorrect conversion of DTLS virtual server<span title="This issue may cause the configuration to fail to load or may significantly impact system performance after upgrade">&starf;</span></td></tr>
  <tr valign="top">
    <td><a href="#A1629077-1">1629077-1</a></td>
    <td>3-Major</td>
    <td></td>
    <td>BIG-IP Next Central Manager does not support NTP configuration via DHCP</td></tr>
  <tr valign="top">
    <td><a href="#A1623609-1">1623609-1</a></td>
    <td>3-Major</td>
    <td></td>
    <td>Skipped certificate marked as imported during application migration via the GUI.</td></tr>
  <tr valign="top">
    <td><a href="#A1623533-1">1623533-1</a></td>
    <td>3-Major</td>
    <td></td>
    <td>Observing drop in traffic throughput with debug-sidecar inline tcpdump packet capture</td></tr>
  <tr valign="top">
    <td><a href="#A1623421-1">1623421-1</a></td>
    <td>3-Major</td>
    <td></td>
    <td>External OpenAPI files cannot be used with HTTPS links</td></tr>
  <tr valign="top">
    <td><a href="#A1622005-1">1622005-1</a></td>
    <td>3-Major</td>
    <td></td>
    <td>OpenAPI files that are extremely large cannot be applied</td></tr>
  <tr valign="top">
    <td><a href="#A1615257">1615257</a></td>
    <td>3-Major</td>
    <td></td>
    <td>Application monitors edit drawer autosaves</td></tr>
  <tr valign="top">
    <td><a href="#A1604657">1604657</a></td>
    <td>3-Major</td>
    <td></td>
    <td>High CPU utilization and reduced throughput in certain conditions when connection mirroring is enabled in HA</td></tr>
  <tr valign="top">
    <td><a href="#A1603561-1">1603561-1</a></td>
    <td>3-Major</td>
    <td></td>
    <td>L1-Network name cannot be changed</td></tr>
  <tr valign="top">
    <td><a href="#A1602001">1602001</a></td>
    <td>3-Major</td>
    <td></td>
    <td>Upgrading from 20.2.1 or Earlier versions will delete all External Loggers<span title="This issue may cause the configuration to fail to load or may significantly impact system performance after upgrade">&starf;</span></td></tr>
  <tr valign="top">
    <td><a href="#A1601573">1601573</a></td>
    <td>3-Major</td>
    <td></td>
    <td>UI elements related to virtual servers not shown after upgrade<span title="This issue may cause the configuration to fail to load or may significantly impact system performance after upgrade">&starf;</span></td></tr>
  <tr valign="top">
    <td><a href="#A1601233-1">1601233-1</a></td>
    <td>3-Major</td>
    <td></td>
    <td>Multi-replica in HA not supported for alert feature</td></tr>
  <tr valign="top">
    <td><a href="#A1600381-1">1600381-1</a></td>
    <td>3-Major</td>
    <td></td>
    <td>WAF enforcer might crash during handling of response</td></tr>
  <tr valign="top">
    <td><a href="#A1593805">1593805</a></td>
    <td>3-Major</td>
    <td></td>
    <td>The air-gapped environment upgrade from BIG-IP Next 20.0.2-0.0.68 to BIG-IP Next 20.2.0-0.5.41 fails<span title="This issue may cause the configuration to fail to load or may significantly impact system performance after upgrade">&starf;</span></td></tr>
  <tr valign="top">
    <td><a href="#A1589865-1">1589865-1</a></td>
    <td>3-Major</td>
    <td></td>
    <td>Licensing via CM fails with &quot;400 The SSL certificate error&quot;</td></tr>
  <tr valign="top">
    <td><a href="#A1586869">1586869</a></td>
    <td>3-Major</td>
    <td></td>
    <td>Unable to create the same standby instance, when Instance HA creation failed using CM-created instances<span title="This issue may cause the configuration to fail to load or may significantly impact system performance after upgrade">&starf;</span></td></tr>
  <tr valign="top">
    <td><a href="#A1584637">1584637</a></td>
    <td>3-Major</td>
    <td></td>
    <td>After upgrade, &#39;Accept Request&#39; will only work on events after policy redeploy<span title="This issue may cause the configuration to fail to load or may significantly impact system performance after upgrade">&starf;</span></td></tr>
  <tr valign="top">
    <td><a href="#A1584625">1584625</a></td>
    <td>3-Major</td>
    <td></td>
    <td>Virtual server information of application containing multiple virtual IP addresses and WAF policies after upgrade is missing<span title="This issue may cause the configuration to fail to load or may significantly impact system performance after upgrade">&starf;</span></td></tr>
  <tr valign="top">
    <td><a href="#A1583541">1583541</a></td>
    <td>3-Major</td>
    <td></td>
    <td>Re-establish trust with BIG-IP after upgrade to 20.2.1 using a 20.1.1 Central Manager<span title="This issue may cause the configuration to fail to load or may significantly impact system performance after upgrade">&starf;</span></td></tr>
  <tr valign="top">
    <td><a href="#A1583049-1">1583049-1</a></td>
    <td>3-Major</td>
    <td></td>
    <td>Central Manager Logs</td></tr>
  <tr valign="top">
    <td><a href="#A1582421-1">1582421-1</a></td>
    <td>3-Major</td>
    <td></td>
    <td>BIG-IP Next Central Manager functionality impacted if the host IP address changes</td></tr>
  <tr valign="top">
    <td><a href="#A1582409-1">1582409-1</a></td>
    <td>3-Major</td>
    <td></td>
    <td>BIG-IP Next Central Manager will not start if the DNS server details are not provided</td></tr>
  <tr valign="top">
    <td><a href="#A1574685">1574685</a></td>
    <td>3-Major</td>
    <td></td>
    <td>Generated WAF report can be loaded without text</td></tr>
  <tr valign="top">
    <td><a href="#A1574681">1574681</a></td>
    <td>3-Major</td>
    <td></td>
    <td>Dynamic Parameter Extract from allowed URLs does not show in the parameter in the WAF policy</td></tr>
  <tr valign="top">
    <td><a href="#A1574573">1574573</a></td>
    <td>3-Major</td>
    <td></td>
    <td>Global Resiliency Group status not reflecting correctly on update</td></tr>
  <tr valign="top">
    <td><a href="#A1574565">1574565</a></td>
    <td>3-Major</td>
    <td></td>
    <td>Inability to edit Generic Host While Re-Enabling Global Resiliency</td></tr>
  <tr valign="top">
    <td><a href="#A1568129">1568129</a></td>
    <td>3-Major</td>
    <td></td>
    <td>During upgrade from BIG-IP Next 20.1.0 to BIG-IP Next 20.2.0, issue identified with instances that has L3-Forwards with non default VRF (L3-Network) configuration</td></tr>
  <tr valign="top">
    <td><a href="#A1567129">1567129</a></td>
    <td>3-Major</td>
    <td></td>
    <td>Unable to deploy Apps on BIG-IP Next v20.2.0 created using Instantiation from v20.1.x<span title="This issue may cause the configuration to fail to load or may significantly impact system performance after upgrade">&starf;</span></td></tr>
  <tr valign="top">
    <td><a href="#A1566745-1">1566745-1</a></td>
    <td>3-Major</td>
    <td></td>
    <td>L3VirtualAddress set to ALWAYS advertise will not advertise if there is no associated Stack behind it</td></tr>
  <tr valign="top">
    <td><a href="#A1495017">1495017</a></td>
    <td>3-Major</td>
    <td></td>
    <td>BIG-IP Next Hostname, Group Name and FQDN name should adhere to RFC 1123 specification</td></tr>
  <tr valign="top">
    <td><a href="#A1495005">1495005</a></td>
    <td>3-Major</td>
    <td></td>
    <td>Cannot create Global Resiliency Group with multiple instances if the DNS instances have same hostname</td></tr>
  <tr valign="top">
    <td><a href="#A1494997">1494997</a></td>
    <td>3-Major</td>
    <td></td>
    <td>Deleting a GSLB instance results in record creation of GR group in BIG-IP Next Central Manager</td></tr>
  <tr valign="top">
    <td><a href="#A1491197">1491197</a></td>
    <td>3-Major</td>
    <td></td>
    <td>Server Name (TLS ClientHello) Condition in policy shouldn&#39;t be allowed when &quot;Enable UDP&quot; option is selected in application under Protocols &amp; Profiles</td></tr>
  <tr valign="top">
    <td><a href="#A1491121">1491121</a></td>
    <td>3-Major</td>
    <td></td>
    <td>Patching a new application service&#39;s parameters overwrites entire application service parameters</td></tr>
  <tr valign="top">
    <td><a href="#A1489945">1489945</a></td>
    <td>3-Major</td>
    <td></td>
    <td>HTTPS applications with self-signed certificates traffic is not working after upgrading BIG-IP Next instances to new version of BIG-IP Next Central Manager<span title="This issue may cause the configuration to fail to load or may significantly impact system performance after upgrade">&starf;</span></td></tr>
  <tr valign="top">
    <td><a href="#A1474801">1474801</a></td>
    <td>3-Major</td>
    <td></td>
    <td>BIG-IP Next Central Manager creates a default VRF for all VLANS of the onboarded Next device</td></tr>
  <tr valign="top">
    <td><a href="#A1403861">1403861</a></td>
    <td>3-Major</td>
    <td></td>
    <td>Data metrics and logs will not be migrated when upgrading BIG-IP Next Central Manager from 20.0.2 to a later release</td></tr>
  <tr valign="top">
    <td><a href="#A1366321-1">1366321-1</a></td>
    <td>3-Major</td>
    <td></td>
    <td>BIG-IP Next Central Manager behind a forward-proxy</td></tr>
  <tr valign="top">
    <td><a href="#A1365433">1365433</a></td>
    <td>3-Major</td>
    <td></td>
    <td>Creating a BIG-IP Next instance on vSphere fails with &quot;login failed with code 501&quot; error message<span title="This issue may cause the configuration to fail to load or may significantly impact system performance after upgrade">&starf;</span></td></tr>
  <tr valign="top">
    <td><a href="#A1360121-1">1360121-1</a></td>
    <td>3-Major</td>
    <td></td>
    <td>Unexpected virtual server behavior due to removal of objects unsupported by BIG-IP Next</td></tr>
  <tr valign="top">
    <td><a href="#A1360097-1">1360097-1</a></td>
    <td>3-Major</td>
    <td></td>
    <td>Migration highlights and marks &quot;net address-list&quot; as unsupported, but addresses are converted to AS3 format</td></tr>
  <tr valign="top">
    <td><a href="#A1360093-1">1360093-1</a></td>
    <td>3-Major</td>
    <td></td>
    <td>Abbreviated IPv6 destination address attached to a virtual server is not converted to AS3 format</td></tr>
  <tr valign="top">
    <td><a href="#A1359209-1">1359209-1</a></td>
    <td>3-Major</td>
    <td></td>
    <td>The health of application service shown as &quot;Good&quot; when deployment fails as a result of invalid iRule syntax</td></tr>
  <tr valign="top">
    <td><a href="#A1358985-1">1358985-1</a></td>
    <td>3-Major</td>
    <td></td>
    <td>Failed deployment of migrated application services to a BIG-IP Next instance</td></tr>
  <tr valign="top">
    <td><a href="#A1355605">1355605</a></td>
    <td>3-Major</td>
    <td></td>
    <td>&quot;NO DATA&quot; is displayed when setting names for appliction services, virtual servers and pools, that exceed max characters</td></tr>
  <tr valign="top">
    <td><a href="#A1314617">1314617</a></td>
    <td>3-Major</td>
    <td></td>
    <td>Deleting an interface on a running BIG-IP Next instance can cause the system to behave unexpectedly</td></tr>
  <tr valign="top">
    <td><a href="#A1134225">1134225</a></td>
    <td>3-Major</td>
    <td> <a href="https://my.f5.com/manage/s/article/K000138849" target="_blank">K000138849</a></td>
    <td>AS3 declarations with a SNAT configuration do not get removed from the underlying configuration as expected</td></tr>
  <tr valign="top">
    <td><a href="#A1122689-3">1122689-3</a></td>
    <td>3-Major</td>
    <td></td>
    <td>Cannot modify DNS configuration for a BIG-IP Next VE instance through API</td></tr>
  <tr valign="top">
    <td><a href="#A1694333">1694333</a></td>
    <td>4-Minor</td>
    <td></td>
    <td>Incorrect VRF VLANs when clicking on badge to show a view of VLANs</td></tr>
  <tr valign="top">
    <td><a href="#A1660913-1">1660913-1</a></td>
    <td>4-Minor</td>
    <td></td>
    <td>For API workflows switching between /declare and /documents is unsupported.</td></tr>
  <tr valign="top">
    <td><a href="#A1634929">1634929</a></td>
    <td>4-Minor</td>
    <td></td>
    <td>Parameter names in api documentation is invalid for metrics api</td></tr>
  <tr valign="top">
    <td><a href="#A1633569-1">1633569-1</a></td>
    <td>4-Minor</td>
    <td></td>
    <td>Default values for new entities in an attached OpenAPI file do not match the policy&rsquo;s current configuration</td></tr>
  <tr valign="top">
    <td><a href="#A1629537">1629537</a></td>
    <td>4-Minor</td>
    <td></td>
    <td>Logged-in admin user will not be able to change password before Central Manager setup</td></tr>
  <tr valign="top">
    <td><a href="#A1615261">1615261</a></td>
    <td>4-Minor</td>
    <td></td>
    <td>Application page may show &quot;No Data&quot; for Active Alerts instead of zero.</td></tr>
  <tr valign="top">
    <td><a href="#A1593745">1593745</a></td>
    <td>4-Minor</td>
    <td></td>
    <td>Issues identified during Backup, Restore, and User Operations between two BIG-IP Next Central Managers for Standalone and High Availability Nodes.</td></tr>
  <tr valign="top">
    <td><a href="#A1588813-1">1588813-1</a></td>
    <td>4-Minor</td>
    <td></td>
    <td>CM Restore on a 3 node BIG-IP Next Central Manager with external storage fails with ES errors</td></tr>
  <tr valign="top">
    <td><a href="#A1588101-1">1588101-1</a></td>
    <td>4-Minor</td>
    <td></td>
    <td>Any changes made on the BIG-IP Next Central Manager after the BIG-IP Next instance backup will not be reflected on the BIG-IP Next Central Manager once the BIG-IP Next instance is restored.</td></tr>
  <tr valign="top">
    <td><a href="#A1581877">1581877</a></td>
    <td>4-Minor</td>
    <td></td>
    <td>An error is seen when no device certificates are present on the BIG-IP Next Instance</td></tr>
  <tr valign="top">
    <td><a href="#A1576273">1576273</a></td>
    <td>4-Minor</td>
    <td></td>
    <td>No L1-Networks in an instance causes BIG-IP Next Central Manager upgrade to v20.2.0 to fail<span title="This issue may cause the configuration to fail to load or may significantly impact system performance after upgrade">&starf;</span></td></tr>
  <tr valign="top">
    <td><a href="#A1575549">1575549</a></td>
    <td>4-Minor</td>
    <td></td>
    <td>BIG-IP Next Central Manager discovery requires an instance to have both Default L2-Network and Default L3-Network if either one already exists</td></tr>
  <tr valign="top">
    <td><a href="#A1574997">1574997</a></td>
    <td>4-Minor</td>
    <td></td>
    <td>BIG-IP Next Central Manager HA node installation requires logout to add node<span title="This issue may cause the configuration to fail to load or may significantly impact system performance after upgrade">&starf;</span></td></tr>
  <tr valign="top">
    <td><a href="#A1560605">1560605</a></td>
    <td>4-Minor</td>
    <td></td>
    <td>Global Resiliency functionality fails to meet expectations on Safari browsers</td></tr>
  <tr valign="top">
    <td><a href="#A1498421">1498421</a></td>
    <td>4-Minor</td>
    <td></td>
    <td>Restoring Central Manager (VE) with KVM HA Next instance fails on a new BIG-IP Next Central Manager</td></tr>
  <tr valign="top">
    <td><a href="#A1498121">1498121</a></td>
    <td>4-Minor</td>
    <td></td>
    <td>BIG-IP Next Central Manager upgrade alerts not visible in global bell icon</td></tr>
  <tr valign="top">
    <td><a href="#A1365445">1365445</a></td>
    <td>4-Minor</td>
    <td></td>
    <td>Creating a BIG-IP Next instance on vSphere fails with &quot;login failed with code 401&quot; error message<span title="This issue may cause the configuration to fail to load or may significantly impact system performance after upgrade">&starf;</span></td></tr>
  <tr valign="top">
    <td><a href="#A1365417">1365417</a></td>
    <td>4-Minor</td>
    <td></td>
    <td>Creating a BIG-IP Next VE instance in vSphere fails when a backslash character is in the provider username<span title="This issue may cause the configuration to fail to load or may significantly impact system performance after upgrade">&starf;</span></td></tr>
  <tr valign="top">
    <td><a href="#A1360709">1360709</a></td>
    <td>4-Minor</td>
    <td></td>
    <td>Application page can show an error alert that includes &quot;FAST delete task failed for application&quot;</td></tr>
  <tr valign="top">
    <td><a href="#A1360621">1360621</a></td>
    <td>4-Minor</td>
    <td></td>
    <td>Adding a Control Plane VLAN must be done only during BIG-IP Next HA instance creation</td></tr>
  <tr valign="top">
    <td><a href="#A1354645">1354645</a></td>
    <td>4-Minor</td>
    <td></td>
    <td>Error displays when clicking &quot;Edit&quot; on the Instance Properties panel</td></tr>
  <tr valign="top">
    <td><a href="#A1350365">1350365</a></td>
    <td>4-Minor</td>
    <td></td>
    <td>Performing licensing changes directly on a BIG-IP Next instance</td></tr>
  <tr valign="top">
    <td><a href="#A1325713">1325713</a></td>
    <td>4-Minor</td>
    <td></td>
    <td>Monthly backup cannot be scheduled for the days 29, 30, or 31</td></tr>
</table>
<br><h2><u>Known Issue details for BIG-IP Next v20.3.0</u></h2>
<h4><a name="A1696253-1" rel="nofollow"></a>1696253-1 : Failed to upload Instance QKView file to iHealth from BIG-IP Next Central Manager</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>The upload of an Instance QKView generated from BIG-IP Next Central Manager fails if the file size exceeds 64 MB.</p>
<p><strong>Conditions:</strong><br>Attempting to upload a BIG-IP Next Instance QKView file that is 64 MB or larger to iHealth.</p>
<p><strong>Impact:</strong><br>Uploading QKView files larger than 64 MB to iHealth fails.</p>
<p><strong>Workaround:</strong><br>Follow these steps to resolve the issue:<br>
<br>
1. The QKView that failed to upload will still be created locally. Select the failed QKView and click the download button to save the generated QKView file to your local machine.<br>
2. Log in to iHealth (https://account.f5.com/ihealth2) using your iHealth credentials.<br>
3. Upload the QKView file from your local machine to the iHealth website, specifying a case number if applicable.<br>
<br>
Note: If the QKView file is not generated when you click the download button, attempt to generate a new QKView file.</p>
<hr>
<h4><a name="A1696161-1" rel="nofollow"></a>1696161-1 : Unable to update the OAuth client configuration</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>Updating the value in the text input field causes it to be saved as a string, resulting in the following error:<br>
<br>
&quot;Request body has an error: doesn&#39;t match the schema: doesn&#39;t match schema due to: doesn&#39;t match schema due to: Error at &#39;/server<br>
type&#39;: value is not one of the allowed values.&quot;</p>
<p><strong>Conditions:</strong><br>Create an access policy with the OAuth Federation. Change the default value of the &quot;Token Validation Interval&quot; on the OAuth Server or the &quot;Access Token Expires In&quot; on the OAuth Provider.</p>
<p><strong>Impact:</strong><br>An error occurs when saving the policy if the default values are changed.</p>
<p><strong>Workaround:</strong><br>The user needs to update the &quot;Token Validation Interval&quot; on the OAuth Server or the &quot;Access Token Expires In&quot; on the OAuth Provider for an access policy using the API.</p>
<hr>
<h4><a name="A1696129-1" rel="nofollow"></a>1696129-1 : Network Interface Instance Data Metrics show all available interfaces rather than only those that are used</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>Network Interface Instance Data Metrics show all available interfaces rather than showing only those that are used.<br>
<br>
Additionally, the metrics charts for the unused interfaces are shown incorrectly with data.</p>
<p><strong>Conditions:</strong><br>Viewing data for a BIG-IP Next instance running on rSeries with at least one application deployed.</p>
<p><strong>Impact:</strong><br>Network Interface Instance Data Metrics show all available interfaces rather than showing only those that are used.<br>
<br>
The data metrics that are shown in the UI for the L1-Network interfaces that are not used will show incorrect data.</p>
<p><strong>Workaround:</strong><br>None</p>
<hr>
<h4><a name="A1696113-1" rel="nofollow"></a>1696113-1 : Save button on application configure network may not be enabled.</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>When all vlans are removed from the list of vlans to listen on the save button will be disabled.</p>
<p><strong>Conditions:</strong><br>There must be vlans listed in the selection box, removing all of them will disable the save button.</p>
<p><strong>Impact:</strong><br>Removing all vlans will not allow the user to disable the &quot;Enable VLANS&quot; option by saving the page.</p>
<p><strong>Workaround:</strong><br>You may disable the &quot;Enable Vlans&quot; option but only with vlans selected in the select box. <br>
In other words, do not delete vlans before disabling vlan filtering, in order to enable the Save button and update the config to disable vlan filtering.</p>
<hr>
<h4><a name="A1695977-1" rel="nofollow"></a>1695977-1 : BGP is unsupported in High Availability (HA) Mode on VELOS</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>BGP connections will not be formed and routes will not be shared with BGP peers.</p>
<p><strong>Conditions:</strong><br>A pair of BIG-IP Next instances running on a VELOS chassis in High Availability (HA) mode will not send BGP peer requests to BGP neighbors. This issue occurs after HA pairs are established on both active and standby instances.</p>
<p><strong>Impact:</strong><br>Traffic will fail to reach the intended applications because BGP routes from the BIG-IP Next instance will not be advertised to BGP peers.</p>
<p><strong>Workaround:</strong><br>None</p>
<hr>
<h4><a name="A1695873-1" rel="nofollow"></a>1695873-1 : BIG-IP Next Central Manager does not load as expected after initial password change<span title="This issue may cause the configuration to fail to load or may significantly impact system performance after upgrade">&starf;</span></h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>After updating the initial password, the BIG-IP Next Central Manager may not load as expected.</p>
<p><strong>Conditions:</strong><br>Fresh installation of BIG-IP Next Central Manager and initial password is changed.</p>
<p><strong>Impact:</strong><br>BIG-IP Next Central Manager information may not load as expected.</p>
<p><strong>Workaround:</strong><br>Log out and log in again to the BIG-IP Next Central Manager.</p>
<hr>
<h4><a name="A1695053-1" rel="nofollow"></a>1695053-1 : QKView Generation may fail if upgrade file remains on BIG-IP Next Instance</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>After upgrading a BIG-IP Next instance, if the upgrade file remains on the filesystem of the instance, a QKView request may fail.</p>
<p><strong>Conditions:</strong><br>A large file is stored in the BIG-IP Next file system.</p>
<p><strong>Impact:</strong><br>Cannot create QKView requests.</p>
<p><strong>Workaround:</strong><br>Delete the large file from the BIG-IP Next instance using a Central Manager API call and ensure successful QKView generation, follow these steps:<br>
<br>
1. Send a DELETE request to:<br>
https://{{cm_mgmt_ip}}/api/device/v1/proxy/{{instance_id}}?path=files/{{file_id}}<br>
<br>
2. Wait 25 minutes before submitting the QKView request.</p>
<hr>
<h4><a name="A1694333" rel="nofollow"></a>1694333 : Incorrect VRF VLANs when clicking on badge to show a view of VLANs</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>When clicking on the badge in the second column of the VRF&#39;s grid, the VLAN column, the VLANs shown will always be the same regardless of which VRF is chosen.</p>
<p><strong>Conditions:</strong><br>More than 1 VLAN exists (including the Default VRF).</p>
<p><strong>Impact:</strong><br>Incorrect information is displayed.</p>
<p><strong>Workaround:</strong><br>View VLANs directly in the VRF by clicking the VRF Name column. To avoid misinformation, do not dive into the VRF VLANs via the badge.</p>
<hr>
<h4><a name="A1694325-1" rel="nofollow"></a>1694325-1 : Unable to save &quot;new&quot; HTTPS client-side certificates on HTTP2 app</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>The save button is grayed out and you are unable click Save when adding new HTTPS client certificates to a HTTP2 app.</p>
<p><strong>Conditions:</strong><br> - Using Central Manager.<br>
- Having an HTTP2 app with Client-side TLS and Server-side TLS deployed to an instance.<br>
- Updating or replacing the Client-side TLS.</p>
<p><strong>Impact:</strong><br>You are unable to save the updated application.</p>
<p><strong>Workaround:</strong><br>Change something on the Server-side TLS and click Save for that change. Revert the change on the Server-side TLS and click Save. Click Save for Protocols &amp; Profiles.</p>
<hr>
<h4><a name="A1694241-1" rel="nofollow"></a>1694241-1 : CM Disconnected mode: Module provision is failing</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>While Central Manager is operating in disconnected mode, when you trigger a provisioning request and then start a second provisioning request before Central Manager completes previous provisioning, ack submission will fail. <br>
<br>
Note: <br>
<br>
In case of connected mode, CM UI allows provisioning request in sequential order. Second provisioning will be allowed only after completion of first provisioning request.</p>
<p><strong>Conditions:</strong><br> -- Central Manager<br>
-- Licensing mode is configured for &quot;Disconnected mode&quot;<br>
-- Provision a second module before you have completed the Ack verification of the first transaction</p>
<p><strong>Impact:</strong><br>The updated report is not applied to the instance.</p>
<p><strong>Workaround:</strong><br>Activate one feature at a time, and perform Ack verification in sequential order.</p>
<hr>
<h4><a name="A1692233-1" rel="nofollow"></a>1692233-1 : AS3 Declaration fails to update serverTLS/clientTLS when multiple SSL profiles are configured</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>The migrated application does not contain serverTLS and/or clientTLS property, which needs updating:<br>
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;Common_virtual_multi_ssl&quot;: {<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;class&quot;: &quot;Service_TCP&quot;,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;persistenceMethods&quot;: [],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;profileTCP&quot;: {<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;use&quot;: &quot;/tenantc3d2758c11a68/Common_virtual_multi_ssl/tcp_default_v14&quot;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;serverTLS&quot;: &quot;&lt;Choose your SSL client-side profile&gt;&quot;,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;snat&quot;: &quot;none&quot;,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;virtualAddresses&quot;: [<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;10.10.10.21&quot;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;virtualPort&quot;: 443<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;},</p>
<p><strong>Conditions:</strong><br>The virtual server includes multiple SSL profiles on the client and/or server side.<br>
<br>
Example:<br>
<br>
ltm virtual /tenantf0154bc117746/Common_virtual_multi_ssl/Common_virtual_multi_ssl {<br>
&nbsp;&nbsp;&nbsp;&nbsp;creation-time 2024-10-04:03:20:50<br>
&nbsp;&nbsp;&nbsp;&nbsp;destination /tenantf0154bc117746/Common_virtual_multi_ssl/10.10.10.21:443<br>
&nbsp;&nbsp;&nbsp;&nbsp;ip-protocol tcp<br>
&nbsp;&nbsp;&nbsp;&nbsp;last-modified-time 2024-10-04:03:20:50<br>
&nbsp;&nbsp;&nbsp;&nbsp;mask 255.255.255.255<br>
&nbsp;&nbsp;&nbsp;&nbsp;profiles {<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;/tenantf0154bc117746/Common_virtual_multi_ssl/ssl_prof_client_ecdsa {<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;context clientside<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;/tenantf0154bc117746/Common_virtual_multi_ssl/ssl_prof_client_rsa {<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;context clientside<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;/tenantf0154bc117746/Common_virtual_multi_ssl/tcp_default_v14 { }<br>
&nbsp;&nbsp;&nbsp;&nbsp;}<br>
&nbsp;&nbsp;&nbsp;&nbsp;source 0.0.0.0/0<br>
&nbsp;&nbsp;&nbsp;&nbsp;translate-port enabled<br>
}<br>
<br>
Saved AS3 application:<br>
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&quot;Common_virtual_multi_ssl&quot;: {<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;Common_virtual_multi_ssl&quot;: {<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;class&quot;: &quot;Service_TCP&quot;,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;persistenceMethods&quot;: [],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;profileTCP&quot;: {<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;use&quot;: &quot;/tenantf0154bc117746/virtual_multi_ssl/tcp_default_v14&quot;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;snat&quot;: &quot;none&quot;,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;virtualAddresses&quot;: [<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;10.10.10.21&quot;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;virtualPort&quot;: 443<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;},</p>
<p><strong>Impact:</strong><br>The virtual server configured with multiple SSL profiles fails to link to any SSL profile.</p>
<p><strong>Workaround:</strong><br>Update the saved Application Service with the serverTLS and/or clientTLS property:<br>
&quot;serverTLS&quot;: &quot;&lt;Choose your SSL client-side profile&gt;&quot;<br>
<br>
and replace &lt;Choose your SSL client-side profile&gt; with a valid TLS_Server class object name.</p>
<hr>
<h4><a name="A1692209-1" rel="nofollow"></a>1692209-1 : Central Manager backup fails after upgrade</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>When you set up NFS external storage, the ownership of the /opt/cm-backup and /opt/cm-qkview directories is changed from admin:admin to root:root. This causes Central Manager backups and QKViews to fail.</p>
<p><strong>Conditions:</strong><br>NFS external storage was configured before the upgrade.</p>
<p><strong>Impact:</strong><br>Backups and QKViews in BIG-IP Next Central Manager may fail.</p>
<p><strong>Workaround:</strong><br>Run the following commands in the BIG-IP Next Central Manager CLI to correct ownership:<br>
- sudo chown -R admin:admin /opt/cm-backup<br>
- sudo chown -R admin:admin /opt/cm-qkview</p>
<hr>
<h4><a name="A1691633-1" rel="nofollow"></a>1691633-1 : VELOS upgrade error alert states status 400 when image is not present or ready on tenant<span title="This issue may cause the configuration to fail to load or may significantly impact system performance after upgrade">&starf;</span></h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>Global alert raised stating: &quot;Initializing upgrade fails with status 400&quot;. This does not communicate sufficiently what went wrong and what the user should do next.</p>
<p><strong>Conditions:</strong><br>When a user triggers a VELOS instance upgrade and specifies an image that is not present or ready on the tenant.</p>
<p><strong>Impact:</strong><br>User will not know that they need to do next to get a successful upgrade.</p>
<p><strong>Workaround:</strong><br>Check that the image specified when triggering upgrade is present and ready on the specified VELOS tenant and try upgrade again.</p>
<hr>
<h4><a name="A1689457-1" rel="nofollow"></a>1689457-1 : Potential Issues with Debug Utility Activation for Users with Underscores (_) in Usernames.</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>Enabling debug utility will fail from Central Manager with the error &quot;Failed to read request Query Param Error at: username. Reason: string does not match the regular expression &#xFF3C;&quot;^[a-z][-a-z0-9]*$&#xFF3C;&quot;&quot;&quot;</p>
<p><strong>Conditions:</strong><br> - User with username that does not follow regex pattern &#39;^[a-z][-a-z0-9]*$&#xFF3C;&#39;.<br>
- Trying to enable debug utility from Central Manager.</p>
<p><strong>Impact:</strong><br>User can not enable debug utility from Central Manager.</p>
<p><strong>Workaround:</strong><br>1. A username should be created using a combination of uppercase letters (A-Z), lowercase letters (a-z), and numbers (0-9) to enable the debug utility. Although the following are allowed, avoid using them as they will impact enabling the debug utility: <br>
- underscores (_), dashes (-) or dots (.).<br>
- starting the username with an uppercase letter or number.<br>
<br>
2. For users with a dash (-) in their username, enable the debug utility using OpenAPI.</p>
<hr>
<h4><a name="A1689421-1" rel="nofollow"></a>1689421-1 : Upgrade of BIG-IP Next instance may require reestablishing trust<span title="This issue may cause the configuration to fail to load or may significantly impact system performance after upgrade">&starf;</span></h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>When upgrading a BIG-IP Next instance to the 20.3.0 release, it is possible that the certificate on the BIG-IP Next instance temporarily changes. When this happens, it requires the user to trust the temporary certificate to complete the upgrade and then to reestablish trust with the original certificate once the upgrade completes.</p>
<p><strong>Conditions:</strong><br>Upgrading BIG-IP Next instance to the 20.3.0 release.</p>
<p><strong>Impact:</strong><br>User intervention may be required to complete the upgrade and ensure the instance can continue to be managed by BIG-IP Next Central Manager after the upgrade. You may need to do this intervention multiple times.</p>
<p><strong>Workaround:</strong><br>If the BIG-IP Next instance upgrade process prompts to trust a new certificate, use the provided button to accept the new certificate.<br>
<br>
If the BIG-IP Next instance shows up with a health status of UNKNOWN at any point after the upgrade, navigate to the instance properties screen and select the Certificates section. Ignore the errors that show up on these screens and click the Establish Trust button. When prompted, accept this certificate fingerprint. After the operation completes, click the Cancel &amp; Exit button and wait for the next health update to bring the instance back to a valid health status.</p>
<hr>
<h4><a name="A1682089-1" rel="nofollow"></a>1682089-1 : Configuration migration of an access policy from BIG-IP 17.1.0 or above to BIG-IP Next 20.3.0 Access is causing an invalid login page</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>Configuration migration of an Access Policy from BIG-IP 17.1.0 or above to BIG-IP Next 20.3.0 Access is causing an invalid login page.</p>
<p><strong>Conditions:</strong><br>Using the application migration tool to migrate the access policy configuration from BIG-IP 17.1.0 or above to BIG-IP Next 20.3.0, customization objects are converted with empty strings.</p>
<p><strong>Impact:</strong><br>The login Page is not loading as expected.</p>
<p><strong>Workaround:</strong><br>Manually edit the access policy configuration to supply the customization strings by referring to the BIG-IP 17.1.0 config file. Later, use the BIG-IP Next Central Manager to save the access policy and then deploy the application.</p>
<hr>
<h4><a name="A1682085-1" rel="nofollow"></a>1682085-1 : OAuth Resource Server agent fails to deploy when using a private key to decrypt the access token</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>The Scope agent validates the received Access Token against a list of JWT providers. Each provider has an associated JWT configuration.<br>
<br>
A known issue occurs when the OAuth Resource Server agent fails to deploy and shows a pre-deploy error if the user uploads a private key to decrypt the token.</p>
<p><strong>Conditions:</strong><br>The Resource Server does not use the ID token and only requires the Access Token. Its primary function is token verification.<br>
Even when the Access Token is attached, the API payload is missing the Access Token key, which results in a pre-deploy error.<br>
<br>
Steps to Reproduce:<br>
<br>
1. Create an Access policy with the OAuth Federation Resource Server, and set the validation mode to internal.<br>
2. Choose JWE encryption and attach the private key for the Access Token.<br>
3. Save and deploy the policy.</p>
<p><strong>Impact:</strong><br>An Access Policy with OAuth Federation will fail for F5 as a Resource Server when using internal validation mode.</p>
<p><strong>Workaround:</strong><br>Using the API, add the private keys to the allowedKeys field under jwtConfig.</p>
<hr>
<h4><a name="A1682029-1" rel="nofollow"></a>1682029-1 : Application and instances graphs may show traffic spikes after HA failover or active node shutdown</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>A spike in the instance/application graphs might be shown after failover or active node shutdown.</p>
<p><strong>Conditions:</strong><br>BIG-IP Next high availability (HA) pair switched from active node to standby node.</p>
<p><strong>Impact:</strong><br>The instance/application graphs may show a spike in traffic.</p>
<p><strong>Workaround:</strong><br>None.</p>
<hr>
<h4><a name="A1682021-1" rel="nofollow"></a>1682021-1 : Unable to save Service Provider configuration changes in the SAML Federation rule</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>The Disable option in the Service Provider configuration does not remove the data from the access policy.</p>
<p><strong>Conditions:</strong><br>This issue occurs under the following conditions:<br>
-- Create an access policy and add a SAML Federation rule.<br>
-- Set the Service Provider configuration to Advanced and enable certain configuration options.<br>
-- Set the value on the Providers page.<br>
-- Save the new policy.<br>
-- Reopen the policy from the policy list.<br>
-- Disable Service Provider configuration options on the SAML Federation Rule Properties page.<br>
-- Save the edited policy.<br>
-- Reopen the policy. However, the changes to the Service Provider configuration have not been saved.</p>
<p><strong>Impact:</strong><br>Changes to the SAML Federation configuration cannot be saved.</p>
<p><strong>Workaround:</strong><br>After disabling the option in the SAML Federation configuration, the user needs to delete and recreate the Service Providers or Identity Providers on the SAML Federation Providers page.</p>
<hr>
<h4><a name="A1682017-1" rel="nofollow"></a>1682017-1 : A possible gap in app/instance graphs might be shown after HA failover/blade shutdown</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>instance metrics are received on a fixed interval of 30 seconds and application metrics are received on a fixed interval of 2 minutes.<br>
<br>
When a failover occurs, switching from one node to the other causes a temporarily disconnection between BIG-IP Next and Central Manager. This can cause a gap to occur in metrics reporting.</p>
<p><strong>Conditions:</strong><br>BIG-IP Next HA pair switches from one node to the other.</p>
<p><strong>Impact:</strong><br>A gap in the app/instance graphs will occur.</p>
<p><strong>Workaround:</strong><br>None</p>
<hr>
<h4><a name="A1680361-1" rel="nofollow"></a>1680361-1 : Expression of the first branch missed after a Rule node was moved and saved</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>After moving a rule node within the same flow and saving the policy, when it was reopened, the moved nodes did not have accurate branch expressions.</p>
<p><strong>Conditions:</strong><br>Move a Rule node from one point to another inside a Flow.</p>
<p><strong>Impact:</strong><br>This may cause errors during policy deployment, and incorrect branch expressions could also impact traffic flow.</p>
<p><strong>Workaround:</strong><br>This enhancement is designed to help users rearrange the policy without deleting the nodes. Even if an error occurs while moving the nodes, users can still be able to create the policy using the traditional method of dropping nodes from the sidebar.</p>
<hr>
<h4><a name="A1680189-1" rel="nofollow"></a>1680189-1 : Creating instances does not enable the &quot;Default VRF&quot; field on VLANs by default</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>BIG-IP Next supports Virtual Routing and Forwarding (VRF), but it must be configured when the networks are being created. If you do not choose a default VRF this can lead to traffic issues later when applications are created.</p>
<p><strong>Conditions:</strong><br>Setting up networking for a new BIG-IP Next instance.<br>
<br>
When creating a new BIG-IP Next instance, you are prompted to set the instance&#39;s network settings, including Self IPs, VLANs, and L1 Networks. The VLANs section includes a checkbox labeled &quot;Default VRF&quot;. This checkbox is unchecked by default and can lead to an empty list of L3 Networks.</p>
<p><strong>Impact:</strong><br>No VLANs get marked as a default VRF, and this setting may be unchangeable after the instance is created.</p>
<p><strong>Workaround:</strong><br>If you do have any VLANS set as a Default VRF, you will not pass any traffic.  When deploying an application, the application will use your Default VRFs by default, but you may select a different VRF if you have created multiple VRFs.<br>
<br>
When creating your isntance, check the &quot;Default VRF&quot; checkbox for VLANs as appropriate when creating a new instance. If you&#39;re uncertain which VLAN to select, select your instance&#39;s External VLAN.</p>
<hr>
<h4><a name="A1679977-1" rel="nofollow"></a>1679977-1 : Creating multiple L1-Networks with same names will only create one L1-Network</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>Creating multiple L1-Networks with same names will only create one L1-Network in instance; however, Central Manager reports that multiple L1-Networks are successfully created.</p>
<p><strong>Conditions:</strong><br>Creating multiple L1-Networks with the same names</p>
<p><strong>Impact:</strong><br>Instance will only create one L1-Network; however, CM reports that multiple L1-Networks are successfully created.</p>
<p><strong>Workaround:</strong><br>Workaround (mitigation) if you have not created the L1-Networks: do not create multiple L1-Networks with same names.<br>
<br>
Workaround if you have already created the L1-Networks:<br>
1. Login to CM using CM login API<br>
2. Grab the instance ID that the L1-Networks have been created by using the GET /api/v1/spaces/default/instances API.<br>
3. Retrieve all the L1-Networks directly in the instance by requesting a GET request to &quot;api/device/v1/proxy/{INSTANCE_ID}?path=/L1-networks&quot;<br>
4. Go to CM UI and delete all the L1-Networks that are not included in step #3</p>
<hr>
<h4><a name="A1679593-1" rel="nofollow"></a>1679593-1 : User should provide an IPv4 address space when an IPv6 address space is provided</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>IPv6-only address space is not supported. You must configure either IPv4 or both IPv4 and IPv6 for split tunneling on Central Manager or BIG-IP Next.</p>
<p><strong>Conditions:</strong><br>This issue occurs under the following conditions:<br>
-- Split tunneling is enabled on the Network Access resource.<br>
-- Only an IPv6 address is specified in the Include Address Space field.<br>
-- The application containing the access policy is deployed.</p>
<p><strong>Impact:</strong><br>BIG-IP Next Central Manager accepts the configuration, but the Edge Client gets stuck on Initializing when trying to connect.</p>
<p><strong>Workaround:</strong><br>Add an IPv4 address space with the IPv6 address space in the Include Address Spaces field under Split Tunneling in the Network Access resource.</p>
<hr>
<h4><a name="A1678817-1" rel="nofollow"></a>1678817-1 : Intermittent failure in Data Group deployment when added from SSL Orchestrator policy</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>When you add two or more data groups to an SSL Orchestrator policy and deploy together, it fails to deploy any one of them.</p>
<p><strong>Conditions:</strong><br>Adding data groups to an SSL Orchestrator policy.</p>
<p><strong>Impact:</strong><br>The policy deployment fails hence the application deployment may also fail for applications which have an SSL Orchestrator policy attached to them.</p>
<p><strong>Workaround:</strong><br>Retry application deployment:<br>
<br>
1) Retry application deployment will redeploy the data group and policy.<br>
2) If the retry deployment fails, create a new policy with the same data groups. Detach the existing policy from the application and attach the new application and retry deployment.</p>
<hr>
<h4><a name="A1678677-1" rel="nofollow"></a>1678677-1 : Re-discover Active node</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>The health state of a HA instance may transition to Unknown when attempting to re-discover using the active device&#39;s management IP address when the HA pair was already discovered using the floating management IP address by Central Manager.</p>
<p><strong>Conditions:</strong><br>HA instance is managed on Central Manager.</p>
<p><strong>Impact:</strong><br>The health state of a HA instance may transition to Unknown</p>
<p><strong>Workaround:</strong><br>Establish Trust by clicking the name of the instance, Click Certificates, Click Establish Trust</p>
<hr>
<h4><a name="A1678561-1" rel="nofollow"></a>1678561-1 : Application health remains stuck in Unknown state.</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>After a successful application deployment the health state does not change to another state in several minutes. The application health remains in &quot;Unknown&quot; state.</p>
<p><strong>Conditions:</strong><br>A new application deployment will start with a health state of &quot;Unknown&quot; as the data plane is being configured.</p>
<p><strong>Impact:</strong><br>When an application health state remains in &quot;Unknown&quot; it is an indication of a configuration error.</p>
<p><strong>Workaround:</strong><br>Find the configuration error: review the f5-fsm-tmm log by way of qkview, debug-sidecar or 3rd party logging tool for configuration errors. Configuration errors can be found by searching for keyword &quot;TMC ERROR&quot; in f5-fsm-tmm log. If an error has been found, delete the application that is in the unknown state. Resolve the configuration error and create new app.</p>
<hr>
<h4><a name="A1678453-1" rel="nofollow"></a>1678453-1 : A high number of application creations and deletions can cause frequent Out of Memory (OOM) errors for WebSSO</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>When hundreds or thousands of applications are created, deployed, and deleted rapidly, the WebSSO container can run out of memory, leading to a loop where the OOM-killer is triggered, causing repeated WebSSO restarts.</p>
<p><strong>Conditions:</strong><br>Rapid creation, deployment, and deletion of hundreds, or thousands of applications.</p>
<p><strong>Impact:</strong><br>The BIG-IP Next instance can become unresponsive.</p>
<p><strong>Workaround:</strong><br> -- Create an application and deploy it, ensuring that you check for 200 OK responses.<br>
-- Before creating the next application, poll the link provided in the response waiting for the deployment task status to change from &quot;pending&quot;. Once the application has been successfully deployed, the cycle restarts.</p>
<hr>
<h4><a name="A1678009-1" rel="nofollow"></a>1678009-1 : CM may not display the complete FQDN if the FQDN is long</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>In the Instance grid, CM may not display the complete FQDN if the FQDN is longer, depending on the screen resolution.</p>
<p><strong>Conditions:</strong><br>When the instance is added using FQDN and when the instance has a long name.</p>
<p><strong>Impact:</strong><br>The user may not view the entire FQDN in the Instance grid page, if they have a smaller resolution.</p>
<p><strong>Workaround:</strong><br>Click on the instance and view the Instance name in the properties section.</p>
<hr>
<h4><a name="A1677913-1" rel="nofollow"></a>1677913-1 : Redeployment fails when CRLDP Responder mode is changed</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>A change in the CRLDP Responder mode triggers a corresponding change in the dependent networking objects.</p>
<p><strong>Conditions:</strong><br>This issue occurs under the following conditions:<br>
-- Deploy the application with an Access policy that includes a CRLDP rule.<br>
-- Update the Access policy to change the CRLDP Authentication Responder mode to a different value and save the policy.<br>
-- Redeploy the application containing the updated Access policy.</p>
<p><strong>Impact:</strong><br>Redeployment of the application failed with an HTTP 400 error.</p>
<p><strong>Workaround:</strong><br>The user needs to delete the deployed application, recreate it with the same policy, and configure the deployment parameters.</p>
<hr>
<h4><a name="A1677537-1" rel="nofollow"></a>1677537-1 : Inspection services endpoints related configuration are not propagated to BIG-IP Next instances during upgrades<span title="This issue may cause the configuration to fail to load or may significantly impact system performance after upgrade">&starf;</span></h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>1. Updates to endpoints configuration of inspection services are not propagated to BIG-IP Next instances. <br>
2. Central Manager shows a different config than what has been configured on the BIG-IP Next instance.</p>
<p><strong>Conditions:</strong><br>1. Deploy an inspection service with only changes to global screens.<br>
2. Update the endpoints configuration on global screen.<br>
3. Deploy the updates without modifying instance specific end points configuration on deployment screen.</p>
<p><strong>Impact:</strong><br>You may notice the traffic is still flowing to previous endpoints configured (updates won&#39;t be propagated)</p>
<p><strong>Workaround:</strong><br>1. Use the deployment screen to update the required configuration.</p>
<hr>
<h4><a name="A1677141" rel="nofollow"></a>1677141 : Updating a L1-Network is not allowed if either HA Control-Plane VLAN or Data-Plane VLAN are part of the same L1-Network</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>Updates to a L1-Network - such as adding/removing a VLAN from defaultVRF, adding/removing a VLAN - are not allowed if either HA Control-Plane VLAN or Data-Plane VLAN are part of the same L1-Network.</p>
<p><strong>Conditions:</strong><br>Creating L1-Network with VLANs including HA Control-Plane VLAN or Data-Plane VLAN and then attempting to update the L1-Network and it&#39;s objects after HA instance is created.</p>
<p><strong>Impact:</strong><br>You cannot update the L1-Network.</p>
<p><strong>Workaround:</strong><br>Prior to create HA, exclude HA VLANs from other VLANs by creating a separate L1-Network for either HA VLANs or the other VLANs</p>
<hr>
<h4><a name="A1674409-1" rel="nofollow"></a>1674409-1 : High API load can render BIG-IP Next unresponsive.</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>Under extremely high API load with FAST applications the BIG-IP Next instance may become unresponsive.</p>
<p><strong>Conditions:</strong><br>When sending 1000s of applications requests or deleting 1000s of applications without sufficient rate limiting.</p>
<p><strong>Impact:</strong><br>The BIG-IP Next instance will become unresponsive.</p>
<p><strong>Workaround:</strong><br>For large scale FAST API deployments use the following guidelines.<br>
<br>
-- Create an application and then deploy it being sure to check for 200 OK responses. <br>
-- Before creating the next application, poll the link in the response waiting for the deployment task status change from &quot;pending&quot;. When that application has been successfully deployed, the cycle may be restarted.</p>
<hr>
<h4><a name="A1672109-1" rel="nofollow"></a>1672109-1 : Unable to reach backend application when configured with &quot;host&quot; in Network Access Optimized Application</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>The backend optimized application is not accessible when confugured with FQDN.</p>
<p><strong>Conditions:</strong><br>The optimized application is configured with an FQDN instead of an IP address.</p>
<p><strong>Impact:</strong><br>The backend optimized application is not reachable.</p>
<p><strong>Workaround:</strong><br>Configure with IP address instead of FQDN.</p>
<hr>
<h4><a name="A1671645-1" rel="nofollow"></a>1671645-1 : Interfaces not properly mapped after switching port profiles from 8x10 to 4x25</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>When a BIG-IP Next instance is initially onboarded using a 4x25 port profile, switching to an 8x10 port profile results in improper interface mapping.</p>
<p><strong>Conditions:</strong><br>This issue occurs when a BIG-IP Next instance, initially configured with a 4x25 port profile, is later modified to use an 8x10 port profile.</p>
<p><strong>Impact:</strong><br>Interfaces from the original 4x25 port profile remain active even after attempting to switch to the 8x10 port profile.</p>
<p><strong>Workaround:</strong><br>Avoid changing a BIG-IP Next instance from a 4x25 to an 8x10 port profile. Instead, onboard a new instance directly with the 8x10 port profile configuration.</p>
<hr>
<h4><a name="A1671465" rel="nofollow"></a>1671465 : FAST-0002: Internal Server Error: Unable to render template Examples/http: rpc error: code = Unknown desc = missed comma between flow collection entries</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>Pool members that end with non hex characters, for example 2001:db8::, will fail config validation on Central Manager.</p>
<p><strong>Conditions:</strong><br> - Central Manager<br>
- Deploying app from template<br>
- Pool member uses ip6 address ending in a colon</p>
<p><strong>Impact:</strong><br>Address cannot be used for a pool member.</p>
<p><strong>Workaround:</strong><br>Write full address with zeros.</p>
<hr>
<h4><a name="A1670977-1" rel="nofollow"></a>1670977-1 : The BIG-IP Next Central Manager backup fails when a node becomes unreachable</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>The BIG-IP Next Central Manager (CM) backup may fail if one of the nodes in a High Availability (HA) group becomes unreachable (e.g., due to shutdown).</p>
<p><strong>Conditions:</strong><br> -- The node is shut down and not reachable.<br>
-- The node has not been removed from the group.</p>
<p><strong>Impact:</strong><br>The BIG-IP Next Central Manager backup process will start, but an alert will indicate that the backup has failed.</p>
<p><strong>Workaround:</strong><br>Remove the unreachable node through the GUI.</p>
<hr>
<h4><a name="A1670689-1" rel="nofollow"></a>1670689-1 : BIG-IP Next Central Manager High Availability Installation Failures in High Disk Latency Environments<span title="This issue may cause the configuration to fail to load or may significantly impact system performance after upgrade">&starf;</span></h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>The installation of BIG-IP Next Central Manager may fail with a &ldquo;node not ready&rdquo; status when there is high disk I/O latency.</p>
<p><strong>Conditions:</strong><br>High disk latency, such as when using NAS-mounted disk storage, can cause the High Availability installation of the BIG-IP Next Central Manager to fail.</p>
<p><strong>Impact:</strong><br>The installation of BIG-IP Next Central Manager in High Availability mode will fail.</p>
<p><strong>Workaround:</strong><br>It is recommended to use low-latency block storage devices for local storage volumes on Virtual Machine (VM) instances.</p>
<hr>
<h4><a name="A1668017-1" rel="nofollow"></a>1668017-1 : Cannot add new VLANs to existing HA L1 Network</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>The L1 Network of an HA that contains the control and data plane VLANs cannot be edited.</p>
<p><strong>Conditions:</strong><br>The HA L1 Network that is being edited uses control and data plane VLANs</p>
<p><strong>Impact:</strong><br> -- You cannot edit the L1 network containing the control and data plane VLANs<br>
-- you are unable to add/remove VLANs or self IPs that are in that network</p>
<p><strong>Workaround:</strong><br>Create another L1 network and add the new VLAN to that new L1</p>
<hr>
<h4><a name="A1660913-1" rel="nofollow"></a>1660913-1 : For API workflows switching between /declare and /documents is unsupported.</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>Applications created with /declare or /documents must be managed with the API endpoint that created them. You can&#39;t switch between endpoints.</p>
<p><strong>Conditions:</strong><br>Switching between the /declare and /documents APIs when managing applications.</p>
<p><strong>Impact:</strong><br>Errors will be encountered when switching end points /declare to /documents.</p>
<p><strong>Workaround:</strong><br>Use the same API endpoint when managing applications.</p>
<hr>
<h4><a name="A1644653-1" rel="nofollow"></a>1644653-1 : BIG-IP Next Central Manager displays a Failed status for High Availability (HA) when adding a third node</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>The BIG-IP Next Central Manager displays a Failed status for High Availability when a third node is added to the group.</p>
<p><strong>Conditions:</strong><br>This issue occurs when nodes are added to a standalone BIG-IP Next Central Manager to form a High Availability group.</p>
<p><strong>Impact:</strong><br>The BIG-IP Next Central Manager displays a Failed status in the High Availability. The last node needs to be removed and replaced with refreshed node or a new node to achieve a healthy High Availability state.</p>
<p><strong>Workaround:</strong><br>When adding a third node to the High Availability cluster, if the status displays as &quot;Failed&quot;, the last node needs to be removed, and a refreshed node or a new node needs to be added back to the Central Manager High Availability group. The steps are as follows:<br>
<br>
1. Log in to the Central Manager UI and remove the last node.<br>
2. Reset the impacted node by running k3s_reset_cluster.py and rebooting it. Clear the contents of /var/lib/f5/infra-manager/registration-details.json.<br>
3. Add the node back to the Central Manager High Availability group.</p>
<hr>
<h4><a name="A1644545-1" rel="nofollow"></a>1644545-1 : Central Manager (CM) restore fails when using a full backup file with an external storage configuration</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>Backup file restoration fails on BIG-IP Next Central Manager version 20.3.0.</p>
<p><strong>Conditions:</strong><br>For Standalone CM:<br>
1. In the UI, configure external storage and start the CM services.<br>
2. After installation completes, log in to the CM and create a full backup.<br>
3. Attempt to restore the backup file on the same CM instance.<br>
<br>
For HA CM:<br>
1. In the UI, configure external storage, add nodes, and start the CM services.<br>
2. After installation completes, log in to the CM and create a full backup.<br>
3. Attempt to restore the backup file on the same CM instance.</p>
<p><strong>Impact:</strong><br>Backup file restoration fails on the BIG-IP Next Central Manager version 20.3.0.</p>
<p><strong>Workaround:</strong><br>For Standalone CM:<br>
<br>
Follow the steps before taking a full backup of the CM:<br>
1. SSH into the CM.<br>
<br>
2. Run command &quot;kubectl exec -it cmdb-elasticsearch-0 -c elasticsearch -- bash&quot;<br>
<br>
3. Inside the Elasticsearch pod, run the following command:<br>
curl -X PUT &quot;http://localhost:9200/application_cm&quot; -H &#39;Content-Type: application/json&#39; -d &#39;{<br>
&nbsp;&nbsp;&quot;settings&quot;: {<br>
&nbsp;&nbsp;&nbsp;&nbsp;&quot;number_of_shards&quot;: 1,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&quot;number_of_replicas&quot;: 0<br>
&nbsp;&nbsp;},<br>
&nbsp;&nbsp;&quot;mappings&quot;: {<br>
&nbsp;&nbsp;&nbsp;&nbsp;&quot;properties&quot;: {<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;@timestamp&quot;: {<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;type&quot;: &quot;date&quot;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;_hash&quot;: {<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;type&quot;: &quot;keyword&quot;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;action&quot;: {<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;type&quot;: &quot;keyword&quot;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;level&quot;: {<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;type&quot;: &quot;keyword&quot;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;msg&quot;: {<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;type&quot;: &quot;text&quot;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;podname&quot;: {<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;type&quot;: &quot;keyword&quot;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;source&quot;: {<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;type&quot;: &quot;keyword&quot;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}<br>
&nbsp;&nbsp;&nbsp;&nbsp;}<br>
&nbsp;&nbsp;}<br>
}&#39;<br>
<br>
4. On the CM UI, navigate to the System &gt; CM Maintenance &gt; Backup &amp; Restore path.<br>
<br>
5. Follow the instructions to create the full backup. <br>
<br>
For HA CM:<br>
<br>
Follow the steps before taking a full backup of the CM:<br>
1. SSH into the main node of HA CM.<br>
<br>
2. Run command &quot;kubectl exec -it cmdb-elasticsearch-0 -c elasticsearch -- bash&quot;<br>
<br>
3. Inside the Elasticsearch pod, run the following command: <br>
curl -X PUT &quot;http://localhost:9200/application_cm&quot; -H &#39;Content-Type: application/json&#39; -d &#39;{<br>
&nbsp;&nbsp;&quot;settings&quot;: {<br>
&nbsp;&nbsp;&nbsp;&nbsp;&quot;number_of_shards&quot;: 1,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&quot;number_of_replicas&quot;: 1<br>
&nbsp;&nbsp;},<br>
&nbsp;&nbsp;&quot;mappings&quot;: {<br>
&nbsp;&nbsp;&nbsp;&nbsp;&quot;properties&quot;: {<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;@timestamp&quot;: {<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;type&quot;: &quot;date&quot;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;_hash&quot;: {<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;type&quot;: &quot;keyword&quot;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;action&quot;: {<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;type&quot;: &quot;keyword&quot;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;level&quot;: {<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;type&quot;: &quot;keyword&quot;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;msg&quot;: {<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;type&quot;: &quot;text&quot;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;podname&quot;: {<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;type&quot;: &quot;keyword&quot;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;source&quot;: {<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;type&quot;: &quot;keyword&quot;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}<br>
&nbsp;&nbsp;&nbsp;&nbsp;}<br>
&nbsp;&nbsp;}<br>
}&#39;<br>
<br>
4. On to CM UI, navigate to the System &gt; CM Maintenance &gt; Backup &amp; Restore path.<br>
<br>
5. Follow the instructions to create the full backup.</p>
<hr>
<h4><a name="A1644157" rel="nofollow"></a>1644157 : &quot;Error sending OCSP request&quot; seen in apmd logs for OCSP authentication access policy</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>OCSP request traffic does not reach the OSCP server if the DNS resolver is improperly configured at first deployment of the application.</p>
<p><strong>Conditions:</strong><br>An application has been deployed that has an Access policy with an OCSP Authentication agent. The OCSP Authentication agent has been configured with an FQDN in the OSCP Responder URL. DNS resolver has not been added to the configuration settings, and the default DNS resolver has not been configured.</p>
<p><strong>Impact:</strong><br>Once the application is deployed in these conditions, OCSP Authentication agent will not be able to forward the request to the OCSP server as it cannot resolve the FQDN. Trying to redeploy the application with appropriate DNS resolver configuration fails to correct the issue.</p>
<p><strong>Workaround:</strong><br>A new application needs to be created with proper DNS resolver configuration and then deployed. Alternatively, the admin can restart tmm once the original application has been redeployed with the correct DNS resolver configuration.</p>
<hr>
<h4><a name="A1641909" rel="nofollow"></a>1641909 : Applications created from the API sometimes can&#39;t be edited in the GUI.</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>In some cases, users are unable to edit FAST applications when they were created through the API.</p>
<p><strong>Conditions:</strong><br>Switching between API and UI can trigger this issue.</p>
<p><strong>Impact:</strong><br>An application can not be edited in the UI.</p>
<p><strong>Workaround:</strong><br>As a workaround, you can go to Protocols and Profiles, enable Server TLS, then disable it and save. Review and Deploy gets enabled. From there, you can Edit the application.</p>
<hr>
<h4><a name="A1641901-1" rel="nofollow"></a>1641901-1 : App configs are not reflected in f5-fsm, causing traffic failure</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>In a case when license activation happens from standby node after odd number of failovers, license would get rejected on TMM due to cluster locking failure.</p>
<p><strong>Conditions:</strong><br>Setup HA cluster, do odd number of failovers and trigger License Activation.</p>
<p><strong>Impact:</strong><br>License activation will get blocked if the activation is being tried on the Standby server after odd number of failovers.</p>
<p><strong>Workaround:</strong><br>Workaround: (Perform any one of below)<br>
1. Restart licensing pod on standby node once HA is assembled. <br>
2. Restart standby node once HA is assembled.</p>
<hr>
<h4><a name="A1636229-1" rel="nofollow"></a>1636229-1 : A vCPU count change can stop traffic for up to 3 hours</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>After changing the number of vCPUs for a BIG-IP Next instance, traffic stops passing through the instance. The instance is running but it will not pass traffic.</p>
<p><strong>Conditions:</strong><br> -- Central Manager managing one or more BIG-IP Next instances<br>
-- The BIG-IP Next instances are licensed and passing traffic<br>
-- From Central Manager, you change the number of vCPUs of a BIG-IP Next instance</p>
<p><strong>Impact:</strong><br>Traffic disrupted until the telemetry report is sent to F5. This can take up to maximum of 3 hours.</p>
<p><strong>Workaround:</strong><br>CM Admin can deactivate and activate the license again for immediate passing of traffic</p>
<hr>
<h4><a name="A1635421" rel="nofollow"></a>1635421 : License server unavailable when a node goes down<span title="This issue may cause the configuration to fail to load or may significantly impact system performance after upgrade">&starf;</span></h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>License feature (mbiq-llm) pod is designed as a single replica. When the node hosting mbiq-llm crashes, k3s tries to reschedule this pod with other available nodes. This process will take time and impacts mbiq-llm trying to come to running state. As a result of this, the license features remain inaccessible during this transition time.</p>
<p><strong>Conditions:</strong><br> -- Central Manager configured for high availability (three nodes)<br>
-- One of the nodes where license server is scheduled goes down</p>
<p><strong>Impact:</strong><br>License features are inaccessible, there will be an error on the instance properties page that the &quot;license service is unavailable&quot;</p>
<p><strong>Workaround:</strong><br>Since license feature (mbiq-llm) will take some time (a couple of minutes) to be up and ready, wait for some time and it should work once that happens.</p>
<hr>
<h4><a name="A1635369-1" rel="nofollow"></a>1635369-1 : CM pool has a manditory monitor constraint.</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>When defining pools, the Central Manager UI requires a monitor to be configured.</p>
<p><strong>Conditions:</strong><br> -- Central Manager<br>
-- Configuring a new pool</p>
<p><strong>Impact:</strong><br>A monitor must be defined when creating a pool.</p>
<p><strong>Workaround:</strong><br>Define a monitor for your environment that will be able to mark your pool members up/down.</p>
<hr>
<h4><a name="A1634929" rel="nofollow"></a>1634929 : Parameter names in api documentation is invalid for metrics api</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>The following error occurs while using the examples from the CM API specification (Retrieve applications time series metrics)<br>
<br>
<br>
{<br>
&quot;status&quot;: 500,<br>
&quot;message&quot;: &quot;ADO-QUERY-00001: Failed to get metrics: unknown metric name: cpu.idle&quot;<br>
}</p>
<p><strong>Conditions:</strong><br>CM applications metrics API</p>
<p><strong>Impact:</strong><br>Unable to retrieve the applications metrics via API calls, i.e.<br>
/api/v1/spaces/default/analytics/application-services/metrics?names=cpu.idle&amp;start=now-1h</p>
<p><strong>Workaround:</strong><br>&#39;cpu.idle&#39; and &#39;cpu.system&#39; are invalid, use &#39;cpu.idle.usage.percent&#39; and &#39;cpu.system.usage.percent&#39; instead.</p>
<hr>
<h4><a name="A1634065-1" rel="nofollow"></a>1634065-1 : BIG-IP Next application telemetry data missing for a brief period from Central Manager when a CM node goes down</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>If one of the nodes in BIG-IP Next Central Manager goes down, then the telemetry data for applications will show gaps in the telemetry charts.</p>
<p><strong>Conditions:</strong><br>Any of the nodes in the BIG-IP Next Central Manager HA Nodes becomes unavailable or goes down.</p>
<p><strong>Impact:</strong><br>BIG-IP Next instance application data metrics will be missing about 2 minutes of data.</p>
<p><strong>Workaround:</strong><br>None</p>
<hr>
<h4><a name="A1633569-1" rel="nofollow"></a>1633569-1 : Default values for new entities in an attached OpenAPI file do not match the policy&rsquo;s current configuration</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>When a user modifies a policy&rsquo;s configuration, it will impact the default values for new entities (such as URLs and parameters). If a new OpenAPI file is added to the policy, the default values for new entities created from the OpenAPI specification (OAS) file will not be determined by the policy&rsquo;s configuration. Instead, they will be based on a policy template.</p>
<p><strong>Conditions:</strong><br>The user&rsquo;s WAF policy has values that are different from the default template values, which impact the creation of entities.</p>
<p><strong>Impact:</strong><br>When a new OpenAPI file is added to the policy, the entities created from it will receive values based on the default template values instead of the current values in the policy.</p>
<p><strong>Workaround:</strong><br>Modify the fields of newly created entities manually to align with the desired values.</p>
<hr>
<h4><a name="A1632833-1" rel="nofollow"></a>1632833-1 : Upgrade to Release version 20.3.0 might create a core file<span title="This issue may cause the configuration to fail to load or may significantly impact system performance after upgrade">&starf;</span></h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>When upgraded to release version 20.3.0 from previous versions, there is possibility that core file generated.</p>
<p><strong>Conditions:</strong><br>Upgrade to release version 20.3.0 from previous versions</p>
<p><strong>Impact:</strong><br>Cores generated are from previous installed versions and there should not be any impact.</p>
<p><strong>Workaround:</strong><br>Core file generated during upgrade can be ignored or deleted.</p>
<hr>
<h4><a name="A1629897-1" rel="nofollow"></a>1629897-1 : Shared object installation status might be incorrect on a migration resume.</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>Shared object installation status (installed) reported in the Migration feature is incorrect when a session is resumed.</p>
<p><strong>Conditions:</strong><br> -- A shared object is installed in a new migration. <br>
-- The object is removed from the Central Manager. <br>
-- The migration is then resumed</p>
<p><strong>Impact:</strong><br>Application may be migrated as draft with a reference to an object that does not exist on Central Manager.</p>
<p><strong>Workaround:</strong><br>Start a new migration instead of resuming it to ensure the status of all shared objects is up to date.</p>
<hr>
<h4><a name="A1629537" rel="nofollow"></a>1629537 : Logged-in admin user will not be able to change password before Central Manager setup</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>The admin user is unable to change the admin password.</p>
<p><strong>Conditions:</strong><br>1. Login with admin to new CM that has not completed setup.<br>
2. Change the default admin password.</p>
<p><strong>Impact:</strong><br>Admin user cannot change the password until Central Manager setup is completed.</p>
<p><strong>Workaround:</strong><br>If you want to change the password without completing the CM setup, you can change the password using the below api<br>
<br>
Api endpoint: &#39;/api/change-password&#39;<br>
Method: POST<br>
Payload: {<br>
&nbsp;&nbsp;&nbsp;&nbsp;&quot;username&quot;: &quot;admin&quot;,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&quot;temp_password&quot;: &quot;current password&quot;,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&quot;new_password&quot;: &quot;new password&quot;<br>
}</p>
<hr>
<h4><a name="A1629161-1" rel="nofollow"></a>1629161-1 : L1-Network cannot be deleted</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>Created L1-Network cannot be deleted.</p>
<p><strong>Conditions:</strong><br> -- BIG-IP Next instance managed and onboarded by Central Manager<br>
-- Configure network &amp; proxy settings are already configured on the instance<br>
-- Clean up the instance&#39;s network configuration (L1 networks, Vlans, &amp; IPs)</p>
<p><strong>Impact:</strong><br>Central Manager is unable to delete the L1 Network objects on the BIG-IP Next instance.</p>
<p><strong>Workaround:</strong><br>Workaround:<br>
1. Delete the L1-Network in instance using CM proxy API (see below).<br>
2. Delete the L1-Network in CM (using the UI).<br>
<br>
How to Delete the L1-Network in instance using CM proxy API:<br>
1. Login to CM using CM login API<br>
2. Grab the instance ID that L1-network object needs to be modified using the GET /api/v1/spaces/default/instances API.<br>
3. Get the L1-Network ID that needs to be deleted by requesting a GET request to &quot;api/device/v1/proxy/{INSTANCE_ID}?path=/L1-networks&quot;<br>
4. Delete the L1-Network by requesting a DELETE request to &quot;/api/device/v1/proxy/{INSTANCE_ID}?path=/L1-networks/{L1_NETWORK_ID}&quot;. Take a note of the Job ID returned in the &quot;id&quot; param of the response.<br>
5. Ensure that the L1-Network deletion is successful by requesting a GET request to &quot;/api/device/v1/proxy/{INSTANCE_ID}?path=/jobs/{JOB_ID}&quot;, and ensure that the &quot;title&quot; under the &quot;message&quot; has a value of &quot;jobUpdateSuccessful&quot;.</p>
<hr>
<h4><a name="A1629105-1" rel="nofollow"></a>1629105-1 : Incorrect conversion of DTLS virtual server<span title="This issue may cause the configuration to fail to load or may significantly impact system performance after upgrade">&starf;</span></h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>If a virtual server with UDP and ssl profile is being migrated, it results in creating Service_UDP class without reference to proper DTLS_server or DTLS_client class in AS3 declaration. Additionally, the declaration includes also TLS_server or TLS_client class, which is incorrect.</p>
<p><strong>Conditions:</strong><br>Migration of DTLS virtual server.</p>
<p><strong>Impact:</strong><br>Migration process results in creating UDP application without TLS.</p>
<p><strong>Workaround:</strong><br>User can manually modify AS3 declaration using CM AS3 editor before deployment to Next instance, according to latest Next schema documentation (DTLS_client/DTLS_server and Service_UDP class)<br>
https://clouddocs.f5.com/bigip-next/latest/schemasupport/schema-reference.html#service-udp<br>
https://clouddocs.f5.com/bigip-next/latest/schemasupport/schema-reference.html#dtls-server</p>
<hr>
<h4><a name="A1629077-1" rel="nofollow"></a>1629077-1 : BIG-IP Next Central Manager does not support NTP configuration via DHCP</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>If you supply NTP server IP address via DHCP, chrony will not be configured to use that data.</p>
<p><strong>Conditions:</strong><br>DHCP server provides NTP server IP addresses to BIG-IP Next Central Manager.</p>
<p><strong>Impact:</strong><br>Custom NTP sources must be configured via the setup utility or cloud-init instead of DHCP.</p>
<p><strong>Workaround:</strong><br>Either run the setup utility to configure the custom NTP server IP addresses or modify the /etc/chrony/sources.d/central-manager.sources file to contain the sources being advertised by DHCP.</p>
<hr>
<h4><a name="A1623609-1" rel="nofollow"></a>1623609-1 : Skipped certificate marked as imported during application migration via the GUI.</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>A certificate that is skipped during migration is incorrectly marked in the GUI as installed.</p>
<p><strong>Conditions:</strong><br>Migration of a virtual server configured with an unsupported or default certificate.</p>
<p><strong>Impact:</strong><br>Cosmetic only.<br>
The declaration is fine and the skipped certificate is not installed on the CM.</p>
<p><strong>Workaround:</strong><br>None</p>
<hr>
<h4><a name="A1623533-1" rel="nofollow"></a>1623533-1 : Observing drop in traffic throughput with debug-sidecar inline tcpdump packet capture</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>This could be observed with earlier release versions and  might be limitation with existing design of tcpdump packet capture flow.<br>
<br>
no traffic throughput drop was observed when tcpdump captured to file.</p>
<p><strong>Conditions:</strong><br>Send some huge traffic using ixia traffic generator with 10G <br>
once the traffic is stable , perform a tcpdump on the tmm debug sidecar container</p>
<p><strong>Impact:</strong><br>Traffic throughput affected when admin/debug user capture packets inline using debug-sidecar tcpdump</p>
<p><strong>Workaround:</strong><br>Traffic throughput drop not observed when tcpdump captured to file.</p>
<hr>
<h4><a name="A1623421-1" rel="nofollow"></a>1623421-1 : External OpenAPI files cannot be used with HTTPS links</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>Creating a new policy using an external OpenAPI file from a HTTPS address is not possible.</p>
<p><strong>Conditions:</strong><br>User has OpenAPI file located in an HTTPS address.</p>
<p><strong>Impact:</strong><br>Cannot create a new policy with an external OpenAPI file located in an HTTPS address.</p>
<p><strong>Workaround:</strong><br>By downloading the OpenAPI file locally, users can then easily create the policy by uploading the file to Central Manager.</p>
<hr>
<h4><a name="A1622005-1" rel="nofollow"></a>1622005-1 : OpenAPI files that are extremely large cannot be applied</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>Uploading a significantly large OpenAPI file containing numerous endpoints and parameters results in the failure to apply it to a policy.</p>
<p><strong>Conditions:</strong><br>User has very large OpenAPI file.</p>
<p><strong>Impact:</strong><br>Large OpenAPI files cannot be used for WAF policy applications.</p>
<p><strong>Workaround:</strong><br>None</p>
<hr>
<h4><a name="A1615261" rel="nofollow"></a>1615261 : Application page may show &quot;No Data&quot; for Active Alerts instead of zero.</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>The main application page may show &quot;No Data&quot; under the Active Alerts heading when the application service page will show zero.</p>
<p><strong>Conditions:</strong><br>Lack of active alerts.</p>
<p><strong>Impact:</strong><br>This is a cosmetic issue. &quot;No Data&quot; is the same state as zero active alerts.</p>
<p><strong>Workaround:</strong><br>None</p>
<hr>
<h4><a name="A1615257" rel="nofollow"></a>1615257 : Application monitors edit drawer autosaves</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>In the &ldquo;Manage Monitors&rdquo; drawer, when selecting monitor types under the &ldquo;Monitor Type&rdquo; drop-down, note that there is no explicit &lsquo;Save&rsquo; button. The drawer auto-saves user input as changes are made. If you have created custom monitors, you could delete one of them, but not a default monitor.</p>
<p><strong>Conditions:</strong><br>The save/delete options on the application monitor page could be confusing.</p>
<p><strong>Impact:</strong><br>The delete and save operations on the Manage Monitors page could lead to confusion.</p>
<p><strong>Workaround:</strong><br>The Manage Monitors page auto saves any user input.</p>
<hr>
<h4><a name="A1604997-1" rel="nofollow"></a>1604997-1 : Central Manager (CM) Prometheus pod in CrashLoopBackOff</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>The Prometheus pod is stuck in a CrashLoopBackOff state with 2 out of 3 containers running.</p>
<p><strong>Conditions:</strong><br>The Central Manager has accumulated a large amount of telemetry data.</p>
<p><strong>Impact:</strong><br>Prior to the BIG-IP Next 20.3.0 release, instance telemetry data will be unavailable.<br>
<br>
Starting with the BIG-IP Next 20.3.0 release, there is no impact on functionality, as instance telemetry data is no longer stored in Prometheus. However, telemetry data for the BIG-IP Next Central Manager will not be available for debugging purposes. This does not affect any Central Manager functionality.</p>
<p><strong>Workaround:</strong><br>SSH into the Central Manager as the admin user and execute the following commands:<br>
<br>
pvc_name=&quot;pvc/prometheus-pv-claim&quot;<br>
pv_name=&quot;pv/$(kubectl get ${pvc_name} -o jsonpath=&#39;{.spec.volumeName}&#39;)&quot;<br>
pod_name=$(kubectl get pod -l app.kubernetes.io/name=prometheus -o name)<br>
<br>
echo &quot;apiVersion: v1<br>
kind: PersistentVolumeClaim<br>
metadata:<br>
&nbsp;&nbsp;name: prometheus-pv-claim<br>
&nbsp;&nbsp;annotations:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&quot;helm.sh/resource-policy&quot;: keep<br>
&nbsp;&nbsp;labels:<br>
&nbsp;&nbsp;&nbsp;&nbsp;helm.sh/chart: prometheus-0.1.0<br>
&nbsp;&nbsp;&nbsp;&nbsp;app.kubernetes.io/instance: prometheus<br>
&nbsp;&nbsp;&nbsp;&nbsp;app.kubernetes.io/name: prometheus<br>
&nbsp;&nbsp;&nbsp;&nbsp;app.kubernetes.io/version: &quot;0.0.0&quot;<br>
&nbsp;&nbsp;&nbsp;&nbsp;app.kubernetes.io/managed-by: Helm<br>
spec:<br>
&nbsp;&nbsp;storageClassName: local-path<br>
&nbsp;&nbsp;accessModes:<br>
&nbsp;&nbsp;&nbsp;&nbsp;- ReadWriteOnce<br>
&nbsp;&nbsp;resources:<br>
&nbsp;&nbsp;&nbsp;&nbsp;requests:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;storage: 10Gi&quot; &gt; pvc.yaml<br>
<br>
kubectl delete &quot;${pv_name}&quot; &quot;${pvc_name}&quot; &quot;${pod_name}&quot;<br>
kubectl apply -f pvc.yaml<br>
<br>
Confirm the Prometheus pod is running successfully and then execute this command:<br>
rm pvc.yaml</p>
<hr>
<h4><a name="A1604657" rel="nofollow"></a>1604657 : High CPU utilization and reduced throughput in certain conditions when connection mirroring is enabled in HA</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>High CPU utilization and reduced throughput in certain conditions when connection mirroring is enabled in HA</p>
<p><strong>Conditions:</strong><br>Connection mirroring is enabled in HA. System is bombarded with short-term connections.</p>
<p><strong>Impact:</strong><br>High CPU utilization and reduced throughput is seen in HA setup as compared to a standalone system.</p>
<p><strong>Workaround:</strong><br>Connection mirroring is usually reserved for long-lived connections or sessions. If the traffic pattern is short-term connections, disable connection mirroring.</p>
<hr>
<h4><a name="A1603561-1" rel="nofollow"></a>1603561-1 : L1-Network name cannot be changed</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>Created L1-Network name cannot be changed.</p>
<p><strong>Conditions:</strong><br> - BIG-IP Next instance managed and onboarded by Central Manager<br>
- Configure network &amp; proxy settings are already configured on the instance.<br>
- Edit the instance&#39;s configured L1-Network name.</p>
<p><strong>Impact:</strong><br>The Central Manager cannot change the name of the L1 Network on the BIG-IP Next instance. It may indicate that the update was successful, but the outcome will be as follows:<br>
1. The previous L1-Network remains unchanged.<br>
2. A new L1-Network (with the updated name) is added.</p>
<p><strong>Workaround:</strong><br>Use the following steps:<br>
1. Delete the L1-Network in instance using Central Manager proxy API (see below).<br>
2. Delete the L1-Network in Central Manager (using the UI).<br>
<br>
How to Delete the L1-Network in an Instance Using the Central Manager Proxy API:<br>
<br>
1. Log in to the Central Manager using the Central Manager login API.<br>
2. Retrieve the instance ID for the L1 network object that needs to be modified by using the GET /api/v1/spaces/default/instances API.<br>
3. Obtain the old L1 network ID that needs to be deleted by sending a GET request to api/device/v1/proxy/{INSTANCE_ID}?path=/L1-networks.<br>
4. Delete the old L1 network by sending a DELETE request to /api/device/v1/proxy/{INSTANCE_ID}?path=/L1-networks/{L1_NETWORK_ID}. Note the Job ID returned in the &quot;id&quot; parameter of the response.<br>
5. Ensure that the deletion of the L1 network is successful by sending a GET request to /api/device/v1/proxy/{INSTANCE_ID}?path=/jobs/{JOB_ID}. Confirm that the &quot;title&quot; under the &quot;message&quot; has the value &quot;jobUpdateSuccessful.&quot;</p>
<hr>
<h4><a name="A1602001" rel="nofollow"></a>1602001 : Upgrading from 20.2.1 or Earlier versions will delete all External Loggers<span title="This issue may cause the configuration to fail to load or may significantly impact system performance after upgrade">&starf;</span></h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>Upgrading BIG-IP Next Central Manager (CM) from 20.2.1 or earlier versions deletes all configured external loggers.</p>
<p><strong>Conditions:</strong><br>An external logger is configured for the BIG-IP Next Instance.</p>
<p><strong>Impact:</strong><br>After the upgrade, the external logger configuration must be reconfigured manually.</p>
<p><strong>Workaround:</strong><br>None</p>
<hr>
<h4><a name="A1601573" rel="nofollow"></a>1601573 : UI elements related to virtual servers not shown after upgrade<span title="This issue may cause the configuration to fail to load or may significantly impact system performance after upgrade">&starf;</span></h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>After the upgrade from 20.2.0 or below to 20.2.1 or above, several things will no longer work until existing applications are redeployed:<br>
1. In the L7 dashboard, the whole section that filters for virtual servers will not be shown.<br>
2. In event logs -&gt; L7 DoS, it will be impossible to filter by virtual server (the dropdown will be empty).<br>
3. In reports, if you click Create, select &quot;Virtual Servers&quot; and the click &quot;Select&quot;, the drop down will be empty.</p>
<p><strong>Conditions:</strong><br>A WAF application was deployed on 20.2.1 or below.</p>
<p><strong>Impact:</strong><br>The following elements are not shown:<br>
1. In the L7 dashboard, the whole section that filters for virtual servers will not be shown.<br>
2. In event logs -&gt; L7 DoS, it will be impossible to filter by virtual server (the dropdown will be empty).<br>
3. In reports, if you click Create, select &quot;Virtual Servers&quot; and the click &quot;Select&quot;, the drop down will be empty.</p>
<p><strong>Workaround:</strong><br>Redeploy affected applications.</p>
<hr>
<h4><a name="A1601233-1" rel="nofollow"></a>1601233-1 : Multi-replica in HA not supported for alert feature</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>Due to multi-replica is not yet implemented for the alert feature in HA, it may take a while for the alert to be activated on the receiving node. This can cause QKViews not to be generated since they rely on the alert feature.</p>
<p><strong>Conditions:</strong><br>The problem occurs when the node does not have the alert feature.</p>
<p><strong>Impact:</strong><br>After restarting one of the instance&#39;s in Central Manager HA, the QKView for BIG-IP Next HA configuration remains in a running state.</p>
<p><strong>Workaround:</strong><br>Once the alert feature is up, we can wait a couple of minutes and it should be functional.</p>
<hr>
<h4><a name="A1600809-1" rel="nofollow"></a>1600809-1 : Upgrading BIG-IP Next Central Manager does not show unsupported properties in migrations created before upgrade.<span title="This issue may cause the configuration to fail to load or may significantly impact system performance after upgrade">&starf;</span></h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>Prior to 20.2.1, Journeys reported only supported objects. If any of the object&#39;s properties was unsupported, you are not informed about that gap. <br>
<br>
After upgrading to 20.2.1, old migration sessions do not get updated to report unsupported properties.<br>
<br>
Configuration Analyzer does not show unsupported properties underlined in application&#39;s configuration files.</p>
<p><strong>Conditions:</strong><br> -- Migrations created in a version prior to 20.2.1<br>
-- Upgrade to 20.2.1<br>
-- Analyze the configuration</p>
<p><strong>Impact:</strong><br>Migration status of the applications is invalid.</p>
<p><strong>Workaround:</strong><br>Start a new migration using the same UCS archive to get the proper reporting of unsupported properties.</p>
<hr>
<h4><a name="A1600381-1" rel="nofollow"></a>1600381-1 : WAF enforcer might crash during handling of response</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>The WAF enforcer may experience a crash if it receives a response containing a specially designed, large Set-Cookie header.</p>
<p><strong>Conditions:</strong><br>The protected server sends a large Set-Cookie header.</p>
<p><strong>Impact:</strong><br>The Enforcer may experience occasional crashes, resulting in disrupted traffic until the WAF-enforcer is restarted.</p>
<p><strong>Workaround:</strong><br>None</p>
<hr>
<h4><a name="A1600377-1" rel="nofollow"></a>1600377-1 : The BIG-IP Central Manager GUI does not support backup file uploads when external storage is configured.</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>Steps:  <br>
1. Configure two BIG-IP Next Central Managers (BIG-IP Next Central Manager 1 and BIG-IP Next Central Manager 2) with external storage. <br>
2. Create a backup on BIG-IP Next Central Manager 1 and download the backup file. <br>
3. Open BIG-IP Next Central Manager 2 to restore.  <br>
4. The &ldquo;Upload Backup File&rdquo; button is not visible, preventing access to the restore functionality.</p>
<p><strong>Conditions:</strong><br>The BIG-IP Next Central Manager GUI does not allow uploading, downloading, or deleting backup files when external storage is configured.</p>
<p><strong>Impact:</strong><br>Users cannot upload a backup file or perform a restore through the BIG-IP Next Central Manager GUI.</p>
<p><strong>Workaround:</strong><br>Manually transfer the backup file from BIG-IP Next Central Manager 1 to BIG-IP Next Central Manager 2. For detailed instructions, refer to the &ldquo;Restore the BIG-IP Next Central Manager with External Storage&rdquo; section.<br>
<br>
https://clouddocs.f5.com/bigip-next/latest/use_cm/cm_backup_restore_using_ui_api.html#restore-the-big-ip-next-central-manager-with-external-storage</p>
<hr>
<h4><a name="A1596929-1" rel="nofollow"></a>1596929-1 : Policy-compiler supports policy versions only up to 17.0.0.</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>Policy versions above 17.0.0. are rejected during import.</p>
<p><strong>Conditions:</strong><br>A policy includes the following parameter &quot;softwareVersion&quot; with a value above 17.0.0.</p>
<p><strong>Impact:</strong><br>Policy import fails.</p>
<p><strong>Workaround:</strong><br>None</p>
<hr>
<h4><a name="A1596801-1" rel="nofollow"></a>1596801-1 : Route Health Injection default for BIG-IP Next is &quot;ANY&quot;<span title="This issue may cause the configuration to fail to load or may significantly impact system performance after upgrade">&starf;</span></h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>The default for route health injection in BIG-IP Next is ANY. This is an intentional change in behavior from previous versions of BIG-IP (17.x and lower) where it is &quot;Disabled&quot;.</p>
<p><strong>Conditions:</strong><br>Dynamic routing enabled (BGP, OSPF)<br>
<br>
For more information on the available route health injection settings, see https://my.f5.com/manage/s/article/K15923612</p>
<p><strong>Impact:</strong><br>BIG-IP Next will always advertise virtual IP addresses by default. Previous versions of BIG-IP (17.x and lower) disable route health injection by default.</p>
<p><strong>Workaround:</strong><br>Set the appropriate default for RHI.</p>
<hr>
<h4><a name="A1596021-1" rel="nofollow"></a>1596021-1 : serverTLS/clientTLS name in Service_TCP do not match the clientSSL/serverSSL profile name</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>When you try to deploy the application service, if the serverTLS/clientTLS name in Service_TCP do not match the clientSSL/serverSSL profile name, you might get the following error messages:<br>
serverTLS: must contain a path pointing to an existing reference&nbsp;<br>
or<br>
clientTLS: must contain a path pointing to an existing reference</p>
<p><strong>Conditions:</strong><br>Object names are truncated if application or partition names are too long.</p>
<p><strong>Impact:</strong><br>Application service deployment to the BIG-IP Next instance fails.</p>
<p><strong>Workaround:</strong><br>Ensure that the serverTLS/clientTLS name in the Service_TCP class and clientSSL/serverSSL name in the declaration are same.</p>
<hr>
<h4><a name="A1593805" rel="nofollow"></a>1593805 : The air-gapped environment upgrade from BIG-IP Next 20.0.2-0.0.68 to BIG-IP Next 20.2.0-0.5.41 fails<span title="This issue may cause the configuration to fail to load or may significantly impact system performance after upgrade">&starf;</span></h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>When upgrading from BIG-IP Next version 20.0.2-0.0.68 to version 20.2.0-0.5.41, the process encounters a failure. Post-upgrade, the Central Manager GUI exhibits a continuous flashing behavior, persisting for a few minutes before returning to normal functionality. Furthermore, the failed upgrade leads to discrepancies in version representation, where the GUI displays the current version while the CLI  indicates the target version.</p>
<p><strong>Conditions:</strong><br>Upgrade CM from version 20.0.2-0.0.68 to version 20.2.0-0.5.41.</p>
<p><strong>Impact:</strong><br>CM becomes dysfunctional, the failed upgrade leads to discrepancies in version representation, where the CM GUI  displays the current version while the CLI indicates the target version.</p>
<p><strong>Workaround:</strong><br>Backup and restore CM from the version 20.0.2, refer How to: Back up and restore BIG-IP Next Central Manager (https://clouddocs.f5.com/bigip-next/20-0-2/use_cm/cm_backup-restore.html).</p>
<hr>
<h4><a name="A1593745" rel="nofollow"></a>1593745 : Issues identified during Backup, Restore, and User Operations between two BIG-IP Next Central Managers for Standalone and High Availability Nodes.</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>Performing a backup on one BIG-IP Next Central Manager, followed by user operations, and then performing a restore on another BIG-IP Next Central Manager with subsequent user operations may result in the following issues<br>
<br>
You cannot download the QKView on the restored BIG-IP Next Central Manager if it was created by the previous BIG-IP Next Central Manager before the backup operation.<br>
<br>
After you restore the backup on the new BIG-IP Next Central Manager setup, any BIG-IP Next instance deleted on the previous BIG-IP Next Central Manager enters an unknown state.<br>
<br>
Additionally, after you restore the backup on the new BIG-IP Next Central Manager, the deleted app on the previous BIG-IP Next Central Manager cannot process traffic until you redeploy the app.<br>
<br>
The BIG-IP Next Central Manager does not support uploading and downloading backup files when configured with external storage.</p>
<p><strong>Conditions:</strong><br>Perform a backup on one BIG-IP Next Central Manager and restore it on another BIG-IP Next Central Manager.</p>
<p><strong>Impact:</strong><br>After restoring the new BIG-IP Next Central Manager, certain operations might not function properly.</p>
<p><strong>Workaround:</strong><br>If you delete the app after taking a backup on the BIG-IP Next Central Manager and then restore it on a new BIG-IP NEXT Central Manager, traffic will not pass through. Users must edit and redeploy the app for traffic to function properly.</p>
<hr>
<h4><a name="A1593613" rel="nofollow"></a>1593613 : When an upgrade fails, CM cannot be restored and becomes dysfunctional due to multiple containers entering the &#39;CrashLoopBackOff&#39; state<span title="This issue may cause the configuration to fail to load or may significantly impact system performance after upgrade">&starf;</span></h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>Upgrade from BIG-IP Next version 20.0.2 to version 20.2.0 fails with status of CrashLoopBackOff on several pods.</p>
<p><strong>Conditions:</strong><br>Upgrading from BIG-IP Next version 20.0.2 to version 20.2.0</p>
<p><strong>Impact:</strong><br>Central Manager is unusable due to pods not being in valid state.</p>
<p><strong>Workaround:</strong><br>Restore CM to a previous version backup and upgrade to next minor version of 20.1.0. Refer How to: Back up and restore BIG-IP Next Central Manager (https://clouddocs.f5.com/bigip-next/20-0-2/use_cm/cm_backup-restore.html).</p>
<hr>
<h4><a name="A1590037-1" rel="nofollow"></a>1590037-1 : Provisioning SSL Orchestrator on BIG-IP NEXT HA cluster fails when using Central Manager UI</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>When user creates an HA cluster of BIG-IP Next instances using Central Manager UI. After successful creation and licensing of the instance, Provisioning SSL Orchestrator from the UI may make it unresponsive and display &quot;Enabling SSL Orchestrator is in progress...&quot; message.</p>
<p><strong>Conditions:</strong><br>When user creates HA cluster of BIG-IP Next instances and tries to provision SSL Orchestrator from the UI.</p>
<p><strong>Impact:</strong><br>Provisioning SSL Orchestrator on HA cluster may result in unresponsive UI.</p>
<p><strong>Workaround:</strong><br>Configure HA cluster, license, and provision SSL Orchestrator using OpenAPI, prior to adding cluster to Central Manager.</p>
<hr>
<h4><a name="A1589865-1" rel="nofollow"></a>1589865-1 : Licensing via CM fails with &quot;400 The SSL certificate error&quot;</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>An error occurs in LLM logs at at the Central Manager licensing screen during BIG-IP Next license activation.<br>
<br>
...... error while getting Signed Ack. Response: &lt;html&gt;<br>
&lt;head&gt;&lt;title&gt;400 The SSL certificate error&lt;/title&gt;&lt;/head&gt;<br>
&lt;body&gt;<br>
&lt;center&gt;&lt;h1&gt;400 Bad Request&lt;/h1&gt;&lt;/center&gt;<br>
&lt;center&gt;The SSL certificate error&lt;/center&gt;<br>
&lt;hr&gt;&lt;center&gt;server&lt;/center&gt;<br>
&lt;/body&gt;<br>
&lt;/html&gt;<br>
<br>
...... ack verification task failed with Error: LICENSING-1120::&lt;html&gt;<br>
&lt;head&gt;&lt;title&gt;400 The SSL certificate error&lt;/title&gt;&lt;/head&gt;<br>
&lt;body&gt;<br>
&lt;/html&gt;</p>
<p><strong>Conditions:</strong><br> - Central Manager<br>
- License Activation</p>
<p><strong>Impact:</strong><br>Unable to perform License Activation on BIG-IP Next Instance from Central Manager.</p>
<p><strong>Workaround:</strong><br>For License Activation:<br>
Login to the CM Shell as admin and perform the below steps:<br>
<br>
-------------------------------------------------------------------<br>
Step A:<br>
-------------------------------------------------------------------<br>
Copy the Vault client certificate from the LLM pod to CM so that access to the Vault server is possible<br>
<br>
Execute the below commands to get the tls.key, tls.crt and ca.crt for performing operations on LLM objects.<br>
<br>
kubectl get secrets/mbiq-llm-vault-client-cert -o &#39;go-template={{index .data &quot;tls.key&quot;}}&#39; | base64 -d &gt; tls.key<br>
kubectl get secrets/mbiq-llm-vault-client-cert -o &#39;go-template={{index .data &quot;tls.crt&quot;}}&#39; | base64 -d &gt; tls.crt<br>
kubectl get secrets/mbiq-vault-cert -o &#39;go-template={{index .data &quot;ca.crt&quot;}}&#39; | base64 -d &gt; ca.crt<br>
<br>
-------------------------------------------------------------------<br>
Step B:<br>
-------------------------------------------------------------------<br>
Get the client token to perform operations on the LLM objects<br>
<br>
1. Execute the below command to get the Vault IP<br>
kubectl get svc | grep mbiq-vault-active<br>
<br>
Example:<br>
$ kubectl get svc | grep mbiq-vault-active<br>
<br>
mbiq-vault-active          ClusterIP   10.1.1.1 &lt;none&gt;    8200/TCP,8201/TCP                                               11h<br>
<br>
Note: If the IP is not available with &quot;kubectl get svc | grep mbiq-vault-active&quot; execute the below command to get the Vault IP<br>
<br>
kubectl get svc | grep mbiq-vault<br>
<br>
Use the IP for mbiq-vault from the above command result.<br>
Example:<br>
<br>
$ kubectl get svc | grep mbiq-vault<br>
mbiq-vault-internal            ClusterIP   None       &lt;none&gt;    8200/TCP,8201/TCP                                                25d<br>
mbiq-vault                ClusterIP  10.1.1.2  &lt;none&gt;    8200/TCP,8201/TCP                                                25d<br>
<br>
&nbsp;<br>
2. Use the obtained IP from the above step to generate the client_token and this token has to be retrieved for every API call on LLM objects<br>
curl --insecure --request PUT --cacert ca.crt --cert tls.crt --key tls.key --data &#39;{&quot;name&quot;: &quot;llm&quot;}&#39; https://&lt;Vault IP&gt;:8200/v1/auth/cert/login | jq &#39;.auth.client_token&#39;<br>
<br>
Example:<br>
<br>
$ curl --insecure --request PUT --cacert ca.crt --cert tls.crt --key tls.key --data &#39;{&quot;name&quot;: &quot;llm&quot;}&#39; https://10.1.1.1:8200/v1/auth/cert/login | jq &#39;.auth.client_token&#39;<br>
<br>
Example client_token output:<br>
<br>
&quot;hvs.CAESIABDIsdPQxrJzfCNqRhTzI4L2f26SOmjp1Wp2dKp2zIvGh4KHGh2cy5DWkRkbVpKVTRLNjZsWW1UejBDM1ZnN0I&quot;<br>
<br>
-------------------------------------------------------------------<br>
Step C:<br>
-------------------------------------------------------------------<br>
<br>
Delete the LLM objects - certs, privateKey, digitalAssetID and certificateChain<br>
<br>
Execute the below commands to delete certs, privateKey, digitalAssetID, and certificateChain of llm pod<br>
Note: For each of the below command execution it should have a new client_token and use Step B-2 to generate the client_token. This is done as the client_token is valid only for a single operation.<br>
<br>
Fetch the client_token from B-2<br>
<br>
curl --insecure --cacert ca.crt --header &quot;X-Vault-Token: &lt;client_token&gt;&quot; -X DELETE https://&lt;Vault IP&gt;:8200/v1/secret/llm/certs<br>
<br>
Fetch the client_token from B-2<br>
<br>
curl --insecure --cacert ca.crt --header &quot;X-Vault-Token: &lt;client_token&gt;&quot; -X DELETE https://&lt;Vault IP&gt;:8200/v1/secret/llm/privateKey<br>
<br>
Fetch the client_token from B-2<br>
<br>
curl --insecure --cacert ca.crt --header &quot;X-Vault-Token: &lt;client_token&gt;&quot; -X DELETE https://&lt;Vault IP&gt;:8200/v1/secret/llm/digitalAssetID<br>
<br>
Fetch the client_token from B-2<br>
<br>
curl --insecure --cacert ca.crt --header &quot;X-Vault-Token: &lt;client_token&gt;&quot; -X DELETE https://&lt;Vault IP&gt;:8200/v1/secret/llm/certificateChain<br>
<br>
Example:<br>
<br>
curl --insecure --cacert ca.crt --header &quot;X-Vault-Token: hvs.CAESIABDIsdPQxrJzfCNqRhTzI4L2f26SOmjp1Wp2dKp2zIvGh4KHGh2cy5DWkRkbVpKVTRLNjZsWW1UejBDM1ZnN0I&quot; -X DELETE https://10.1.1.1:8200/v1/secret/llm/certs<br>
curl --insecure --cacert ca.crt --header &quot;X-Vault-Token: hvs.CAESIFiHjXY4LNxlKoIyO1NfdGQBs-bK3Cpkh4SSc_k4u75eGh4KHGh2cy5uVGR5RFFLd2d6dGhEaVRZeEpvUFdTT1E&quot; -X DELETE https://10.1.1.1:8200/v1/secret/llm/privateKey<br>
<br>
curl --insecure --cacert ca.crt --header &quot;X-Vault-Token: hvs.CAESIJfIwIzEprlD5r589sK9YELSlhOuZtx-rpMx8sV-e6YvGh4KHGh2cy5vTFBzM0JYUlZkQzBlWE43bGQ4NG1uTXQ&quot; -X DELETE https://10.1.1.1:8200/v1/secret/llm/digitalAssetID<br>
<br>
curl --insecure --cacert ca.crt --header &quot;X-Vault-Token: hvs.CAESIOJj_40Zd2z3vyXzRBaRMS-A_o-GR8ottySpNn4SvStKGh4KHGh2cy5pZHBOY0djWVNGUnZwS3ZIdFpPTnZCaHk&quot; -X DELETE https://10.1.1.1:8200/v1/secret/llm/certificateChain<br>
<br>
-------------------------------------------------------------------<br>
Step D:<br>
-------------------------------------------------------------------<br>
Restart the LLM pod<br>
<br>
Execute the below command to retrieve the POD details<br>
<br>
kubectl get pods | grep llm<br>
<br>
Restart the pod using the below command<br>
<br>
kubectl delete pod &lt;pod name&gt;<br>
<br>
Check the status of the LLM pod using the below command<br>
kubectl get pods | grep llm<br>
<br>
As the llm objects are cleared they will be recreated on pod restart with correct values.<br>
<br>
Example:<br>
<br>
$ kubectl get pods | grep llm<br>
mbiq-llm-84c56d748d-jn7jm             2/2   Running  0       29m<br>
<br>
$ kubectl delete pod mbiq-llm-84c56d748d-jn7jm<br>
pod &quot;mbiq-llm-84c56d748d-jn7jm&quot; deleted<br>
<br>
$ kubectl get pods | grep llm<br>
mbiq-llm-84c56d748d-5rdlz             0/2   PodInitializing  0       4s<br>
<br>
$ kubectl get pods | grep llm<br>
mbiq-llm-84c56d748d-5rdlz             2/2   Running  0       6s<br>
<br>
<br>
Once the pod is in a running state, from CM initiate the license operation(Activate/ Switch License)<br>
<br>
After the activation is successful delete the certificates from Step A<br>
<br>
rm -f tls.key tls.crt ca.crt</p>
<hr>
<h4><a name="A1588813-1" rel="nofollow"></a>1588813-1 : CM Restore on a 3 node BIG-IP Next Central Manager with external storage fails with ES errors</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>BIG-IP Next Central Manager restore fails with critical alert raised with the description:<br>
Error registering Elasticsearch snapshot repository: failed to register Elasticsearch snapshot repository: response not acknowledged. result: map[error:map[caused_by:map[caused_by:map[reason:/vol/elasticsearch-snapshot/restore-temp/elasticsearch type:access_denied_exception] reason:[elastic-repo] cannot create blob store type:repository_exception] reason:[elastic-repo] Could not determine repository generation from root blobs root_cause:[map[reason:[elastic-repo] cannot create blob store type:repository_exception]] type:repository_exception] status:500]</p>
<p><strong>Conditions:</strong><br>Configure a 3 node BIG-IP Next Central Manager and take a CM backup. <br>
Now on a fresh 3 node BIG-IP Next Central Manager use the backup file and restore the BIG-IP Next Central Manager.</p>
<p><strong>Impact:</strong><br>CM restore succeeds but Elasticsearch is not restored.</p>
<p><strong>Workaround:</strong><br>When Elasticsearch is not restored, an alert message is raised. Run ./opt/cm-bundle/cm restore_es and this will make sure ES is restored.</p>
<hr>
<h4><a name="A1588101-1" rel="nofollow"></a>1588101-1 : Any changes made on the BIG-IP Next Central Manager after the BIG-IP Next instance backup will not be reflected on the BIG-IP Next Central Manager once the BIG-IP Next instance is restored.</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>After an instance restore is successful, and all the data present in instance is restored, BIG-IP Next Central Manager still shows incorrect data.</p>
<p><strong>Conditions:</strong><br>Create a BIG-IP Next Central Manager, and discover an instance.<br>
Create an instance backup.<br>
Make any changes on the instance using BIG-IP Next Central Manager. For example, create a QKview, modify networks or selfips, or delete application services on the instance using BIG-IP Next Central Manager UI.<br>
Restore the instance with the backup file created.<br>
The change is reflected only on the instance but not on BIG-IP Next Central Manager. As a result, changes before the backup are not visible on the BIG-IP Next Central Manager UI.</p>
<p><strong>Impact:</strong><br>There are discrepancies between BIG-IP Next Central Manager and the instance.</p>
<p><strong>Workaround:</strong><br>None</p>
<hr>
<h4><a name="A1586869" rel="nofollow"></a>1586869 : Unable to create the same standby instance, when Instance HA creation failed using CM-created instances<span title="This issue may cause the configuration to fail to load or may significantly impact system performance after upgrade">&starf;</span></h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>Using CM-created instances, if instance HA creation is failed and standby instance is removed from CM, then you will not be able to create the same standby instance configuration.</p>
<p><strong>Conditions:</strong><br>Creating Instance HA using CM-created instances.</p>
<p><strong>Impact:</strong><br>Unable to create the same standby instance.</p>
<p><strong>Workaround:</strong><br>Remove the active instance from CM. This will delete both active and standby instance from CM and the provider. Create both instances again.</p>
<hr>
<h4><a name="A1585309" rel="nofollow"></a>1585309 : Server-Side traffic flows using a default VRF even though pool is configured in a non-default VRF</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>Traffic flows when a default VRF is configured and a pool is configured in a non-default VRF without a route in non-default VRF.</p>
<p><strong>Conditions:</strong><br> - Default VRF is configured<br>
- Pool is configured in non-default VRF<br>
- Route to pool exists in default VRF, but not in non-default VRF.</p>
<p><strong>Impact:</strong><br>Traffic continues to work when pool is configured in non-default VRF, but there is no route in non-default VRF.</p>
<p><strong>Workaround:</strong><br>For network isolation, do not configure a default VRF. Use all non-default VRFs in the configuration.</p>
<hr>
<h4><a name="A1584637" rel="nofollow"></a>1584637 : After upgrade, &#39;Accept Request&#39; will only work on events after policy redeploy<span title="This issue may cause the configuration to fail to load or may significantly impact system performance after upgrade">&starf;</span></h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>After BIG-IP Next Central Manager is upgraded to version 20.2.1 from a previous version,  the &#39;Accept Request&#39; option on events does not work.</p>
<p><strong>Conditions:</strong><br>Upgrade BIG-IP Next Central Manager that contains WAF events to version 20.2.1.<br>
<br>
Click &#39;Accept Request&#39; for an event. The results return:<br>
<br>
&#39;No policy builder data in event [support id]&#39;</p>
<p><strong>Impact:</strong><br>All events (new or pre-update events) in the WAF event log will not return results when you select &#39;Accept Request&#39;.</p>
<p><strong>Workaround:</strong><br>You can receive and accept results for new events from the event log when you manually redeploy the WAF policy.</p>
<hr>
<h4><a name="A1584625" rel="nofollow"></a>1584625 : Virtual server information of application containing multiple virtual IP addresses and WAF policies after upgrade is missing<span title="This issue may cause the configuration to fail to load or may significantly impact system performance after upgrade">&starf;</span></h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>When creating a report based on specific virtual servers after upgrading BIG-IP Next Central Manager with multi-VIP applications, the action is not possible.<br>
<br>
Filtering by virtual server in BaDoS logs and dashboard after upgrade is not possible</p>
<p><strong>Conditions:</strong><br>Create a report for an application that contains multiple virtual servers.</p>
<p><strong>Impact:</strong><br>Limitations in the actions you can take in the Web Application dashboard and in reports when filtering by virtual servers.</p>
<p><strong>Workaround:</strong><br>Re-deploy the applications after upgrade.</p>
<hr>
<h4><a name="A1583541" rel="nofollow"></a>1583541 : Re-establish trust with BIG-IP after upgrade to 20.2.1 using a 20.1.1 Central Manager<span title="This issue may cause the configuration to fail to load or may significantly impact system performance after upgrade">&starf;</span></h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>Central Manager will report BIG-IP upgrade as failed due to timeout waiting for it to complete.</p>
<p><strong>Conditions:</strong><br>Using a 20.1.1 Central Manager to upgrade a BIG-IP Next instance to 20.2.1</p>
<p><strong>Impact:</strong><br>BIG-IP Next upgrade will have succeeded but Central manager will think it never completed. The version displayed for the BIG-IP in the Central Manger UI will be inaccurate. Until trust is re-established all communication with the BIG-IP will fail.</p>
<p><strong>Workaround:</strong><br>1. Open instance properties drawer, click &quot;Certificates&quot; and establish trust with the instance manually<br>
2. Select the instance in the grid and re-trigger the upgrade to the Nutmeg version again<br>
The instance upgrade should detect the upgrade to the version it is already at and return success. CM task will then fetch the new version and update its DB, in turn updating the version in the UI grid.</p>
<hr>
<h4><a name="A1583049-1" rel="nofollow"></a>1583049-1 : Central Manager Logs</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>When a Kubernetes pod is restarted, the logs from the previously running pod are lost.</p>
<p><strong>Conditions:</strong><br> -- Viewing logs for a Kubernetes pod what was restarted<br>
-- You wish to review log messages occurred before the pod was restarted</p>
<p><strong>Impact:</strong><br>QkView does not have logs from before the pod was restarted<br>
<br>
The new logs generated after the restart will not contain information about the cause of the restart.</p>
<p><strong>Workaround:</strong><br>To understand why a pod was restarted, you can get CM logs that have all comprehensive feature-level logs which provide a user with all of the previous/current CM activities and all information about what might have caused the pod to restart. You can follow the instruction to generate a CM qkview file and uplad it to F5 iHealth from this link: https://clouddocs.f5.com/bigip-next/latest/support/cm_qkview_script.html<br>
<br>
Once you upload the generated CM qkview to iHealth, <br>
<br>
1. click on the entry that you have just uploaded to iHealth webpage. <br>
<br>
2. Go to &quot;Files&quot; tab on the left side of your screen to see the whole file tree.<br>
3. Go to <br>
all -&gt; host-qkview -&gt; filesystem -&gt; var -&gt; log -&gt; application<br>
and download the latest application log (application.0.log)</p>
<hr>
<h4><a name="A1582421-1" rel="nofollow"></a>1582421-1 : BIG-IP Next Central Manager functionality impacted if the host IP address changes</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>If the IP address of BIG-IP Next Central Manager virtual machine changes, then BIG-IP Next Central Manager functionality will be impacted.</p>
<p><strong>Conditions:</strong><br>BIG-IP Next Central Manager virtual machine IP address is changed.</p>
<p><strong>Impact:</strong><br>Once the BIG-IP Next Central Manager service is up and running, changing the IP address of the host would cause Central Manager functionality to be impacted.</p>
<p><strong>Workaround:</strong><br>Do not change the host IP address of BIG-IP Next Central Manager.</p>
<hr>
<h4><a name="A1582409-1" rel="nofollow"></a>1582409-1 : BIG-IP Next Central Manager will not start if the DNS server details are not provided</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>If no DNS server is configured via DHCP or the setup script, then BIG-IP Next Central Manager initialization fails.</p>
<p><strong>Conditions:</strong><br>No DNS server is configured on the BIG-IP Next Central Manager host.</p>
<p><strong>Impact:</strong><br>The BIG-IP Next Central Manager initialization fails if no DNS server is configured.</p>
<p><strong>Workaround:</strong><br>Run &#39;setup&#39; from the BIG-IP Next Central Manager CLI and manually configure a static IP address along with one or more DNS servers.</p>
<hr>
<h4><a name="A1581877" rel="nofollow"></a>1581877 : An error is seen when no device certificates are present on the BIG-IP Next Instance</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>A certificate not found error is shown when the user has not uploaded the device certificate directly to BIG-IP Next. In this case, BIG-IP Next will use the self-signed certificate by default.</p>
<p><strong>Conditions:</strong><br> -- A BIG-IP Next instance is created outside of Central Manager<br>
-- A device certificate was not uploaded during onboarding<br>
-- The instance is added to Central Manager</p>
<p><strong>Impact:</strong><br>On the Certificate page for the BIG-IP device, an error is displayed &quot;Unable to GET certificates, received 13167-01025&quot;.<br>
<br>
This error message has no impact on the functionality of the BIG-IP Next instance.</p>
<p><strong>Workaround:</strong><br>None</p>
<hr>
<h4><a name="A1579977-1" rel="nofollow"></a>1579977-1 : BIG-IP Next instance telemetry data is missing from the BIG-IP Next Central Manager when a BIG-IP Next Central Manager High Availability node goes down.</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>BIG-IP Next Instance telemetry can be missing from the BIG-IP Next Central Manager for five to ten minutes if any of the BIG-IP Next Central Manager HA nodes go down or unavailable.<br>
<br>
- Instance data metrics such as Instance health, Traffic, and Network Interface metrics will be lost, as they are available only for the previous hour.  <br>
- All other data such as Application metrics and WAF logs, will not be lost. However, these metrics could be unavailable for 5-10 minutes during the node down event.</p>
<p><strong>Conditions:</strong><br>Any of the nodes in the BIG-IP Next Central Manager HA Nodes becomes unavailable or goes down.</p>
<p><strong>Impact:</strong><br>BIG-IP Next instance data metrics, such as instance health, traffic, and network interface metrics, will be lost. All other metrics, such as application metrics and WAF logs, might be missing for 5-10 minutes.</p>
<p><strong>Workaround:</strong><br>Wait for 5-10 minutes and BIG-IP Next Telemetry data will resume on the BIG-IP Next Central Manager.<br>
<br>
Run the following command on the VM console of the BIG-IP Next Central Manager to resume the instance data metrics:<br>
<br>
kubectl delete pods prometheus-mbiq-kube-prometheus-prometheus-0 --grace-period=0 --force</p>
<hr>
<h4><a name="A1579441-1" rel="nofollow"></a>1579441-1 : Connection requests on rSeries may not appear to be DAG distributed as expected</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>Connection requests on rSeries may not be distributed across TMM instances as expected. For example, TMM0 may appear to service more requests than other TMMs, when a round-robin even distribution across TMMs was expected. This may be due to the `port adjust` setting not having the default value of `xor5mid-xor5low`.</p>
<p><strong>Conditions:</strong><br>Multiple TMMs on rSeries, where connection requests are not distributed across TMMs as expected.</p>
<p><strong>Impact:</strong><br>Connection requests may be unevenly distributed across TMMs, causing some TMMs to be under heavier load than other TMMs.</p>
<p><strong>Workaround:</strong><br>Adjust traffic patterns for load balancing, or tune DAG behavior with additional DAG configuration options to adjust assignment of connection requests to TMMs.</p>
<hr>
<h4><a name="A1576277" rel="nofollow"></a>1576277 : &#39;Backup file creation failed&#39; for instance after upgrade to v20.2.0</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>Instance backup fails on BIG-IP Next Central Manager version 20.2.0 with message:<br>
<br>
&#39;Backup file creation failed&#39;</p>
<p><strong>Conditions:</strong><br>A BIG-IP Next Central Manager version 20.1.1 and BIG-IP Next version 20.1.0<br>
<br>
1. Upgrade the BIG-IP Next instances to version 20.2.0. <br>
<br>
2. In the BIG-IP Next Central Manager UI, go to Infrastructure &gt; Instances.<br>
<br>
3. Create a backup file for the instance by selecting the desired BIG-IP Next instance, select the Actions menu, select Back Up &amp; Schedule and input the required information.<br>
&nbsp;&nbsp;<br>
4. Create a backup file for the instance by selecting the desired BIG-IP Next instance, click Actions and select Back Up &amp; Schedule and input the required information.<br>
&nbsp;&nbsp;<br>
5. In the BIG-IP Next Central Manager UI, go to Infrastructure &gt; Instances &gt; Backup &amp; Restore.</p>
<p><strong>Impact:</strong><br>Instance backup file generation fails with message:<br>
<br>
&#39;Backup file creation failed&quot;</p>
<p><strong>Workaround:</strong><br>Once you have upgraded the BIG-IP Next instances to version 20.2.0, you need to delete large image files as they prevent the successful backup. In addition, you need to delete the failed backup file.<br>
<br>
<br>
You must send API calls to the instance to remove the large upgrade files and failed backup files before the backup will succeed. This example uses Postman to send the API calls. The following is an example procedure with variables {{ }} around them. You can use variables or insert the actual value for each request:<br>
<br>
1. Send a login request to BIG-IP Next Central Manager and record the &ldquo;access_token&rdquo; from the response. This is used to make all other API calls.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a. Use the command POST https://{{remote-CM-address}}/api/login, or if no variables are used, then use the command POST https://10.145.69.227/api/login <br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;b. The body for the request is a JSON object with the credentials for the user.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{ &quot;username&quot;: &quot;username&quot;, &quot;password&quot;: &quot;password&quot; }<br>
<br>
2. Send a request to BIG-IP Next Central Manager&#39;s inventory and identify the instance that you want to delete the file from. Record the &ldquo;id&rdquo; from the response. The access_token from the previous step is used as the Bearer Token for the request. Repeat this for all other requests as well:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;GET https://{{remote-CM-address}}/api/device/v1/inventory<br>
<br>
3. Delete the large image files and failed backup files. Send a request for the files present on the instance. Note the instance ID from the previous step is used in the request URL. In the response, record the &quot;id&quot; for the &quot;file name&quot; or &quot;description&quot; in the response. Example files:<br>
<br>
- The upgrade image file: BIG-IP Next 20.2.0....tgz  <br>
<br>
- The original backup file: backup and restore of the system<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;GET https://{{remote-CM-address}}/api/device/v1/proxy/{{remote-Big-IP-Next-ID}}?path=/files<br>
<br>
4. Send a request to delete the file on the instance. The file ID from the previous step and paste it to the end of delete URL. For example:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;DELETE https://{{remote-CM-address}}/api/device/v1/proxy/{{remote-Big-IP-Next-ID}}?path=/files/644fcd02-fa38-4383-ac1c-f67e0c899e0d<br>
<br>
5.Wait at least 20 minutes after the deletion before initiating steps to create another instance backup.<br>
<br>
IMPORTANT NOTE: The file deletion process can take up to 20 minutes to complete. If the files are not fully deleted, the new backup attempt will fail.<br>
<br>
6. If required, repeat step 4 to delete any other large files, unrelated to upgrade, such as QKView or core files.</p>
<hr>
<h4><a name="A1576273" rel="nofollow"></a>1576273 : No L1-Networks in an instance causes BIG-IP Next Central Manager upgrade to v20.2.0 to fail<span title="This issue may cause the configuration to fail to load or may significantly impact system performance after upgrade">&starf;</span></h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>Upgrade to of BIG-IP Next Central Manager v20.2.0 fails.</p>
<p><strong>Conditions:</strong><br>BIG-IP Next Central Manager has an instance with no L1-Networks.</p>
<p><strong>Impact:</strong><br>Cannot upgrade to v20.2.0.</p>
<p><strong>Workaround:</strong><br>Add a blank DefaultL1Network to each instance using instance editor.</p>
<hr>
<h4><a name="A1575549" rel="nofollow"></a>1575549 : BIG-IP Next Central Manager discovery requires an instance to have both Default L2-Network and Default L3-Network if either one already exists</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>Discovering an instance on BIG-IP Next Central Manager requires the instance only be discovered if it is configured with neither Default L2-Network or Default L3-Network, or has both of them, and Default L2-Network be under the Default L3-Network.</p>
<p><strong>Conditions:</strong><br>A BIG-IP Next Central Manager user attempts to discover an instance they own. Before discovering this instance on BIG-IP Next Central Manager, the user configured it with an L2 Network named &quot;Default L2-Network&quot; and an L3 Network named something other than &quot;Default L3-Network&quot;. When the user tries discovering the instance on CM, discovery failed noting that Default L2-Network was present, but no Default L3-Network.</p>
<p><strong>Impact:</strong><br>BIG-IP Next Central Manager cannot discover an instance if  either one of &quot;Default L2-Network&quot; or &quot;Default L3-Network&quot; exist, but not both.</p>
<p><strong>Workaround:</strong><br>If a user configures an instance with an L2 Network named Default L2-Network, they should create an L3 network named Default L3-Network and have its L2 Network be the default L2. If neither exists, or both exist, and the Default L2 is under the Default L3, discovery succeeds.</p>
<hr>
<h4><a name="A1574997" rel="nofollow"></a>1574997 : BIG-IP Next Central Manager HA node installation requires logout to add node<span title="This issue may cause the configuration to fail to load or may significantly impact system performance after upgrade">&starf;</span></h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>As part of a BIG-IP Next Central Manager HA installation, the you must log out of the User Interface (UI), when the first node is added to the cluster.</p>
<p><strong>Conditions:</strong><br>1. Create 3 VM instances on BIG-IP Next Central Manager<br>
2. From UI, change the BIG-IP Next Central Manager password for all 3 instances<br>
3. Login to node-1<br>
4. Click Set up button<br>
5. Fill out the form and click Add</p>
<p><strong>Impact:</strong><br>You must log out and then re log-in to successfully add the new node.</p>
<p><strong>Workaround:</strong><br>Wait for up to 5 minutes for the BIG-IP Next Central Manager cluster to be ready before re-logging in.</p>
<hr>
<h4><a name="A1574685" rel="nofollow"></a>1574685 : Generated WAF report can be loaded without text</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>When generating a WAF report, the loaded print screen for PDF is displayed without text content.This issue is reported primarily on Mac OS and intermittently.</p>
<p><strong>Conditions:</strong><br>No specific conditions apply, it happens intermittently and mainly on Mac operating systems.</p>
<p><strong>Impact:</strong><br>The report does not contain text and is not usable.</p>
<p><strong>Workaround:</strong><br>Retry  generating a WAF report.</p>
<hr>
<h4><a name="A1574681" rel="nofollow"></a>1574681 : Dynamic Parameter Extract from allowed URLs does not show in the parameter in the WAF policy</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>After successfully creating a dynamic parameter with its respective extract URLs, reentering the parameter settings won&#39;t show the saved extract URLs.</p>
<p><strong>Conditions:</strong><br>Configure a WAF policy parameter as &#39;Dynamic&#39; with extract URLs.</p>
<p><strong>Impact:</strong><br>Inability to see configured extract URLs from the UI parameter configuration screen within the WAF policy.</p>
<p><strong>Workaround:</strong><br>Go to the WAF policy and select the Policy Editor from the Panel menu. Once in the policy editor, search for the key word &quot;extractions&quot;: The JSON shows the parameter extraction with its respective extract URLs.</p>
<hr>
<h4><a name="A1574585-3" rel="nofollow"></a>1574585-3 : Auto-Failback cluster cannot upgrade active node<span title="This issue may cause the configuration to fail to load or may significantly impact system performance after upgrade">&starf;</span></h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>A cluster created with the auto-failback flag enabled will not upgrade the active node.</p>
<p><strong>Conditions:</strong><br>Enable the auto-failback flag.</p>
<p><strong>Impact:</strong><br>The active node cannot be upgraded.</p>
<p><strong>Workaround:</strong><br>Auto-failback cannot be configured through Central Manager GUI or API to prevent getting into this situation. Once the issue is resolved, this feature will be re-enabled in the product.</p>
<hr>
<h4><a name="A1574573" rel="nofollow"></a>1574573 : Global Resiliency Group status not reflecting correctly on update</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>After updating the Global Resiliency group, the group status may not immediately switch to &quot;DEPLOYING,&quot; potentially causing the UI to inaccurately reflect the ongoing provisioning process, despite deployment being in progress.</p>
<p><strong>Conditions:</strong><br>During updates to the Global Resiliency group.</p>
<p><strong>Impact:</strong><br>Update Status of Global Resiliency Group is incorrect.</p>
<p><strong>Workaround:</strong><br>To mitigate this issue, wait for approximately 5 minutes after updating the Global Resiliency group. This will allow the DNS listener address to become available for the newly added instance.</p>
<hr>
<h4><a name="A1574565" rel="nofollow"></a>1574565 : Inability to edit Generic Host While Re-Enabling Global Resiliency</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>Following the re-enabling of Global Resiliency from a previously disabled state, users are unable to simultaneously add or edit Generic Hosts.</p>
<p><strong>Conditions:</strong><br>During the re-enabling process of Global Resiliency.</p>
<p><strong>Impact:</strong><br>Unable to add or edit Generic Host information.</p>
<p><strong>Workaround:</strong><br>Refrain from making any changes to the Generic Host when re-enabling Global Resiliency from a previously disabled state.<br>
<br>
After the application has been deployed, you can then proceed to add or modify Generic Hosts during the next application edit.</p>
<hr>
<h4><a name="A1568129" rel="nofollow"></a>1568129 : During upgrade from BIG-IP Next 20.1.0 to BIG-IP Next 20.2.0, issue identified with instances that has L3-Forwards with non default VRF (L3-Network) configuration</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>In BIG-IP Next 20.1.0, it is possible for instances to have a L3-forward that uses non-default L3-Network (VRF).<br>
<br>
In BIG-IP Next 20.2.0, the parameter L3-Network (VRF) completely removed in the L3-forward GUI. For any L3-forward in CM version 20.2.0, always use the Default VRF configuration.<br>
<br>
In BIG-IP Next 20.2.0, Central Manager is not supporting creating or editing L3-Forward using non default VRF configuration. All the L3-Forward that is shown in the L3-Forward GUI will be assumed using default VRF configuration. If the L3-Forward is using non-default VRF configuration, the only action that user can do is deleting that L3-Forward.</p>
<p><strong>Conditions:</strong><br>Upgrade from BIG-IP Next 20.1.0 to BIG-IP Next 20.2.0 with L3-Forward config using non default VRF</p>
<p><strong>Impact:</strong><br>You cannot assume that the existing L3-Forward config is using the default VRF or non default VRF in the CM UI. You will have to re-create an L3-Forward using the CM UI so that it will use the default VRF.</p>
<p><strong>Workaround:</strong><br>Delete the L3-Forward</p>
<hr>
<h4><a name="A1567129" rel="nofollow"></a>1567129 : Unable to deploy Apps on BIG-IP Next v20.2.0 created using Instantiation from v20.1.x<span title="This issue may cause the configuration to fail to load or may significantly impact system performance after upgrade">&starf;</span></h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>1. Install BIG-IP Next Central Manager with v20.1.x build BIG-IP-Next-CentralManager-20.1.1-0.0.1.<br>
2. Deploy 2 tenants on rseries via IOD process with v20.1.x build(20.1.0-2.279.0+0.0.75) and 20.2.0 build(20.2.0-2.375.1+0.0.1). Configure L1-L3 during IOD process on both tenants. <br>
3. Deploy FastL4 migrated app on the v20.2.0 tenant. Observed below error during deployment- <br>
<br>
<br>
The task failed, failure reason: AS3-0007: AS3 Deploy Error: Failed to accept request on BIG-IP Next instance: {&quot;code&quot;:422,&quot;message&quot;:&quot;At least one L3-network object must be configured before applying a declaration.&quot;,&quot;errors&quot;:[]}</p>
<p><strong>Conditions:</strong><br>If v20.2.0 BIG-IP Next was created using instantiation from BIG-IP Next Central Manager.</p>
<p><strong>Impact:</strong><br>Since there are no default objects created for v20.1.x BIG-IP Next Central Manager and v20.2.0 BIG-IP Next combination, the application creation will fail as it expects the presence of a VRF object.</p>
<p><strong>Workaround:</strong><br>1. Upgrade BIG-IP Next Central Manager to v20.2.0 and create VLANs by editing the BIG-IP instance and make sure to check the &quot;Default VRF&quot; check box.</p>
<hr>
<h4><a name="A1566745-1" rel="nofollow"></a>1566745-1 : L3VirtualAddress set to ALWAYS advertise will not advertise if there is no associated Stack behind it</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>L3VirtualAddress set to RHI Mode ALWAYS advertise will not advertise if there is no associated Application Stack behind it.</p>
<p><strong>Conditions:</strong><br>Configuration of RHI Mode to ALWAYS advertise on an L3VirtualAddress without an associated Application Stack.</p>
<p><strong>Impact:</strong><br>L3VirtualAddress will not be advertised as expected.</p>
<p><strong>Workaround:</strong><br>None</p>
<hr>
<h4><a name="A1560605" rel="nofollow"></a>1560605 : Global Resiliency functionality fails to meet expectations on Safari browsers</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>Global Resiliency Group UI main pane goes under the left navigation in Safari browser.</p>
<p><strong>Conditions:</strong><br>When creating a Global Resiliency group in Safari browser.</p>
<p><strong>Impact:</strong><br>Not able to create Global Resiliency Group.</p>
<p><strong>Workaround:</strong><br>Use Chrome browser for creating Global Resiliency Group.</p>
<hr>
<h4><a name="A1550345-2" rel="nofollow"></a>1550345-2 : BIG-IP Next API gateway takes long time to respond large access policy playload</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>The BIG-IP Next API gateway takes a long time to respond to a large access policy config payload. API gateway timeouts could occur.</p>
<p><strong>Conditions:</strong><br>Create an access policy tree with depth over 10 using &quot;nextItems&quot; property.</p>
<p><strong>Impact:</strong><br>API performance is degraded, and the API gateway may time out.</p>
<p><strong>Workaround:</strong><br>Break the policy tree into multiple macros and stitch them together.</p>
<hr>
<h4><a name="A1498421" rel="nofollow"></a>1498421 : Restoring Central Manager (VE) with KVM HA Next instance fails on a new BIG-IP Next Central Manager</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>The user cannot restore BIG-IP Next Central Manager for the first time.</p>
<p><strong>Conditions:</strong><br>BIG-IP Next Central Manager on VE managing instances which includes a KVM HA instance.</p>
<p><strong>Impact:</strong><br>For first time, user will not be able to restore the backup archive into a new BIG-IP Next Central Manager.</p>
<p><strong>Workaround:</strong><br>The user must perform a second restoration of the backup archive into a new BIG-IP Next Central Manager.</p>
<hr>
<h4><a name="A1498121" rel="nofollow"></a>1498121 : BIG-IP Next Central Manager upgrade alerts not visible in global bell icon</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>User of BIG-IP Next Central Manager not able to see the alerts sent by upgrade of BIG-IP Next Central Manager.</p>
<p><strong>Conditions:</strong><br>During upgrade of Central Manager from version 20.0.x to 20.1.x, it may encounter errors.</p>
<p><strong>Impact:</strong><br>Alerts does not reflect in the &#39;Global Bell Icon&#39; if there are errors during BIG-IP Next Central Manager upgrade.</p>
<hr>
<h4><a name="A1495017" rel="nofollow"></a>1495017 : BIG-IP Next Hostname, Group Name and FQDN name should adhere to RFC 1123 specification</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>Hostname, Group Name and FQDN Name used in Global Resiliency feature should be lowercase.</p>
<p><strong>Conditions:</strong><br>Providing names for above mentioned fields with capital letters causes failure.</p>
<p><strong>Impact:</strong><br>Group creation or FQDN creation fails when capital letters are used in them.</p>
<p><strong>Workaround:</strong><br>Always create names with small letters and should adhere to RFC 1123 specification.</p>
<hr>
<h4><a name="A1495005" rel="nofollow"></a>1495005 : Cannot create Global Resiliency Group with multiple instances if the DNS instances have same hostname</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>The hostname is defaulted and cannot be modified when the hostname is not specified for the BIG-IP Next instances on BIG-IP Next Central Manager</p>
<p><strong>Conditions:</strong><br>Create a Global Resiliency Group with more than one BIG-IP Next instance with same name.</p>
<p><strong>Impact:</strong><br>Global Resiliency Group creation fails.</p>
<p><strong>Workaround:</strong><br>Make sure the hostname is set and unique for the BIG-IP Next instances going to be used in Global Resiliency Group creation.</p>
<hr>
<h4><a name="A1494997" rel="nofollow"></a>1494997 : Deleting a GSLB instance results in record creation of GR group in BIG-IP Next Central Manager</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>Deleting the BIG-IP instance from the &quot;Infrastructure -&gt; My Instances&quot; will disrupt the Global Resiliency Configuration using those instances.</p>
<p><strong>Conditions:</strong><br>The issue occurs when an instance is deleted directly while it is being used in a Global Resiliency Configuration.</p>
<p><strong>Impact:</strong><br>Deleting the instance under these conditions will break the Global Resiliency feature, leading to DNS resolution failure for the GR Group.</p>
<p><strong>Workaround:</strong><br>Refrain from deleting the instances when they are currently being used in a Global Resiliency Group.</p>
<hr>
<h4><a name="A1492705" rel="nofollow"></a>1492705 : During upgrading to BIG-IP Next 20.1.0, the BIG-IP Next 20.1.0 Central Manager failed to connect with BIG-IP Next 20.0.2 instance</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>BIG IP Next 20.1.0 Central Manager is managing BIG-IP Next 20.0.2 instances.<br>
When upgrading Next instance from BIG-IP 20.0.2 to BIG-IP 20.1.0, Central Manager failed to connect with the instance.</p>
<p><strong>Conditions:</strong><br>BIG IP Next 20.1.0 Central Manager managing BIG-IP Next 20.0.2 instances.</p>
<p><strong>Impact:</strong><br>Connection to BIG-IP Next instances fails.</p>
<p><strong>Workaround:</strong><br>Following is the workaround:<br>
<br>
1. Start with BIG-IP Next Central Manager of 20.0.2 managing BIG-IP Next 20.0.2 instances<br>
2. Upgrade Next instances of 20.0.2 version to 20.1.0 version<br>
3. Upgrade Central Manager from 20.0.2 version to 20.1.0 version.</p>
<hr>
<h4><a name="A1491197" rel="nofollow"></a>1491197 : Server Name (TLS ClientHello) Condition in policy shouldn&#39;t be allowed when &quot;Enable UDP&quot; option is selected in application under Protocols &amp; Profiles</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>Validation is not available in BIG-IP Next Central Manager  for the mutually exclusive configurations &quot;Enable UDP&quot; in application and &quot;TLS ClientHello&quot; condition in SSL Orchestrator policies.<br>
<br>
When we deploy Application with UDP enabled, then attach SSL Orchestrator policies to the application, it should not have &quot;TLS Client Hello&quot; condition based on &quot;Server Name&quot;.</p>
<p><strong>Conditions:</strong><br>Below are the condition in sequence:<br>
1. Create an application with UDP enabled <br>
2. Create and Attach an sslo policy, to that application, which has &quot;TLS ClientHello&quot; condition based on &quot;Server Name&quot; and deployed to next instance.</p>
<p><strong>Impact:</strong><br>Traffic processing will not work as the configuration is not valid and will not be sent to TMM until fixed.</p>
<hr>
<h4><a name="A1491121" rel="nofollow"></a>1491121 : Patching a new application service&#39;s parameters overwrites entire application service parameters</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>When sending a PATCH API request to append an application service&#39;s parameters, all parameter are completely replaced with changes, rather than partially changing the parameters according to the PATCH request.</p>
<p><strong>Conditions:</strong><br>Use a PATCH API request to partially update application service parameters. <br>
<br>
Deploy changes.</p>
<p><strong>Impact:</strong><br>If you send incomplete application service parameters, the changes will completely replace the existing parameters, and only partial parameters will be saved. This will lead to failed application service deployment as the parameters are incomplete.</p>
<p><strong>Workaround:</strong><br>When using the API request to change application service parameters, include in the body of the request full application service parameters, and not just partial changes.</p>
<hr>
<h4><a name="A1489945" rel="nofollow"></a>1489945 : HTTPS applications with self-signed certificates traffic is not working after upgrading BIG-IP Next instances to new version of BIG-IP Next Central Manager<span title="This issue may cause the configuration to fail to load or may significantly impact system performance after upgrade">&starf;</span></h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>HTTPS traffic is not working after upgrading the BIG-IP Next instances for the application service previously deployed using BIG-IP Next Central Manager version 20.0.x.</p>
<p><strong>Conditions:</strong><br>1. Install BIG-IP Next Central Manager version 20.0.x and add BIG-IP Next instance(s).<br>
2. Deploy the HTTP application service with a self-signed certificate created on BIG-IP Next Central Manager to an instance.<br>
3. Observed traffic is working fine.<br>
4. Now upgrade from 20.0.x to the newest version and observe HTTPS had traffic stopped working.</p>
<p><strong>Impact:</strong><br>This impacts HTTPS application service traffic.</p>
<p><strong>Workaround:</strong><br>1. Upgrade BIG-IP Next Central Manager to latest version.<br>
2. Create new self-signed certificates for the already deployed self-signed certificates through application services.<br>
3. Replace the existing self-signed certificate in the application service with newly created self-signed certificate and re-deploy the application service.<br>
4. After successfully re-deploying the application service, make sure traffic is working on the instance.<br>
5. Delete the old self-signed certificate(s) created in the earlier versions of BIG-IP Next Central Manager.</p>
<hr>
<h4><a name="A1474801" rel="nofollow"></a>1474801 : BIG-IP Next Central Manager creates a default VRF for all VLANS of the onboarded Next device</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>BIG-IP Next Central Manager creates a default VRF for all VLANS of the onboarded Next device.</p>
<p><strong>Conditions:</strong><br>The user wants to use specific VLANS for application traffic.</p>
<p><strong>Impact:</strong><br>Users would not be able to select VLANS for an application.</p>
<p><strong>Workaround:</strong><br>1. User must create VLANs using /L1 Networks endpoint directly on BIG-IP Next, before adding the device to BIG-IP Next Central Manager.<br>
2. The user can add the device to CM and choose the VLANs for SSL Orchestrator use cases.<br>
<br>
Subsequently:<br>
1. User should perform L1Network related operations on Next only.</p>
<hr>
<h4><a name="A1474669-2" rel="nofollow"></a>1474669-2 : Fluentbit core may be generated when restarting the pod</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>When shutting down a pod, fluent-bit core files may be generated as a result of access to an invalid pointer.</p>
<p><strong>Conditions:</strong><br>Restarting the pod where fluentbit runs.</p>
<p><strong>Impact:</strong><br>Core files might be generated during pod shutdown. Since fluentd is third-party software, the core files cannot be used for debugging.</p>
<p><strong>Workaround:</strong><br>None</p>
<hr>
<h4><a name="A1466305" rel="nofollow"></a>1466305 : Anomaly in factory reset behavior for DNS enabled BIG-IP Next deployment</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>Factory reset API does not bring TMM to default provisioned modules. DNS pods along with cne-proxy and cne-controller are not deleted.</p>
<p><strong>Conditions:</strong><br>BIG-IP Next cluster with DNS provisioned and WAF disabled.</p>
<p><strong>Impact:</strong><br>BIG-IP Next cluster with DNS provisioned will not go back to default deployment and user will have to deprovision DNS and re-provision WAF.</p>
<p><strong>Workaround:</strong><br>Deprovision DNS if cluster needs to go to factory defaults.</p>
<hr>
<h4><a name="A1410241-1" rel="nofollow"></a>1410241-1 : Traffic for TAP is not seen on service interface when connection mirroring is turned on</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>Traffic for TAP is not seen on service interface when connection mirroring is turned on</p>
<p><strong>Conditions:</strong><br> -- Connection mirroring is turned on for a CM HA set up. <br>
-- Application is configured with SSL Orchestrator service. <br>
-- Traffic is passed.</p>
<p><strong>Impact:</strong><br>Traffic for TAP is not seen on service interface.</p>
<p><strong>Workaround:</strong><br>Connection Mirroring is not supported for SSL Orchestrator. It should be turned off when configuring with SSL Orchestrator policies or services.</p>
<hr>
<h4><a name="A1403861" rel="nofollow"></a>1403861 : Data metrics and logs will not be migrated when upgrading BIG-IP Next Central Manager from 20.0.2 to a later release</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>In the version 20.1.0 of BIG-IP Next Central Manager, OpenSearch is replaced by Elasticsearch as the main storage for data metrics and logs.<br>
<br>
Due to incompatibility between OpenSearch and Elasticsearch, metrics and logs that are stored on BIG-IP Next Central Manager in earlier versions will not be available after upgrading.</p>
<p><strong>Conditions:</strong><br>Upgrade BIG-IP Next Central Manager from a release version prior to 20.1.0.</p>
<p><strong>Impact:</strong><br>After the upgrade is complete, the data metrics and logs from the previous version will not be available on the upgraded BIG-IP Next Central Manager.</p>
<hr>
<h4><a name="A1366321-1" rel="nofollow"></a>1366321-1 : BIG-IP Next Central Manager behind a forward-proxy</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>Using &quot;forward proxy&quot; for external network calls from BIG-IP Next Central Manager fails.</p>
<p><strong>Conditions:</strong><br>When the network environment BIG-IP Next Central Manager is deployed in has a policy of routing all external calls through a forward proxy.</p>
<p><strong>Impact:</strong><br>BIG-IP Next Central Manager does not currently support proxy configurations, so you cannot deploy BIG-IP Next instances in that environment.</p>
<p><strong>Workaround:</strong><br>Allow BIG-IP Next Central Manager to connect to external endpoints by bypassing the &quot;forward proxy&quot; until BIG-IP Next Central Manager supports proxy configurations.</p>
<hr>
<h4><a name="A1365445" rel="nofollow"></a>1365445 : Creating a BIG-IP Next instance on vSphere fails with &quot;login failed with code 401&quot; error message<span title="This issue may cause the configuration to fail to load or may significantly impact system performance after upgrade">&starf;</span></h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>Creating a BIG-IP Next VE instance in vShpere fails.</p>
<p><strong>Conditions:</strong><br>This happens when the randomly generated initial admin password contains an unsupported character.</p>
<p><strong>Impact:</strong><br>Creating a BIG-IP Next VE instance fails.</p>
<p><strong>Workaround:</strong><br>Try recreating the BIG-IP Next VE instance.</p>
<hr>
<h4><a name="A1365433" rel="nofollow"></a>1365433 : Creating a BIG-IP Next instance on vSphere fails with &quot;login failed with code 501&quot; error message<span title="This issue may cause the configuration to fail to load or may significantly impact system performance after upgrade">&starf;</span></h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>Creating a BIG-IP Next VE instance fails and returns a code 503 error.</p>
<p><strong>Conditions:</strong><br>Attempting to create a BIG-IP Next VE instance from BIG-IP Next Central Manager when the vSphere environment has insufficient resources.</p>
<p><strong>Impact:</strong><br>Creating a BIG-IP Next VE instance fails.</p>
<p><strong>Workaround:</strong><br>Use one of the following workarounds.<br>
<br>
- Retry creating the BIG-IP Next instance.<br>
- Create the BIG-IP Next instance directly in the vSphere provider environment then add it to BIG-IP Next Central Manager.</p>
<hr>
<h4><a name="A1365417" rel="nofollow"></a>1365417 : Creating a BIG-IP Next VE instance in vSphere fails when a backslash character is in the provider username<span title="This issue may cause the configuration to fail to load or may significantly impact system performance after upgrade">&starf;</span></h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>If you include a backslash character in the provider username when creating a BIG-IP Next VE instance creation fails because BIG-IP Next Central Manager parses it as an escape character.</p>
<p><strong>Conditions:</strong><br>Creating a BIG-IP Next VE instance that includes a backslash character in the provider username.</p>
<p><strong>Impact:</strong><br>Creation of the BIG-IP Next instance fails.</p>
<p><strong>Workaround:</strong><br>Do not use the backslash character in the provider username.</p>
<hr>
<h4><a name="A1365005" rel="nofollow"></a>1365005 : Analytics data is not restored after upgrading to BIG-IP Next version 20.0.1<span title="This issue may cause the configuration to fail to load or may significantly impact system performance after upgrade">&starf;</span></h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>After upgrading from BIG-IP Next version 20.0 to 20.0.1, analytic data is not restored.</p>
<p><strong>Conditions:</strong><br>After upgrading from BIG-IP Next version 20.0 to 20.0.1.</p>
<p><strong>Impact:</strong><br>Analytics data is not automatically restored after upgrading and cannot be restored manually.</p>
<hr>
<h4><a name="A1360709" rel="nofollow"></a>1360709 : Application page can show an error alert that includes &quot;FAST delete task failed for application&quot;</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>After you successfully delete a BIG-IP Next instance that has application services deployed to it, an alert banner on the Applications page states that the delete task failed even though it&#39;s successful.</p>
<p><strong>Conditions:</strong><br>Delete a BIG-IP Next instance and then navigate to the Applications page.</p>
<p><strong>Impact:</strong><br>This can cause confusion.</p>
<p><strong>Workaround:</strong><br>None</p>
<hr>
<h4><a name="A1360621" rel="nofollow"></a>1360621 : Adding a Control Plane VLAN must be done only during BIG-IP Next HA instance creation</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>If you attempt to edit a BIG-IP Next HA instance properties to add a Control Plane VLAN, it fails.</p>
<p><strong>Conditions:</strong><br>Editing the properties for an existing BIG-IP Next VE HA instance and attempting to add a Control Plane VLAN.</p>
<p><strong>Impact:</strong><br>The attempt to edit/add Control Plane VLAN fails.</p>
<p><strong>Workaround:</strong><br>Create the Control Plane VLAN when you initially create the BIG-IP Next HA instance.</p>
<hr>
<h4><a name="A1360121-1" rel="nofollow"></a>1360121-1 : Unexpected virtual server behavior due to removal of objects unsupported by BIG-IP Next</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>The migration process ensures that application services are supported by BIG-IP Next. If a property value is not currently supported by BIG-IP Next, it is removed and is not present in the AS3 declaration. If the object was a default value, the object is replaced by a default value that is supported by BIG-IP Next.</p>
<p><strong>Conditions:</strong><br>1. Migration a UCS archive from BIG-IP to BIG-IP Next Central Manager.<br>
<br>
2. Review the AS3 declaration during the Pre Deployment staged.<br>
<br>
<br>
<br>
Example for &quot;cache-size&quot; property of &quot;web-acceleration&quot; profile:<br>
- BIG-IP config cache-size = 500mb OR 0mb<br>
- AS3 schema supported range = 1-375mb<br>
- BIG-IP Next stack (clientSide/caching/cacheSize) supported range 1-375mb<br>
- AS3 output created by migration does not produce &quot;cacheSize&quot; property if cache-size is greater than 375mb or lower than 1mb.<br>
- Deployment of AS3 declaration uses BIG-IP Next defaults in both cases (cache-size 375 or 0mb)</p>
<p><strong>Impact:</strong><br>Default values of virtual server&#39;s objects may change, impacting virtual server&#39;s behavior.</p>
<p><strong>Workaround:</strong><br>Although you cannot use values which are unsupported by BIG-IP Next, you can update the AS3 declaration with missing properties to specify values other than default ones added during the migration process.<br>
<br>
To do so, read: https://clouddocs.f5.com/bigip-next/latest/schemasupport/schema-reference.html<br>
<br>
to modify AS3 declaration by adding missing properties and specifying values within supported range.</p>
<hr>
<h4><a name="A1360097-1" rel="nofollow"></a>1360097-1 : Migration highlights and marks &quot;net address-list&quot; as unsupported, but addresses are converted to AS3 format</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>Objects of a type: &quot;net address-list&quot; are incorrectly marked as unsupported, while virtual servers in AS3 output contain the property &quot;virtualAddresses&quot;.</p>
<p><strong>Conditions:</strong><br>If an address list is used to configure a virtual server, it will be highlighted as unsupported in the configuration editor even if it is properly translated to AS3 &quot;virtualAddresses&quot; property.<br>
<br>
Example of the object:<br>
<br>
net address-list /tenant3892a81b1f9e6/application_11/IPv6AddressList {<br>
&nbsp;&nbsp;&nbsp;&nbsp;addresses {<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;fe80::1ff:fe23:4567:890a-fe80::1ff:fe23:4567:890b { }<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;fe80::1ff:fe23:4567:890c { }<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;fe80::1ff:fe23:4567:890d { }<br>
&nbsp;&nbsp;&nbsp;&nbsp;}<br>
&nbsp;&nbsp;&nbsp;&nbsp;description IPv6<br>
}<br>
<br>
Example of an AS3 property:<br>
&nbsp;&quot;virtualAddresses&quot;: [<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;fe80::1ff:fe23:4567:890a-fe80::1ff:fe23:4567:890b&quot;,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;fe80::1ff:fe23:4567:890c&quot;,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;fe80::1ff:fe23:4567:890d&quot;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;],</p>
<p><strong>Impact:</strong><br> - The object is translated to virtualAddresses property in the AS3, but an application is marked as yellow.<br>
- The object is translated, but one of the values from the address list is not supported on BIG-IP Next (IPv6 value range)</p>
<p><strong>Workaround:</strong><br>Verify that all addresses from &#39;net address-list&#39; object are configured as &quot;virtualAddresses&quot; property value list in the AS3 output.<br>
<br>
Verify that all addresses from &#39;net address-list&#39; are supported on BIG-IP Next. Remove or modify virtualAddresses value list if needed.</p>
<hr>
<h4><a name="A1360093-1" rel="nofollow"></a>1360093-1 : Abbreviated IPv6 destination address attached to a virtual server is not converted to AS3 format</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>Service class in AS3 output does not have &#39;virtualAddresses&#39; property, for example:<br>
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;Common_virtual_test&quot;: {<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;snat&quot;: &quot;none&quot;,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;class&quot;: &quot;Service_TCP&quot;,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;profileTCP&quot;: {<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;use&quot;: &quot;/tenant017b16b41f5c7/application_9_SMtD/tcp_default_v14&quot;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;persistenceMethods&quot;: []<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</p>
<p><strong>Conditions:</strong><br>Migrate an application service with abbreviated IPv6 address:<br>
<br>
ltm virtual-address /tenant017b16b41f5c7/application_9_SMtD/aa::b {<br>
&nbsp;&nbsp;&nbsp;&nbsp;address aa::b<br>
&nbsp;&nbsp;&nbsp;&nbsp;arp enabled<br>
&nbsp;&nbsp;&nbsp;&nbsp;traffic-group /Common/traffic-group-1</p>
<p><strong>Impact:</strong><br>Virtual server is misconfigured, no listener on a specific IP address is created.</p>
<p><strong>Workaround:</strong><br>All application services containing virtual servers configured with abbreviated IPv6 addresses should be updated once they are migrated to BIG-IP Next Central Manager.<br>
<br>
Go to Applications -&gt; My Application Services, find your application service name and edit it.<br>
<br>
Find your virtual server name and update it with a property<br>
<br>
&nbsp;&quot;virtualAddresses&quot;: [<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;aa::b&quot;,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;]<br>
<br>
like this:<br>
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;Common_virtual_test&quot;: {<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;snat&quot;: &quot;none&quot;,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;class&quot;: &quot;Service_TCP&quot;,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;virtualAddresses&quot;: [<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;aa::b&quot;,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;profileTCP&quot;: {<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;use&quot;: &quot;/tenant017b16b41f5c7/application_9_SMtD/tcp_default_v14&quot;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;persistenceMethods&quot;: []<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}</p>
<hr>
<h4><a name="A1359209-1" rel="nofollow"></a>1359209-1 : The health of application service shown as &quot;Good&quot; when deployment fails as a result of invalid iRule syntax</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>When an application servvice with an invalid iRule is deployed to an instance from BIG-IP Next Central Manager, deployment is shown as successful but the post deployment iRule validation failed on the instance. Health status should be changed to &quot;Critical/Warning&quot; but it is still shown as &quot;good&quot;.</p>
<p><strong>Conditions:</strong><br>Deploy an application service with an invalid iRule.</p>
<p><strong>Impact:</strong><br>Incorrect status of the application service is shown in the My Application Services page.</p>
<p><strong>Workaround:</strong><br>Always try to use a valid iRule when deploying to BIG-IP Next.</p>
<hr>
<h4><a name="A1358985-1" rel="nofollow"></a>1358985-1 : Failed deployment of migrated application services to a BIG-IP Next instance</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>Deployment of a migrated application service to a BIG-IP Next instance might fail even if the declaration is valid. This can occur after the application service was successfully saved as draft on BIG-IP Next Central Manager<br>
The following can appear in the deployment logs:<br>
- No event with error code from deployment to instance in migration logs<br>
- 202 response code &quot;in progress&quot; from deployment to instance in migration logs<br>
- 503 response code &quot;Configuration in progress&quot; from deployment to instance in migration logs</p>
<p><strong>Conditions:</strong><br>1. Migrate an application service during a migration session<br>
<br>
2. Select a deployment location and deploy the application service.<br>
<br>
Review the migration log: the application service was successfully saved to BIG-IP Next Central Manager, but the deployment to the selected location failed with error.</p>
<p><strong>Impact:</strong><br>There are 3 different errors that can result in the deployment logs (Deployment Summary&gt;View logs):<br>
<br>
<br>
Reason 1:<br>
<br>
Migration process started.<br>
Application: &lt;application name&gt; saved as draft to BIG-IP Next Central Manager.<br>
Migration process failed.<br>
<br>
<br>
Reason 2:<br>
<br>
Migration process started<br>
Application: &lt;application name&gt; saved as draft to BIG-IP Next Central Manager.<br>
Log Message: Deployment to &lt;BIG-IP Next IP address&gt; failed with the error: &#39;{&#39;code&#39;: 202, &#39;host&#39;: &#39;&lt;hostname&gt;, &#39;message&#39;: &#39;in progress&#39;, &#39;runTime&#39;: 0, &#39;tenant&#39;: &#39;&lt;tenant name&gt;&#39;}&#39;.<br>
Migration process failed.<br>
<br>
<br>
Reason 3:<br>
<br>
If you are currently processing the same AS3 declaration sent from a different source or migration session:<br>
Migration process started.<br>
Application: &lt;application name&gt; saved as draft to BIG-IP Next Central Manager.<br>
Log message: Deployment to &lt;BIG-IP Next IP address&gt; failed with the error: &#39;{&#39;code&#39;: 503, &#39;errors&#39;: [], &#39;message&#39;: &#39;Configuration operation in progress on device, please try again later.&#39;}&#39;.<br>
Migration process failed.</p>
<p><strong>Workaround:</strong><br>The application service was successfully saved as a draft on BIG-IP Next Central Manager. <br>
<br>
You can go to My Application Services, select the application service that failed to deploy, and deploy the application service to a selected instance location.</p>
<hr>
<h4><a name="A1355605" rel="nofollow"></a>1355605 : &quot;NO DATA&quot; is displayed when setting names for appliction services, virtual servers and pools, that exceed max characters</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>&quot;NO DATA&quot; is displayed in the application metrics charts when setting a name that exceeds 33 characters for an application service, pool, or virtual server.</p>
<p><strong>Conditions:</strong><br>1. Create an application service with a virtual server and a pool.<br>
2. Set the name of each of the objects above to be 34 characters or longer.<br>
3. Add an endpoint to the pool.<br>
4. Deploy the application service, and wait for the application service to pass traffic.</p>
<p><strong>Impact:</strong><br>&quot;NO DATA&quot; is displayed in the application service, pool and virtual server data metrics charts.</p>
<p><strong>Workaround:</strong><br>When creating an application the names of the application services, pools and virtual servers cannot exceed 33 characters.</p>
<hr>
<h4><a name="A1354645" rel="nofollow"></a>1354645 : Error displays when clicking &quot;Edit&quot; on the Instance Properties panel</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>When editing the properties of a BIG-IP Next instances page, a &quot;Error: unsupported platform type&quot; displays.</p>
<p><strong>Conditions:</strong><br>When viewing the Instances page, the BIG-IP Next instance&#39;s hostname to view its properties. On the Instance Properties panel, click the Edit button.</p>
<p><strong>Impact:</strong><br>This can cause confusion.</p>
<p><strong>Workaround:</strong><br>Wait for the BIG-IP Next instance&#39;s hostname to load on Instance Properties panel before clicking the Edit button.</p>
<hr>
<h4><a name="A1354265" rel="nofollow"></a>1354265 : The icb pod may restart during install phase</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>The icb may generate a core during the install phase which will cause a restart of icb pod. However, we have observed icb to restart fine with no issues.</p>
<p><strong>Conditions:</strong><br>The issue is seen during the upgrade install.</p>
<p><strong>Impact:</strong><br>Post-first panic, icb restarts fine and no known bad impact is observed.</p>
<p><strong>Workaround:</strong><br>None</p>
<hr>
<h4><a name="A1353589" rel="nofollow"></a>1353589 : Provisioning of BIG-IP Next Access modules is not supported on VELOS, but containers continue to run</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>1) Containers that belong to the BIG-IP Next Access module keep running on BIG-IP Next all the time on VELOS &amp; rSeries.<br>
2) On VE, the containers run only if the BIG-IP Next Access module is provisioned using: /api/v1/systems/{systemID}/provisioning api</p>
<p><strong>Conditions:</strong><br>This is observed all the time when BIG-IP Next is deployed on VELOS/r-series.</p>
<p><strong>Impact:</strong><br>Containers that belong to the BIG-IP Next Access module keep running all the time and this can lead to wastage of resources on VELOS &amp; rSeries.</p>
<p><strong>Workaround:</strong><br>If you do not want to run BIG-IP Next Access containers as part of a BIG-IP Next tenant deployment, you can use this workaround before installing the tenant:<br>
<br>
1) Run the following command on the standby controller:<br>
<br>
sed -i &#39;s/access: true/access: false/g&#39; /var/F5/partition&lt;partition-ID&gt;/SPEC/&lt;IMAGE_VERSION&gt;. yaml <br>
<br>
2) Trigger failover from partition cli -&gt;:<br>
<br>
system redundancy go-standby <br>
<br>
3) Install the tenant.</p>
<hr>
<h4><a name="A1352969" rel="nofollow"></a>1352969 : Upgrades with TLS configuration can cause TMM crash loop</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>After upgrading from a version prior to 20.0.1, connection is lost.</p>
<p><strong>Conditions:</strong><br> - Keys and certificates are configured as files in TLS configuration.<br>
- Upgrading from a version prior to 20.0.1.</p>
<p><strong>Impact:</strong><br>An error similar to the following is logged: Failed to connect to &lt;IP address port: xx&gt; No route to host</p>
<p><strong>Workaround:</strong><br>After upgrading, reconfigure the private key files so that validation properly occurs. <br>
<br>
Fix any existing mismatch keys and certificates.</p>
<hr>
<h4><a name="A1350365" rel="nofollow"></a>1350365 : Performing licensing changes directly on a BIG-IP Next instance</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>BIG-IP Next Central Manager will become out of sync with a managed BIG-IP Next instance if you perform licensing actions directly to the BIG-IP Next instance.</p>
<p><strong>Conditions:</strong><br>Add a BIG-IP Next instance to BIG-IP Next Central Manager. Perform licensing actions directly on the BIG-IP Next instance.</p>
<p><strong>Impact:</strong><br>BIG-IP Next Central Manager is no longer synchronized with its managed instance.</p>
<hr>
<h4><a name="A1350285-1" rel="nofollow"></a>1350285-1 : Traffic is not passing after the tenant is licensed and network is configured</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>After configuring and licensing the BIG-IP Next tenant, such as after an upgrade, traffic is not passing.</p>
<p><strong>Conditions:</strong><br>A BIG-IP Next tenant is configured without vlans, and a /PUT to create the L1 networking interface is performed; and then vlans are later allocated to the tenant.  In this scenario, the (later-) allocated vlans will not take effect for the previously configured L1 network interface.</p>
<p><strong>Impact:</strong><br>Data traffic associated with the later-added vlans will not be processed.</p>
<p><strong>Workaround:</strong><br>Workaround is to allocate vlans to the BIG-IP Next tenant before the /PUT call to create the L1 network interface; at which point the L1 network interface will be associated with a vlan allocated to that BIG-IP Next instance.</p>
<hr>
<h4><a name="A1343005-1" rel="nofollow"></a>1343005-1 : Modifying L4 serverside after the stack is created can result in the update not being applied</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>Any updates to L4 server-side settings after the stack is created can result in the update not being applied, leading to traffic disruption.</p>
<p><strong>Conditions:</strong><br>Modifying an L4 serverside with a snat pool attached to an application stack to snat type AUTOMAP.</p>
<p><strong>Impact:</strong><br>Traffic is disrupted after updating the L4 serverside config</p>
<p><strong>Workaround:</strong><br>Resend the stack config after the updating the L4 serverside to resume the traffic.</p>
<hr>
<h4><a name="A1325713" rel="nofollow"></a>1325713 : Monthly backup cannot be scheduled for the days 29, 30, or 31</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>You cannot schedule a monthly backup on the last 3 days of the month (29, 30, or 31) because some months do not contain these days (for example, February).</p>
<p><strong>Conditions:</strong><br>Creating a monthly backup schedule from BIG-IP Next Central Manager that contains the days 29, 30, or 31.</p>
<p><strong>Impact:</strong><br>If you select these days for your schedule, BIG-IP Next Central Manager returns a 500 error.</p>
<hr>
<h4><a name="A1314617" rel="nofollow"></a>1314617 : Deleting an interface on a running BIG-IP Next instance can cause the system to behave unexpectedly</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>Once a BIG-IP Next instance is configured to use certain interfaces on the first boot up, deleting one of them would put the system in an unpredictable state. This update to the instance should be avoided.</p>
<p><strong>Conditions:</strong><br>Remove an interface from an existing BIG-IP Next instance.</p>
<p><strong>Impact:</strong><br>The BIG-IP Next behaves unpredictably once a network interface is removed from a running instance.</p>
<p><strong>Workaround:</strong><br>None</p>
<hr>
<h4><a name="A1134225" rel="nofollow"></a>1134225 : AS3 declarations with a SNAT configuration do not get removed from the underlying configuration as expected</h4>
<p><strong>Links to More Info: </strong> <a href="https://my.f5.com/manage/s/article/K000138849" target="_blank">K000138849</a></p>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>AS3-configured L4-serversides object contains a SNAT property when it should not, given that SNAT was previously configured in the declaration and then subsequently removed.</p>
<p><strong>Conditions:</strong><br>SNAT configuration was specified in the AS3 declaration and then subsequently removed.</p>
<p><strong>Impact:</strong><br>A SNAT cannot be removed once it has been added.</p>
<p><strong>Workaround:</strong><br>Remove the L4-serversides object, either by removing the relevant configuration from the AS3 declaration or by using DELETE /api/v1/L4-serversides, and then re-POST the AS3 declaration without the SNAT.</p>
<hr>
<h4><a name="A1122689-3" rel="nofollow"></a>1122689-3 : Cannot modify DNS configuration for a BIG-IP Next VE instance through API</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>Making updates to BIG-IP Next Virtual Edition (VE) DNS configuration through onboarding or the API does not update the DNS configuration as expected.</p>
<p><strong>Conditions:</strong><br>Making updates to a BIG-IP Next DNS configuration through the API.</p>
<p><strong>Impact:</strong><br>The BIG-IP Next instance continues to use the DNS servers supplied by DHCP on the interface by default.</p>
<p><strong>Workaround:</strong><br>Prior to updating the BIG-IP Next DNS configuration through the API, issue the following commands.<br>
<br>
$ rm -f /etc/resolv.conf; touch /etc/resolv.conf<br>
<br>
This removes all DNS configurations. DNS can then be managed through the BIG-IP Next instance&#39;s API, and the DNS provided by DHCP is ignored.</p>
<hr>
<h4><a name="A1087937" rel="nofollow"></a>1087937 : API endpoints do not support page query</h4>
<p><strong>Component: </strong>BIG-IP Next</p>
<p><strong>Symptoms:</strong><br>The &#39;page&#39; query is not supported.</p>
<p><strong>Conditions:</strong><br>This issue is seen when the API is called directly. There is no impact on the functionality if BIG-IP Next Central Manager or AS3 is used.</p>
<p><strong>Impact:</strong><br>Pagination of results does not function correctly.</p>
<p><strong>Workaround:</strong><br>Remove &#39;limit&#39; parameter. This causes all objects to be returned in the response.</p>
<hr>
<br>
<br>
<font size="3">&starf; <i>This issue may cause the configuration to fail to load or may significantly impact system performance after upgrade</i></font><br>
<br>
<br>
    <div id="notice" style="width:740px; background-color:#D8D8D8">
    <div align="center">
    *********************** NOTICE ***********************</div><br>
    For additional support resources and technical documentation, see:
    <ul>
      <li>The F5 Technical Support website: <a class="internalLink" href="http://www.f5.com/support/" target="_blank">http://www.f5.com/support/</a></li>
      <li>The MyF5 website: <a href="https://my.f5.com/manage/s/" target="_blank">https://my.f5.com/manage/s/</a></li>
      <li>The F5 DevCentral website: <a class="internalLink" href="http://devcentral.f5.com/" target="_blank">http://devcentral.f5.com/</a></li>
    </ul>
    <div align="center">******************************************************</div>
    </div>
    <div id="footer">
    <h5>Generated: Mon Oct 21 06:51:02 2024 PDT</h5>
    <h5>&copy;2024 F5, Inc. All rights reserved.</h5>
    </div>
<!-- bz2rn.pl -p mbip &#45;&#45;version Orange &#45;&#45;debug /tmp/bz2rn-debug.txt &#45;&#45;build 2.716.2+0.0.50 -->
<!-- Script version 218 -->
  </body>
</html>
</div>


                </div>
                
                <div class="row next-prev-btn-row">
                    <div class="col-lg-12">
                        
                        <a href="big-ip-next-rn-new-features.html" title="BIG-IP Next 20.3.0 Overview" accesskey="p"
                            class="btn btn-primary left"><i class="fa fa-arrow-circle-left" aria-hidden="true"></i>
                            Previous</a>
                        
                        
                        <a href="../install/index.html" title="Install BIG-IP Next" accesskey="n"
                            class="btn btn-primary right">Next <i class="fa fa-arrow-circle-right"
                                aria-hidden="true"></i></a>
                        
                    </div>
                </div>
                
                
                <hr>
                <div id="medallia_inline_survey" class="medallia_survey">
                </div>
                <script type="text/javascript" src="https://resources.digital-cloud-west.medallia.com/wdcwest/102748/onsite/embed.js" async></script>
                
                

            </article>
        </div>

    </div>

    
<div id="clouddocs-footer"></div>
<!-- Bootstrap core JavaScript
  ================================================== -->
<!-- Placed at the end of the document so the pages load faster -->

<script src="../_static/js/index.js"></script>

<script src="../_static/js/feedback.js"></script>
<script src="../_static/js/jquery.appear.js"></script>
<script src="../_static/js/printThis.js"></script>
<script src="../_static/js/bootstrap.min.js"></script>
<script src="../_static/js/clouddocs.js"></script>
<script src="../_static/js/CoveoJsSearch.Lazy.min.js"></script>
  </body>
</html>
